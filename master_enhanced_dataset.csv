text,country,publication,date,ai_mentions,word_count,year_month,quarter,length_category,article_id,safety_security,safety_security_prop,safety_security_binary,privacy_concerns,privacy_concerns_prop,privacy_concerns_binary,ethical_concerns,ethical_concerns_prop,ethical_concerns_binary,job_displacement,job_displacement_prop,job_displacement_binary,ai_capability,ai_capability_prop,ai_capability_binary,human_vs_ai,human_vs_ai_prop,human_vs_ai_binary,uncertainty_unknown,uncertainty_unknown_prop,uncertainty_unknown_binary,regulation_needed,regulation_needed_prop,regulation_needed_binary,innovation_progress,innovation_progress_prop,innovation_progress_binary,economic_benefits,economic_benefits_prop,economic_benefits_binary,sentiment_composite,sentiment_category
"The Government wants to set up an Artificial Intelligence Advisory Council to help set national policy on AI. Ultimately, it will lead to the ""development of trustworthy, person-centred AI"" which will see three-quarters of all Irish businesses using AI, according to the Junior Minister for Trade Promotion, Digital and Company Regulation, Dara Calleary. Publishing the State's progress report on the implementation of the national AI strategy, ""AI - Here for Good"", he said the advisory council would be set up to give ""independent expert advice on artificial intelligence policy, with a specific focus on building public trust"". ""For businesses, AI can increase competitiveness and productivity by freeing up staff for more complex roles,"" he said. ""Under our National Digital Strategy, we have set a very ambitious target of 75pc of enterprises in Ireland using AI by 2030. Government has also introduced the Digital Transition Fund which will help achieve this target."" The Government wants to ""influence"" the regulation of AI in Ireland and internationally, ""in particular in the development of the EU AI Act"", he said. It also intends to ""finalise and implement the principles and guidelines for the use of AI in the public sector"". The State's progress report on AI found that while most AI-related goals are still under development, some ""building blocks"" have been put in place. ""We appointed Ireland's first AI Ambassador (Dr Patricia Scanlan), who is leading a national conversation on the role of AI in all our lives and helping to demystify some of the myths and fears around AI,"" said Mr Calleary. ""We also established the Enterprise Digital Advisory Forum, convened a National Youth Assembly on AI, and joined the Global Partnership on AI. ""The strategy outlines the need for a robust governance framework to safeguard against risk and to underpin public trust in AI. Work is well progressed on a horizontal EU regulatory framework which will underpin trustworthy use of AI in the EU, and Ireland is actively engaged in this."" The National AI Strategy was originally published in June 2021 and is framed by the Government as part its ""digital transformation"" plan, including last year's National Digital Strategy, ""Connecting Government 2030"", and the White Paper on Enterprise. The new AI Advisory Council will now seek to recruit ""suitably qualified"" people from ""academia, business, law, security, social sciences, economics and civil society"" to serve on it.",Ireland,Irish Independent; Dublin,2023-08-09,14,394,2023-08-01,2023.3,Medium,1,2,5.076142131979695,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2.5380710659898473,1,7,17.766497461928935,1,5,12.690355329949238,1,1,2.5380710659898473,1,-0.12926230473939562,Neutral
"A I has risen to top of the agenda in business tech, with most Irish organisations saying that they'll invest in it somehow. But it comes with a hefty environmental price. A simple ChatGPT question uses about the same amount of energy as a few minutes' illumination from an LED light bulb. With OpenAI Sam Altman admitting this month that 800 million people use it, it's now taking up the equivalent electricity of hundreds of thousands of typical homes. And that's just one single large language model (LLM) generative AI platform. Throw in Google Gemini, Meta's Llama, Perplexity, Grok and a handful of others - not to mention the backend and bespoke systems that are multiplying every month - and that's a serious sustainability challenge facing the world. In Ireland, around 27pc of the country's grid is now taken up by AI-hungry data centres, with that figure set to increase to about a third in the coming years. That will be more than all the country's urban and rural homes combined. This has provoked a moratorium on new data centres - and a sense of panic over where our national grid will generate new electricity from. So what can businesses do to prevent themselves being part of a growing environmental scourge, while not missing out on the productivity features that most rivals will be harvesting from AI tools that are becoming increasingly standardised in everyday software and online systems? While Google claims that its Deep- Mind AI technology makes its data centres up to 40pc less energy-intensive, where do ordinary Irish set-ups start? Some Irish companies offer AI technology that they claim make the operations of their users less energy-intensive. Dublin-based FoodCloud uses a light form of AI to match unused food from restaurants and other outlets to places that can accept it, so that it (and the energy used to make it) doesn't go to waste. Its Foodiverse app is used by the likes of supermarket workers to find recipients. Galway-based CitySwift makes a platform that lets transport companies optimise the most efficient routes, taking into account things like weather patterns. Meath-based Hanley Energy uses predictive maintenance and real-time optimisation to predict when machines will break down before they fail. Sensors track hundreds of points, such as temperature, humidity, machine speeds and power loads. Its AI then learns from this data, figuring out when different systems are running too much or not enough. It has several large data-centre clients that use its technology. Dublin-based GridBeyond does something similar, using sensors on customers' factory machines that track and report on energy usage, sometimes even taking action based on its findings. An arguably more advanced version of this idea is used by Dublin-based Ecocem, which uses AI to help run machinery in plants with a claimed reduction of around half in CO2 output. This is a big deal for the cement industry, which is responsible for about 8pc of the world's carbon dioxide pollution. For Irish businesses who want to think about this more concretely, there are some financial incentives and grants available. Enterprise Ireland has a slew of them, as does the SEAI and even the local enterprise boards. Where AI can clearly result in efficiency and sustainability, the cost of deploying it increasingly qualifies for support from national agencies. Other than trying to bend AI to helping with the rationalisation of resources or smarter planning, some less conventional approaches have been mooted to lessen the environmental impact of AI. In the LLM world, one tentative option is to choose the Chinese LLM Deepseek, instead of using ChatGPT or Gemini. DeepSeek uses far less energy than any of its US counterparts. It can also be contained and used in a far smaller system; the whole thing can be stored on, and run from, a maxed-out version of Apple's latest Mac Studio with its M3 Ultra chip (which will cost over €10,000 per machine, but that's still comparatively cheap and efficient). Not everyone thinks this is a good idea, though. A Swiss AI software firm, LatticeFlow, told Bloomberg that two versions of DeepSeek's R1 model ""rank lowest"" among other leading systems when it comes to cyber security. Deep- Seek's models, the company claimed, were especially vulnerable to ""goal hijacking"" and prompt leakage - when an AI can be tricked into ignoring its safety guardrails and either reveal sensitive information or perform harmful actions it's supposed to prevent. Another response is a radical reconsideration of energy sources. While Ireland grapples with the pros and cons of allowing new liquid natural gas developments or where wind turbines should go, several big tech firms are considering small nuclear reactors. Last year, Google and Amazon announced they would back a wave of new newly-built small reactors to power their data centres from 2030. Microsoft is doing something similar. These are quite different to the conventional image of a nuclear power plant, with no large, fat funnel. They more resemble industrial buildings and are capable of putting out around a third of the power of a larger nuclear installation, but are quicker to build and - the power companies claim - also easier to decommission. Regardless, our historical aversion to nuclear power, on our own territory, means it might be decades before Ireland is ready to seriously consider such a move. 'A simple CHATGPT question uses about the same amount of energy as a few minutes of light from an LED bulb'",Ireland,Irish Independent; Dublin,2025-05-01,20,906,2025-05-01,2025.2,Long,2,4,4.415011037527594,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.1037527593818985,1,2,2.207505518763797,1,-0.7016623318972055,Negative
"Heart disease patients in Ireland and across the world can look forward to better outcomes and avoiding unnecessary treatments due to artificial intelligence (AI) advances, a leading cardiologist and researcher at University of Galway has said. Professor of Interventional Cardiology William Wijns said that angiograms were routinely used to look at blood vessels, allowing doctors to check for flow blockages. But due to limitations around this form of test, it is estimated that diagnosis and treatment of patients through the cardio-angiogram can lead to the inappropriate use of stents which are used to keep arteries open but carry risks like blood clots. It is estimated that around 30pc of interventional procedures following an angiogram may be unnecessary while 20pc, which might benefit the patient, are not carried out. Only around one in 10 patients have additional diagnostic tests after an angiogram to improve the picture so there is a need to give doctors more precise information. ""However, by integrating advanced technologies, such as artificial intelligence, we are able to see a lot of hidden information in the angiogram that is not possible to see now,"" he said. He revealed advances in artificial intelligence, machine learning and virtual reality will give more informa- tion within minutes to interventional cardiologists and a level of computer analysis not possible during conventional treatment. ""This will lead to a better understanding of the heart's condition, allow better patient outcomes, reduce the risk of adverse events and prevent later problems,"" said Prof Wijns. He said that blockages in smaller blood vessels can be identified from an image based on AI. ""It means doctors will be able to tailor the treatment to the particular patient. They can identify the best therapy for that individual patient,"" added Prof Wijns. ""The beauty of AI is that it's revealing hidden information which means it will be available to every patient in every cat lab in the world. No extra examination is needed."" Prof Wijns is among cardiologists and researchers who are calling for the use of these advanced technologies, some of which are available and others in trial, to improve patient outcomes. He is co-author of a paper on the topic along with colleagues from China, Italy, Switzerland and the United States in the journal Nature. Around half of patients who have an angiogram have sub-optimal functional outcomes, which leaves them at risk of problems in the short and long term. The application of advanced technology could push the success rate to around 80pc, said the professor. Around 10,000 people die in Ireland from cardiovascular disease annually, making up 36pc of deaths a year. However, around 80pc of all heart disease is deemed preventable through lifestyle changes and modifying risk factors. ""Additional research is currently underway to validate the effectiveness of these new strategies, and we are optimistic about the potential to revolutionise care of coronary artery disease and improve the lives of millions of patients worldwide,"" said Prof Wijns.",Ireland,Irish Independent; Dublin,2024-07-02,11,490,2024-07-01,2024.3,Medium,3,4,8.16326530612245,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,10.204081632653061,1,0,0,0,-0.4705249450618001,Negative
"Nearly 40pc of Irish businesses are now using special software to detect job applicants who may have embellished their job applications with artificial intelligence tools, according to a new survey published this morning. More firms are also intensifying their scrutiny of applicants during interviews to make sure they get what is described as a more ""holistic"" view of candidates who may have used artificial intelligence when making job applications, according to the survey from Irishjobs. The survey highlights the extent to which artificial intelligence is now permeating many aspects of modern life. It found that more than a quarter of employers in Ireland are using artificial intelligence tools in their recruitment processes. Irishjobs said 28pc of employers are now utilising the technology as its use gains traction among businesses. The survey found that 56pc of the firms using AI tools to hire staff have only introduced the technology within the past 12 months. Companies confirmed they are increasingly checking to see which prospective job applicants might be trying to embellish their job applications with the help of artificial intelligence. The survey notes that 39pc of firms have introduced AI content detection software into their hiring processes, and 20pc now have increased levels of candidate testing. Almost a third of recruiters said they are aware of jobseekers using AI technology during the hiring process. Just over 70pc of employers who are aware of candidates using AI have increased their focus on interview evaluation to ensure a more ""holistic"" assessment of candidates, according to the survey. Sam Dooley, the country director of Stepstone Group Ireland, who has responsibility for Irishjobs, said artificial intelligence is now ""transforming"" the recruitment sector. ""While we're still at an early stage in the lifecycle of AI, it's clear from our findings that the technology is having a significant impact in how employers attract and recruit potential talent - more than 1 in 4 organisations in Ireland are already leveraging the technology to drive change,"" he said. Mr Dooley added: ""As more jobs candidates experiment with these tools, it is unsurprising to see employers put an increased emphasis on more personal evaluation methods, such as interviews, to get more comprehensive insights into candidate performance earlier in the hiring process."" Of the firms that were surveyed, 39pc said that AI has resulted in a faster hiring process, while 38pc said it has improved the candidate experience of the recruitment process. The two main uses so far of AI in recruitment is for sending reminders and tracking candidates' application status. The survey found that 42pc of firms use AI for each of those facets of hiring. Another 39pc are using artificial intelligence tools to schedule job interviews. But firms are still working out exactly how beneficial AI might be in their recruitment processes. Just over half of recruiters - 51pc - said AI has made their job easier, but 46pc said it has made their job more complicated. The survey included responses from 250 senior leasers in businesses across the country. 'Over half of recruiters said AI has made their job easier'",Ireland,Irish Independent; Dublin,2024-09-02,19,512,2024-09-01,2024.3,Medium,4,0,0,0,1,1.953125,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.953125,1,0,0,0,-0.19135004527365787,Neutral
"A RTIFICIAL INTELLIGENCE (AI) is increasingly influencing every aspect of our lives. But trust in these systems, which is critical to their widespread adoption in society, is an issue that still needs to be addressed. AI is being used to make decisions, from whether a bank loan is approved to suggesting the next thing to watch on Netflix. It can make these decisions quickly and efficiently after learning from huge amounts of data. However, while using AI would seem to eradicate human bias, there have been multiple high-profile cases where AI-based decisions adversely affected individuals or groups. We have seen concern around algorithms used for predicting Irish Leaving Cert results. Facial recognition software has also come in for scrutiny for issues with racial bias. We have seen IBM exploring how it can develop tools to ensure online advertising algorithms do not only show ads to particular groups - such as men or wealthy people. In the US, it was claimed that the algorithm 'Compas', which is used to determine whether defendants awaiting trail are too dangerous to be released on bail, is biased against black people. This AI bias is one of the reasons why people are questioning their trust in AI systems, with many consumers not convinced that their data is being used in a fair and transparent way. And these are among the issues I have been looking at during my current research for CeADAR, Ireland's Centre for Applied AI (with industry partner Idiro Analytics) which involves auditing algorithms to detect bias using sociodemographic data. From its public consultation on the proposed European Commission Artificial Intelligence Act, the European Commission has opted for a mandatory, regulatory framework for high-risk AI systems. AI areas considered to be high-risk include medical devices, financial services, education, employment and law enforcement as well as critical infrastructure such as transportation, water, gas and electricity. Providers of such systems will have to undergo a conformity assessment and the system will have to bear the CE mark before being placed on the market in the EU. Non-compliance will come at a heavy price. Fines could go as high as €30m or 6pc of total worldwide annual turnover, depending on which is higher. But it is believed that providers of non-high-risk AI systems will only have to follow a voluntary code of conduct. The European Parliament and the Council of Europe could ratify the final text of this proposed legislation as early as this year. If it is adopted, it will immediately come into law in all EU countries. What is being attempted requires striking balances between trade-offs such as regulation and innovation for AI system providers as well as transparency and privacy for people affected by these AI systems. It is not an easy task but, with this draft legislation, the EU is looking to become the global leader in trustworthy, ethical and more human-centred AI. However, I believe the legislation might struggle to be effective coexisting alongside other legislation. For example, the regulation of social media is to be handled separately within the Digital Services Act despite the prominent use of AI on its platforms. I also question the ability of regulators to police the act. Who would be in charge of making sure the act is adhered to in each country and do they have the staff and resources to take on such a momentous task? Even still, passing this legislation would show citizens that their privacy is being respected and that all data is not being mined just for profit. It would also give people a sense that their data is being used in a fair and transparent way and, if it is not, they will have recourse to challenge those decisions - for example if an application for a bank loan were to be rejected. On balance, there is a genuine need for this legislation in order to bring humans back to the centre of AI. Dr Adrian Byrne is a Marie Sklodowska-Curie Career- Fit Plus fellow at Ireland's Centre for Applied AI. He is also lead researcher, AI Ethics Centre, Idiro Analytics. Fines could go as high as €30m or 6pc of total annual turnover",Ireland,Irish Independent; Dublin,2022-03-03,18,695,2022-03-01,2022.1,Medium,5,1,1.4388489208633093,1,2,2.8776978417266186,1,7,10.071942446043165,1,0,0,0,4,5.755395683453237,1,0,0,0,0,0,0,12,17.26618705035971,1,1,1.4388489208633093,1,1,1.4388489208633093,1,-0.4253199996437793,Negative
"rotex AI, a three-year-old Dublin start-up, has just completed a Series B funding round of $36m (€34.8m). P It brings to $54m the amount raised by the company, which employs 60 people and was co-founded by Dubliner Dan Hobbs and Limerick-born Ciaran O'Mara. The round was led by the UK investment firm Hedosophia with participation from Salesforce Ventures and existing investors such as the Irish investment house Elkstone. Protex's other investors include Notion Capital and Flexport. Protex AI makes a software platform that plugs into CCTV cameras and other sensors to offer on-site safety tips and accident-prevention analysis for factories and other workplaces. It counts industrial giants including DHL among its customers. The start-up will use the extra money to expand, particularly in the US, according to CEO Dan Hobbs. It's not the first foray into successful company-building for Hobbs, who sold a third level software start-up, Better- Examinations, to the tech firm Terminal Four in 2019. Hobbs spoke to the Irish Independent about how Protex AI is going, how artificial intelligence is now judged, and his own start-up journey so far. Adrian Weckler [AW]: Where did the company idea come from? Dan Hobbs [DH]: One of [co-founder] Ciaran's family members was working on a site and there was a fatality. They thought it was a freak accident, but when they went back through the CCTV, they saw some behaviour patterns, whether it was speeding forklifts or ergonomic issues, leading to it. They didn't know about this stuff because they didn't have visibility. AW: So Protex AI increases awareness of future accidents? DH: Sort of. Take near misses with forklifts, which can weigh almost two tonnes. When someone gets hit with one, people think they're freak accidents. But there are times when people are almost getting hit by forklifts again and again, a series of constant little near misses. If they're not identified and fixed, they're going to lead to something that could be a fatality. Think of it like trying to spot a car crash before it occurs. With our system, clients pick these movements up, and are able to do something as simple as to move something 10 feet to the left so that they're not in a situation where someone could almost die. AW: What sort of impact is it having? DH: On average, clients tell us that they see a 64pc reduction in risk, which we didn't think we'd get starting off. The biggest behaviour change isn't necessarily the workers on site or anything like that. It's actually how people are looking at the data. Usually, when an injury happens, it's only then that something is done about it, because that's when all the data points are looked at. What we're doing is flipping that on its head and saying, 'if we can look at the leading indicators, these small data points that lead to it, maybe we can stop things happening before they actually occur'. AW: You've raised quote a lot of money in a relatively short time. Was that hard? DH: We actually found it quite straightforward, to be honest. It was almost all inbound. We didn't have to go outbound at all. We weren't planning to raise right now, but we had a lot of VCs getting in touch with us. I think no matter the environment or the climate of the investment scenes, good companies always raise. I definitely think we're in that bracket. We were top tier when it came to being performance-tracked against other portfolio companies. AW: Is including the term 'AI' a supercharging effect when raising money? Or might it even be table stakes in 2025? DH: We're only three and a half years old, but we've now raised $54m. Which is a bit crazy and some of the reason for that is because of the AI, where there's this deep curve or adoption that's happening where people are taking to it a lot faster - and you don't have to oversell or over-engineer what you're talking about. I think the fact that that's happening is helping hugely with regards to our roll-outs and people signing long-term contracts with us. But I also think that it's moving away from just investing in any Gen AI company and more into applicable AI. We're seeing a lot more people be more targeted about where they're investing. AW: You're focusing squarely on the US market now. With all that's going on, how feasible is it to do that based in Ireland? DH: We want to build one of the biggest companies in Ireland and be headquartered here. We have operations in the US, and we're building out in the US a lot, but there is space for that. Like, you have the Intercoms of the world and you see Tines doing really well too. We've got an incredibly smart workforce in Ireland in this space as well. So we're really trying to utilise that. AW: You and Ciaran O'Mara have known each other since the BT Young Scientist event in 2010, when you were competing as 15-year-olds. DH: The cool thing is that we still go every single year. The idea [for Protex AI] actually came when we were at the Young Scientists. That event is so brilliant. It's a first taste of business and innovation. Even when we're interviewing people, we can tell if they've done the Young Scientists or not. Those who have have that kind of savvy that comes with being able to talk about a project. And just from a confidence perspective, they're all smart kids, so we love that. We go every single year, and we owe a lot to the Young Scientists for introducing ourselves to each other. AW: And what was participation in (start-up accelerator) Y-Combinator like? DH: It was great, although because it was during the pandemic, it was done from west of Ireland, rather than the west of the US. So we didn't get the glamour of San Francisco, it was more the glamour of Castletroy [Ciaran's home town]. ""If we look at these small data points that lead to an accident, maybe we can stop things happening before they actually occur""",Ireland,Irish Independent; Dublin,2025-02-06,15,1029,2025-02-01,2025.1,Long,6,2,1.943634596695821,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.9718172983479105,1,3,2.9154518950437316,1,0.015768585085379,Neutral
"Former Finance Minister Michael McGrath faced three hours of questioning in the European Parliament in his successful bid to become commissioner for democracy, justice and rule of law. Mr McGrath required a two-thirds majority from a voting committee of MEPs to be approved as commissioner. While some nominees have been rejected during this process in the past, Mr McGrath will be officially confirmed on November 21. The hearing was the final hurdle in the process of taking on the wide-ranging justice portfolio that includes responsibility for the protection of data, consumer rights, ensuring free and fair elections, media freedom and the tackling of corruption and disinformation. He is one of more than 20 nominated commissioners to face the three-hour questioning about their portfolios this week, after which representatives of various committees assess whether a candidate is qualified. Meetings to discuss the hearing between each political grouping took place yesterday, with Fine Gael MEP Regina Doherty saying Mr McGrath put in a strong performance and that her committee discussion after the hearing went ""very well"". During the hearing, Mr McGrath said there were ""many risks"" identified in the lead-up to the recent European elections, including disinformation and the threats posed by AI. ""The fact there was no disruption to the conduct of elections was remarkable,"" he said, adding that new proposals on foreign interference, disinformation and manipulation would be prioritised. Mr McGrath said the benefits of European membership should be conditional on compliance with the European Charter of Human Rights. He pledged to be an ""honest broker"" in addressing ruleof-law deficiencies in member states. ""I will not hesitate to act when appropriate to use the full toolbox of measures at my disposal,"" he said, adding that compliance is a ""must"" to gain access to European funds. The justice portfolio makes Mr McGrath one of the most senior watchdogs on the rule of law in the EU. His appointment takes place against the backdrop of a regression of democratic values in Hungary under prime minister Viktor Orban. Hungary currently holds the EU presidency. ""If we don't have a respect for rule of law, everything else falls apart,"" said Mr McGrath, adding that there could not be an ""a la carte"" approach from member states. One of his priorities will also involve wielding the EU digital rulebook to ensure social media influencers are not misleading consumers and that children are not manipulated online. He said he was dedicated to protecting children against threats posed by the internet, in particular by in-app currencies and features such as ""loot boxes"" within video games. He added that such features had a ""long-term impact"" and that the role of influencers in promoting things such as vaping and plastic surgery should be examined. Mr McGrath said measures would be developed to address the design of some online platforms and how they can lead to addictive use. Such features include infinite scrolling, autoplay and notifications that keep users on the site. ""I understand the business model of big tech - these firms want to keep people online,"" he said. Independent MEP Luke 'Ming' Flanagan asked Mr McGrath about the potential infringement proceedings that could be brought against Ireland due to an alleged failure to carry out market surveillance on defective products. Mr McGrath said people had been ""devastated by the defective concrete block issue"" and that it was important they received ""full remediation"". He said that on the basis of the evidence and a response from the Government, the commission would make a decision on whether a formal infringement proceeding would be brought against Ireland. He committed to introducing the first EU strategy against corruption and a new victims' rights strategy, adding that he would also work on anti-fraud, cross-border crime and strengthening the European arrest warrant. The Fianna Fáil TD said he would also like to look at the issue of dynamic pricing in the event ticketing sector. He added that increasing digital consumer protection would help increase competitiveness within the single market. He also addressed questions on illegal migrants and crime. ""I do not believe that we should seek to be divisive, to set one group in society up against another group,"" Mr McGrath said, adding that a crime by one person is not more serious than a crime by another ""by virtue of their identity or where they come from"". ""If we don't have respect for rule of law, everything else falls apart"" Michael McGrath",Ireland,Irish Independent; Dublin,2024-11-06,16,739,2024-11-01,2024.4,Medium,7,3,4.059539918809201,1,1,1.3531799729364007,1,3,4.059539918809201,1,0,0,0,0,0,0,0,0,0,0,0,0,6,8.119079837618402,1,0,0,0,0,0,0,-1.014093050212668,Negative
"Phil Codd 'tends to ignore the hype' around AI, but he is confident of big benefits in a number of sectors When McDonald's announced late last year that it would roll out generative artificial intelligence (AI) capability from 2024 at thousands of ""drive-thru"" locations in the United States, it became the latest chain to embrace the technology. Already, rivals there have been experimenting with the use of AI-driven chatbots to handle order taking at drive-thru locations and it marks just another application of a technology that will bring significant societal opportunities, challenges and change. Not least is the question of what human jobs, and in what sectors, are going to be most at risk of being automated. Finance is one that can likely expect continuing significant change, for instance. ""If you're not investing, then ultimately you're not going to grow; and if you're not growing, you'll die or you'll be consumed in some other way,"" cautions Phil Codd (63), the managing director of the Irish arm of Paris-listed consulting group Expleo, on businesses that don't embrace the fast pace of technological change. ""I tend to ignore the hype around AI,"" he adds. ""We see all the stuff about deepfake image creation, voice and music. The reality is that big benefits can be secured for medical research and delivery, for instance. We're going to see great strides in medicine."" Those benefits will extend to drug research and delivery, as well as to the development of medical devices, he believes. Codd points out that since last year, the Mater hospital in Dublin has been using AI in its radiography department to deliver rapid notification of suspected illnesses, freeing up staff for other work and enhancing patient care. ""It's being used as a second set of eyes,"" says Codd. ""That's where we start to see the real benefit. It's where it's really exciting."" But he also thinks that face-to-face contact will continue to be a core part of what some businesses offer, despite the use of AI in what have traditionally been customer-facing roles. ""We're still going to be looking for an experience,"" he says. ""The human race thrives on that. Maybe I'm old fashioned, but I still think there's that desire to see somebody real and get the reaction that ultimately cannot be created."" Indeed, it could be the jobs we thought might be most susceptible to being lost to the rise of AI that could see the greatest number of survivors. People do value human contact, especially when the unexpected happens, or we want advice on the restaurant menu or which drill in the DIY store might be best suited to the job. Even in other areas such as finance, where it has been predicted that AI-driven solutions would terraform a new landscape, there's been some pushback. AI is great for some facets of finance - such as in the world of hedge funds where the rapid analysis of vast amounts of data can give firms an edge. But many normal retail customers still want to talk to a human. A survey in the US last year of banking customers found that 46pc of them seek hu- man interaction in dealing with their providers. Among older customers that figure is even higher. ""Financial services are already highly automated,"" Codd says. ""They've invested in tech for 50 years. We're seeing how AI is very applicable in finance and banking in terms of optimisation and efficiency. In areas such as anti-money laundering, there are massive efficiencies that can be achieved."" In Ireland, Expleo employs more than 1,000 staff at offices around the country and works across sectors from telecoms and banking to energy and medical. In 2022, the Irish arm generated revenue of €69.2m in the Republic of Ireland, which was up from €51.5m a year earlier. Its pre-tax profit halved to €1.5m as it invested in the business. On an all-island basis, its revenue that year was about €76m. Revenues this year across Ireland will be in the region of €100m, according to Codd. That compares to about €86m last year. Its business spans consultancy services including product engineering and digital transformation, to complete transformation and the overhaul of supply chains and manufacturing processes. Among its clients has been stock market-listed Irish healthcare services firm Uniphar. Expleo helped the firm implement a new enterprise resource planning software product - SAP S4/HANA - designed to reduce future complexity and running costs. Expleo also helped the ESB to implement the SAP S4/HANA system. But on a company and group basis, Expleo's work is diverse. ""We've a project with one aviation client within our engineering division looking at sustainable materials and using a bamboo composite on an aircraft nosecone and also on aircraft interiors to replace plastics,"" he says. ""It's lighter and ultimately recyclable."" Such concept designs could have big implications for airlines looking to reduce fuel burn and overall running costs, for instance. The expected growth in the production of green hydrogen - which can then be used to power vehicles, for instance - is also an area of focus for Expleo and it's already involved in assisting in such programmes in Ireland, according to Codd. In an economy such as Ireland's, currently operating at what is effectively full employment, the potential benefits of AI are heightened. With it, it could be possible that such situations in the future don't pose as many challenges to businesses trying to grow, as the inability to recruit staff might not be as important an issue when roles can be automated and productivity enhanced using AI. ""To some degree, that's going to be a societal decision,"" Codd says. He adds that at any rate, flexible working and the fact that Ireland is now the only major English-speaking country in the EU, mean that Ireland's full talent pool lies both within and outside its borders. ""We can hire people and they can work on Irish businesses, but they don't have to be physically here every day of the week - they're in Spain, Portugal or France,"" he says. ""People in their 20s and 30s have a completely different view of the world than mine. ""The challenge is one of engagement and how do we make sure we can engage with all of our employees in the right way."" Meanwhile, Codd adds that the ""next iteration"" for Ireland is to become a hotbed forAI development. ""I think Ireland is really well placed to become a centre of excellence for AI,"" he says. Last year, OpenAI - the creator of ChatGPT - announced that it was opening an office in Dublin and would work with industry, start-ups and researchers to ""understand priorities and ideas for advancing AI development and deployment"". Earlier this year, Dublin City Council said it was partnering with OpenAI as the city becomes the European Capital of Smart Tourism for 2024. That collaboration has seen the pair work to demonstrate how AI can enhance visitors' experiences. The initiative included the proof of concept of an itinerary planner that can make bespoke travel recommendations, for example. But Codd says Ireland's education system is also going to have to adapt to the new reality. ""We've still got a lot more to do to encourage students into Stem (science, technology, engineering and mathematics) subjects, and particularly females into those subjects in secondary and third-level,"" he adds. ""It's going to have to find new ways to educate,"" he says. ""Attention spans have dwindled because people are bombarded with short, sharp pieces of information and we're very good at consuming those, but reading articles becomes a lot harder."" ""We're still going to be looking for an experience. Maybe I'm old fashioned, but I still think there's that desire to see somebody real"" ""It's being used as a second set of eyes. That's where it's really exciting""",Ireland,Irish Independent; Dublin,2024-05-09,23,1303,2024-05-01,2024.2,Long,8,1,0.7674597083653109,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,2.3023791250959325,1,5,3.837298541826554,1,1.123172633362203,Positive
"Ireland could benefit from a sharp rethink of the investment case for generative AI that helped fuel volatility which rocked financial markets this week, according to Grit Young, a partner in EY Ireland's M&A practice. While markets have recouped much of their losses from the early part of the week, one of the triggers for the initial shock was a reassessment of the likely timescale of financial returns on generative AI investments, which is set to have longer-term consequences, she said. After two rollercoaster years that drove anticipation of returns sky-high, triggering massive valuations for AI-linked stocks, expectations are starting to be reined in. ""Investors are reviewing their AI expectations and that is a good thing for Ireland because it means less pressure to drive returns quickly, which translates into a more considered investing pattern,"" Ms Young said. An early, large and somewhat frothy round of AI-driven venture capital investment in the wake of the ChatGPT launch in 2022 skewed heavily to US start-ups and was dominated by Silicon Valley's 'big tech' echo system, she said. That reflected a dash among some investors into a space they hoped to generate big and rapid returns. Investors still see huge potential in AI but the speed of anticipated return on investment is seen by many to be slower than initially hoped. ""A more considered, slower send - spread across a longer period - should be a good thing, including for Ireland,"" Ms Young said. The new EU Artificial Intelligence Act, a law that sets out rules for emerging AI technologies that came into force this week, coupled with the established GDPR privacy regime, means Ireland benefits from a clear regulatory framework to underpin investment, she said. While many in Silicon Valley have traditionally baulked at the regulation of new and emerging technologies, AI has proved an exception with widespread acceptance in the industry of a need for controls. Meanwhile, slower and more disciplined investment is likely to be more sustainable, she said. It also means investors including corporate managers are seeking greater detail on the use case for technology before committing capital into a space where a lot of data on how good models really are is still missing, she said. A more moderated investment environment will also give Ireland more time to upskill the workforce. Reduced AI valuations will also broaden the pool of potential buyers for technology, a market that had risked being cornered by cashed-up big tech firms and venture capital crowding out industrial players, including so-called 'acqui-hires' - buying start-ups for their staff and talent rather than for their products or business case. As the investment pool becomes more differentiated, there will be scope for some start-up businesses to command niche premiums, including in sectors like autonomous driving selling to established car manufacturers. ""Corporate buyers might pay a strategic premium if the case stacks up to buy technology, including for internal transformation,"" Ms Young said.",Ireland,Irish Independent; Dublin,2024-08-10,13,487,2024-08-01,2024.3,Medium,9,1,2.053388090349076,1,1,2.053388090349076,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,8.213552361396305,1,0,0,0,7,14.373716632443532,1,-0.6353217937594129,Negative
"A significant gap has opened up between multinationals and local Irish companies in deploying artificial intelligence (AI), new research has found. While half of employees in multinationals (MNCs) have access to generative (gen) AI tools, only 38pc of workers at Irish organisations do, according to a report by Accenture. The proportion of the workforce confident in basic digital skills is 14 percentage points lower in local firms. Meanwhile, the proportion of workers who say they have not received digital-skills training over the past two years is 17 percentage points higher in those Irish firms. Ireland has been slow in building the foundations for AI and the cracks are beginning to show, according to the report. ""A deployment gap is opening as organisations struggle to move their use of gen AI beyond proofs of concept,"" it says. ""Among those that have invested in the technology, 91pc have yet to scale its use across their business. One in three organisations believe their cloud capabilities are insufficient to leverage gen AI, highlighting the need to accelerate the modernisation of their technological foundations."" While 43pc of Irish employees have access to gen AI tools to support their work, only 24pc use them at least once a week and one in 10 are applying them to critical decision-making or high-impact analysis. A surge in digital skills training is urgently needed, the Accenture report says. Irish executives they surveyed estimate that 64pc of their workforces will require reskilling, the equivalent of over 1.7 million people. The authors point out that almost one in 10 organisations are using gen AI at scale, ""so we know it can be done"". Overall, AI has the potential to contribute up to €148bn to Ireland's annual GDP by 2038, which would be a 22pc increase over the baseline forecast. If leveraged effectively, gen AI could increase Ireland's average annual GDP growth rate for 2023-2038 from the baseline of 2.5pc to 3.9pc, according to the report. The biggest productivity gains, of up to 20pc, could be enjoyed by the life-sciences industry, but nowhere is the opportunity bigger than in the public sector, the report says. ""We estimate that 42pc of working hours in the Irish public sector (excluding healthcare) could be transformed by gen AI, either through automation or augmentation. ""This translates into a potential productivity gain of 12-18pc that, if realised, could result in €2.9bn in annual savings."" Accenture's modelling suggests the average Irish worker could regain 17pc of the working hours they spend on routine activities. A typical doctor could save five hours a week, while a commercial sales rep could save 12. The top three areas where organisations in Ireland have piloted generative AI so far are IT (71pc), customer service (68pc) and marketing (68pc). ""There is no guarantee the full potential for productivity and growth will be realised,"" the report says. ""Today, too few organisations use gen AI optimally and to amplify human abilities. But without a people-centric approach that empowers workers to perform higher-value tasks, rather than simply automating existing processes, €96bn in economic value could be left untapped by 2038 - an amount nearly equivalent to Ireland's total public investment in 2024."" Hilary O'Meara, country managing director of Accenture in Ireland, said Ireland stands at a pivotal moment in its AI journey. ""Achieving this potential will require collective action from the full ecosystem - government, business, and academia - to build an AI-skilled workforce and foster responsible innovation,"" she said. ""Given the remarkable pace at which gen AI is advancing and the size of the opportunity, we need to move quickly. Establishing a robust digital core, preparing the workforce and fostering a culture of continuous learning are essential and should be underscored by responsible principles to ensure data privacy, transparency, and fairness remain central in all implementations."" 'Irish executives estimate 64pc of their workforces will require reskilling, the equivalent of over 1.7 million people'",Ireland,Irish Independent; Dublin,2024-12-10,21,648,2024-12-01,2024.4,Medium,10,0,0,0,1,1.5432098765432098,1,2,3.0864197530864197,1,0,0,0,1,1.5432098765432098,1,0,0,0,0,0,0,0,0,0,3,4.62962962962963,1,4,6.172839506172839,1,0.3696027340957017,Positive
"Did Apple do enough to impress people in their iPhone 16 event on Monday? Will people buy a new phone for 'Apple Intelligence', their new artificial intelligence platform? And if that's the main draw, is the standoff between companies such as Apple and the EU enough to put off people from upgrading their iPhones in Ireland, at least for the time being? At this week's big reveal in Cupertino, there were upgrades and new models in three main categories - iPhones, AirPods and Watches. Broadly speaking, the upgrade to the iPhone 16 was the inclusion of 'Apple Intelligence' features and a new physical camera button. The new iPhone 16 Pro models got those items and a bit more, including larger displays (of 6.3 inches and 6.9 inches). The smaller Pro model got a new, longer telephoto zoom lens, while the larger Pro Max got better battery life. There were a few extra upgrades, too, such as more power under the hood via the A18 and A18 Pro chips and additional Ram memory (necessary to power the AI features), alongside better image-processing capabilities. And the individual cameras across some of the models got resolution upgrades: the ultrawide lens on the basic iPhone 16 has a new 48-megapixel sensor, while the video quality on the iPhone 16 Pro's cameras now shoot in 4K at 120 frames per second, allowing for ""cinematic"" slow motion. But the overarching theme was artificial intelligence. The 'Apple Intelligence' features set to start landing on iPhones from October will only work on iPhone 16 models. They include things such as writing tools for texts and emails, finding a photo by describing it or getting quick reminders of plans we made somewhere on our phone but can't remember, simply by asking a question in plain English about it. The camera will also be used through a feature called 'Visual Intelligence', which will return information based on taking photographs. Siri, the voice assistant, will also be a big part of this. ""I'm impressed with the Apple Intelligence stuff because it isn't super flashy,"" said former Evernote CEO Phil Libin. ""Apple is leaning into the toothbrush test - low-stakes, high-utility functionality that people will use multiple times a day. The AI industry needs more of this."" But the unresolved question that hangs over Apple's AI - and maybe the chances of the iPhone 16 driving a sales ""super cycle"" for the company - is European regulation. Under the Digital Markets Act, big tech companies such as Apple and Google don't yet know whether they are allowed to process personal data for the purposes of creating AI tools such as Apple Intelligence. What might stick in the craw of those of us in Ireland is that the 'Apple Intelligence' features will be launching in UK because of its post-Brexit, non-EU status. ""I'm really surprised at that,"" said Ben Wood, an analyst with CCS Insight. ""The UK is usually considered as being aligned with the EU from a regulatory perspective."" The gap between what an iPhone 16 can do, depending on whether it's used north or south of the Irish Border, may be the first visible example of what Mario Draghi was talking about when he excoriated the EU for putting barriers in the way of innovation for economic and technological progress. ""The EU's regulatory stance towards tech companies hampers innovation,"" he wrote in a report published on Monday. ""The EU now has around 100 techfocused laws and over 270 regulators active in digital networks across all member states. Many EU laws take a precautionary approach, dictating specific business practices ex ante to avert potential risks ex post. ""For example, the AI Act imposes additional regulatory requirements on general purpose AI models that exceed a predefined threshold of computational power - a threshold which some stateof-the-art models already exceed."" Draghi isn't the only one making this point. In a recent interview with this newspaper, IDA chief executive Michael Lohan made a similar observation, referring to the ""heavy burden"" on tech companies in an age of AI. ""It's probably going to require a situation where there are some process changes within the regulatory system to allow for some of these new innovations and technologies,"" said Mr Lohan. ""What has happened is that the US has probably gotten more agile in their processes. Whereas in Europe, we probably haven't."" For the time being, what this means is that Apple won't be able to properly develop 'multi-modal' artificial intelligence features - where images can be created from writing text and similar cross-functional features. And that means significant limitations on what is supposed to be the iPhone 16's killer feature.",Ireland,Irish Independent; Dublin,2024-09-12,13,773,2024-09-01,2024.3,Medium,11,1,1.29366106080207,1,0,0,0,0,0,0,0,0,0,1,1.29366106080207,1,0,0,0,0,0,0,8,10.34928848641656,1,3,3.8809831824062098,1,0,0,0,-0.4667313229415518,Negative
"IN PERSON CO-FOUNDER AND DIRECTOR, IDIRO ANALYTICS GERALDINE MAGNIER Many people are at risk of being left marginalised by the increasing use of AI, Idiro Analytics co-founder tells The artificial intelligence (AI) powering decision-making in the workplace is so potentially damaging it needs to be audited as closely and frequently as financial accounts, according to Geraldine Magnier. The co-founder and director of Idiro Analytics has spent two decades looking at the potential benefits and risks surrounding big data. AI can dramatically speed up processes and save time for businesses. However, Magnier believes that if the technology is left unchecked, it risks creating as many problems as it solves. ""Machine learning learns from historical data so if historical data is, I'm going to say, contaminated with bias, we're going to compound that problem,"" she says. Idiro's core mission is to produce products to eliminate bias in decision-making and promote the ethical use of AI. Over a cup of tea at Idiro's office, a stone's throw from Grafton Street in Dublin's city centre, Magnier cites an example of a project with DCU to assess which students are most at risk of dropping out. Determining patterns through data analysis then supported the university in creating solutions to pre-empt and prevent student departures. As AI entered the mainstream, Magnier looked at different types of data that companies were grappling with - data for good and the other kind - that had ""unintended consequences or disadvantages"" for some members of society, as she put it. ""If we look at recruitment scanning processes, if the person behind that AI tool for choosing particular applicants and CVs happens to have an unconscious bias in their own coding of a system, then maybe it might marginalise women, marginalise other ethnic groups,"" she says. This need to raise awareness about risks of reliance on the technology inspired the creation of the Ethical AI Centre by Idiro in the summer of 2021. The aim of the centre is to shine a light on the potential pitfalls of artificial intelligence if biases are not detected. Magnier wants to see AI systems that are accountable, responsible and respectful. This is achieved through regular monitoring and auditing of algorithms. It also involves the introduction of ""explainable AI"" from companies, creating transparency about what a system is doing and how it reached its eventual decision or outcome. Magnier is also insistent that humans remain linked to the new technology. ""Where is the human in this? Is there a human in the loop that is watching this, that is making sure that (the system) is staying in line with the intentionality?"" she asks. ""People don't realise that data is growing the whole time, so it's like something that has to be watched,"" she says. As the volume of data grows, so too does the need for increased monitoring. ""You can't just start off and do your auditing and go 'we have our algorithm set up now, it's good to go forever'. No, it has to be watched and monitored regularly,"" she says. Idiro has been around for 20 years but the current roll-out of AI has shone a spotlight on an area she says has been significant since the era of the original dotcoms. Idiro Analytics originally focused on telecoms companies, businesses that already held a large volume of data at the time. This was before AI reached ""buzzword"" status and advanced data analytics became viewed as a necessary step in understanding a company's potential impact. ""We probably spent the first decade educating the market,"" Magnier recalls. ""Ireland was a hard market to break through because we were in such high tech and we were doing something very unusual. ""Being pioneers did come with pain."" Now the business offers clients across a range of sectors, including education, insurance and financial services, guidance on implementing AI strategies, developing AI solutions and analysing data to boost overall performance. This includes private AI systems, similar to ChatGPT, but customised with a company's own data, which can be used for hiring and recruitment or content generation. It can also be used for external purposes, such as customer support. At this point, Idiro has analysed the data of more than 20pc of the world's population. While Magnier points out that a significant amount of that data is anonymous, it is still a mind-blowing number for a single Irish startup. ""The AI Ethics Centre was born out of a need to create awareness because sometimes policy doesn't catch up. Technology accelerates faster than policy,"" Magnier says. Policy makers are reacting. The EU AI Act, a regulatory framework proposed by the EU, is set to come into force at the end of 2023. This act looks to set out concrete rules which govern the development and use of AI across the EU, with penalties set to be enforced for non-compliance. The final agreement is expected at the end of this year or early next year. While chatter around AI has reached a fever pitch in recent months, Magnier says the business has not escaped the challenges faced by the tech sector as a whole. ""All of the tech sector took a hit in the last year,"" she says. The company currently employs 25 people, down 20pc from 2022. The severe lack of available housing in Dublin has also contributed to this decline in team numbers. ""The housing crisis caused an issue for some of our staff here in Dublin,"" she says. ""We try to have a hybrid working environment, purely because of innovation. Even though it's a tight market anyway, we were extra challenged with getting people because we wanted them to be somewhat available here,"" she adds. ""Living in Dublin is a challenge for a lot of the younger generation."" As businesses in Dublin and beyond also grapple with rising costs, she is also championing the cause of small businesses across Ireland in her voluntary role as chairperson of the Small Firms Association. ""(Small businesses) are the backbone of Ireland and they are the reason why towns and villages can remain with vitality."" Small businesses provide jobs in locations outside of the capital, she says, contributing to an all-important rise in spending in the local economy. Magnier points to her own hometown of Carrick-on-Suir, Co Tipperary, which now has a co-working space, Stable Lane Hub. This hub has 38 businesses working there, she tells me. Magnier's role at the association includes advocacy and policymaking. She points to the fact that these businesses, already time poor, require additional support in matters such as cyber-security and upcoming demands, such as mandatory ESG reporting and auto enrolment. ""A huge amount of regulation is coming. Small businesses are absolutely drowned in regulation."" AI also presents an opportunity for these businesses with fewer employees to lighten the workloads, according to Magnier. ""AI could manage to free up some of your energy to look towards your strategy and maybe your marketing. We can automate things like invoicing, finance. ""That would be an example of where some powers would be given back to the owner in the week,"" she says. I ask Magnier about speculation that AI will replace millions of jobs - in companies large and small - in the coming years. ""There will be displacement but I feel the displacement will be for lowskilled jobs, so the ethical aspect comes in again,"" she replies. ""If we are displacing people from jobs, then they must be given training,"" she continues. ""Let's look after people to upskill for a new age that we are in. We do need to start talking about AI literacy."" Originally, Magnier envisioned a career on the stage - the journey of a startup founder closely resembles the emotional highs and lows experienced in the life of a performer, she tells me. ""I actually found my creative spirit in the canvas of business. Becoming an entrepreneur is like becoming an artist - it is not dependable and is not predictable,"" she says with a smile. While she is wise to the potential dangers, AI may be a boon to creativity, if it means people spend less time sweating the small stuff and have more time to live their lives, she says. ""It will give people back some time,"" Magnier concludes. ""We are in a fastpaced world, we are human doings instead of human beings."" ""Ireland was a hard market to break through because we were in such high tech and we were doing something very unusual"" ""I actually found my creative spirit in the canvas of business. Becoming an entrepreneur is like becoming an artist, it is not predictable""",Ireland,Irish Independent; Dublin,2023-09-14,33,1435,2023-09-01,2023.3,Long,12,5,3.484320557491289,1,2,1.3937282229965158,1,9,6.2717770034843205,1,0,0,0,2,1.3937282229965158,1,0,0,0,0,0,0,6,4.181184668989547,1,4,2.7874564459930316,1,0,0,0,0.07625710403106667,Neutral
"Malicious fake images of a female politician circulated during the general election are being investigated by gardaí. The sexually explicit photos were created using AI deepfake technology to cast a slur on the victim's character. The smear campaign against the candidate with one of the main political parties is being treated as a cyber crime. The phoney pictures portrayed the woman as posing topless - and are viewed as the first incident of AI interference in an election in this country. The political figure was targeted by anonymous trolls sending the images on social media. She was running in the general election at the time and the harmful communications were an attempt to damage her prospects. She became aware of the images during the campaign and went to the gardaí. Her party is also aware of the disturbing incident. The Garda Press Office confirmed an investigation was under way into the origin of the material. ""Gardaí in the region received a report in relation to the distribution of a harmful communication in November 2024. Investigations are ongoing,"" a garda spokesman said. Local detectives are being helped by the Garda National Cyber Crime Bureau, which specialises in forensic examination of technology. ""Gardaí are making inquiries about it. What was the intent of it and what was behind it?"" a garda source said. The female political figure has been contacted for comment but has declined to respond. Distributing, publishing or sending threatening or grossly offensive communications is against the law. The victim is entitled to remain anonymous in such cases. The Harassment, Harmful Communications and Related Offences Act 2020 provides for prison sentences of up to two years if convicted. The law also specifically references intent to cause harm. But the incident also highlights the treatment of women running for political office in Ireland. Female politicians have spoken about being the subject of abuse on social media, harassment and intimidation. Brian Sheehan, CEO of Women for Election, an organisation which offers training and support programmes for women seeking to enter public life, said women disproportionately face these kinds of abuse. ""Tech poses increasing challenges to people in public life, including politicians - and especially female politicians and candidates,"" he said. ""It's yet another challenge women have to overcome when standing for election, even though we have had record numbers of female candidates in all elections. ""Recognising the threats, Women for Election has worked with gardaí through the election period to provide channels for support and reporting. ""There is a gender aspect to these tech-based assaults, whether in image or text form. They are often sexualised. Women disproportionately face these kinds of abuse, and it's becoming more apparent. ""It is a threat to our democracy if these forms of abuse deter people from standing to represent their communities. ""The onus is on the tech companies and the regulators to ensure safe participation in public life."" UCC political scientist Dr Theresa Reidy said such incidents are examples of AI being used to interfere with the voting process. The lecturer in politics has researched the issue in previous campaigns. ""This is one of the challenges to election integrity. This is one of the ways that AI can undermine our democracy and it is a particular problem for female candidates. ""Politicians in Northern Ireland and recently in the UK general election were affected by the circulation of deepfake porn videos,"" she said. ""These are designed to delegitimise women, to demean them and to discourage them from going into politics. They are specifically targeted at women, aimed at pushing them out of the public sphere - they are basically saying to women: 'You do not belong in politics.'"" While this is the first incident in the Republic of Ireland, a female politician in Northern Ireland has previously been targeted. Weeks before the Northern Ireland Assembly elections of 2022, anonymous trolls circulated a 40-second hardcore pornographic video clip, using an image of the SDLP's Cara Hunter. The MLA has spoken of being asked for a sexual act by a man in the street after the deepfake video of her was circulated. Ms Hunter has also described a text message she received in April 2022: ""You're a little hoare and we've all seen your little video."" The MLA has previously been the victim of sexual harassment, something she believes was used as a ""chauvinistic digital weapon"" aimed at reputational damage and derailing her electoral ambitions. The pornographic video clip was shared thousands of times on WhatsApp and was accompanied with false claims that she appeared in it. As a result, she received numerous unsolicited messages from men on social media. ""It got to the point where I was recognised on the street and approached by a stranger, who asked me for a sexual favour. ""For me, in the blink of an eye, everything had just changed. It was utterly humiliating. I didn't want to leave the house,"" she told a conference concerning AI last autumn. ""People think that they've seen you engage in a sexual act, so that somehow enables... their behaviour towards you to be openly predatory."" The SDLP politician said it impacted upon her canvassing. ""I was getting people smirking at me, sniggering. There was a lot of shame. I felt shame even though I hadn't done anything,"" she said. ""Even though it wasn't me, it may as well have been - because it was so widely believed."" The Oireachtas set up a cross-party taskforce examining safe participation in political life last year. A study examining abuse against TDs and senators found that 94pc of respondents experienced some form of threat, harassment, abuse or violence during the course of their work. A report recommended an opt-in social media monitoring service for TDs and senators, and a review of the security allowance available to mem- bers, which facilitates the purchase of equipment such as alarms and CCTV. The research was commissioned by the Oireachtas, conducted by University College Dublin, and was published with the taskforce recommendations. The targeting of female politicians with smears using AI is now happening in elections around the world. The Alan Turing Institute, a UK thinktank which studies data science and artificial intelligence, has monitored AI use in the various elections held across the world during 2024. The institute studied the impact of AI on the security of elections and on democracy more generally. ""Artificial intelligence was used in malicious ways in most major elections, but there is a lack of evidence that it measurably affected any election results,"" the institute said. ""We've seen AI-generated content designed to damage the reputation of political candidates, AI bot farms mimicking voters, and fabricated celebrity endorsements. ""We even saw incidents such as female politicians being targeted with deepfake pornography smears, harming their well-being and underscoring the discriminative gendered aspect with some of these threats."" ""These are designed to delegitimise and demean women, and to discourage them from going into politics""",Ireland,Irish Independent; Dublin,2025-02-15,27,1153,2025-02-01,2025.1,Long,13,18,15.611448395490026,1,1,0.8673026886383347,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,2.6019080659150045,1,0,0,0,0,0,0,-2.942718680915927,Negative
"In Fergal McCann's attic lies a glass box that featured for more than decade in illustrating one of Ireland's most notorious business/political sagas. It was the container which showcased Esat Digifone's mobile phone licence, most seen next to the champagne-swigging duo of Denis O'Brien and Michael Lowry, who would become embroiled in controversy for years over how the licence was won. Now chief technology officer at Eir, McCann had little knowledge as a 22-year-old junior engineer that the box would play such a central, if accidental, visual prop to one of the State's enduring industrial controversies. ""I took the glass box at the time as a piece of memorabilia,"" he says. ""I just thought it was something unique. And it was being disposed of, so I thought 'I'll have a piece of that.'"" McCann now keeps a collection of old mobile phones ""stashed"" in it, archiving one from each technical generation. ""I especially like old Nokias,"" he says. A family man with two sets of teenage twin boys, living near Castlebellingham, Co Louth, where his wife grew up, McCann's journey from Esat's birth to overseeing the technical side of Ireland's biggest operator has been a steady one. He worked under entrepreneurs Barry Maloney and Denis O'Brien at Digifone, and then under the more corporate Danuta Gray at Digifone's acquirer, Telefonica, before being appointed head of mobile networks at Eir in 2015. He now sits over what he says is Ireland's biggest mobile network. ""By far the biggest,"" he says. ""We have just short of 2,700 sites, which is 200 more than any of our competitors."" This, he says, is the result of a turnaround, based on €200m of investment and a ""complete rebuild"" of what former CEO Carolan Lennon once described as Ireland's ""third-best"" mobile network. Vodafone - for many years regarded as the country's best quality mobile operator - has been dethroned, McCann claims, due in part to not matching investment levels of rivals such as Eir. ""There are really only two networks in Ireland that have invested phenomenally over the last number of years, and that's us and Three,"" he says. ""If you forget about sponsored tests and look at the actual outsourced data, there are only two operators performing really well in terms of data quality and voice quality, and we're now focused on making sure that we're top of that pile."" On top of the €200m spend is another €100m coming down the line for mobile, he says, out of a total €2bn capital spending programme announced in 2019. Some of the fruits of that are being seen in Eir's mobile customer base, which has grown to 1.4m. While that's still some way off Vodafone and Three's 2m-plus bases, its cut-price GoMo subsidiary, which launched the €10-per-month budget mobile wars in Ireland six years ago, has swelled to 500,000 - almost 100,000 more than this time last year. McCann also claims that 5G is now available to 99pc of the population here, although that can't be independent tested due to testing resource shortcomings at Comreg, the telecoms regulator. ""If you look at where we were coming from, we used to be the third operator - with the poorest antenna position on many mobile sites around the country. So we decided to do a complete refresh and built 850 new sites."" The build-out has not been without problems. This month, a mobile site in Ringaskiddy was attacked several times, causing an estimated hundreds of thousands of euro of damage, McCann says. Although the incident echoed attacks during the pandemic, when conspiracy theorists whipped up community anxiety over imaginary health problems from 5G, McCann says that there was no clear motive for the attack. ""It happens from time to time,"" he says. ""It's going to be a full rebuild, costing many hundreds of thousands - and I've no doubt that people in the area will notice the deterioration in service. But to be fair, that's fairly rare these days."" Doubling down on 5G appears to be Eir's main objective, with a 2Gbs speeds in preparation. But 6G will not arrive ""for years"", McCann predicts. ""There's no real use case for it yet that we can see,"" he says. ""It will come - make no mistake. But not for years yet."" As the network-building tech person, McCann has avoided most of the controversy over customer service woes that plagued Eir during Covid (and elicited multiple apologies from successive CEOs). Though he has a key role in what could be a controversial event for the telco in years to come. Ireland is due to begin a switchover process from existing copper telephone line infrastructure in the coming years. This is likely to start in earnest when the National Broadband Plan is completed in 2027, with around 550,000 homes and business hooked up to proper fibre, separate to Eir's own projected fibre broadband buildout of 1.9m premises. But there are still 800,000 active copper line services in Ireland, including hundreds of thousands of home alarms, business services and even elevators. Eir's sales staff are already using the ""copper switch off"" line with broadband customers. So how soon does Eir think it will happen? ""I'd say it won't be until 2030, maybe even a little bit beyond that. The challenge will be to make sure we can bring every customer with us, because we can't leave any behind. And we have to engage the wider industry to make sure that solutions they're putting in are properly migrated and swapped. ""But it won't be a long process when it does finally kick off. We've done a submission to Comreg in recent weeks, so we're waiting for feedback to review that. But we're keen to get on that journey."" McCann says working for Xavier Niel's NJJ reminds him of the earliest days at Digifone, where the culture was fast and agile. ""They [NJJ] have the ability, the appetite and the energy to make decisions quickly,"" he says. ""The agility is phenomenal. And there's lots of information sharing as well. ""We take the lead on quite a lot of products and services as well, in terms of development, like wifi 7 or android TV. They're not shy to utilise Ireland as a test bed."" Despite the cutting edge, McCann doesn't see AI replacing staff at the company anytime soon, unlike some big tech and accounting firms. ""We're using AI, and we'll continue to do so, but it's not displacing roles. It's not dissimilar to previous technology changes, where you just have a shift of resourcing. ""We're still taking on graduates here, because they're the next technology leaders. So my view is that we need that. We will need them."" ""We are using AI and we will continue to do so - but it's not displacing roles. It's not dissimilar to previous technology changes""",Ireland,Irish Independent; Dublin,2025-07-17,9,1138,2025-07-01,2025.3,Long,14,1,0.8787346221441125,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.8787346221441125,1,1,0.8787346221441125,1,2,1.757469244288225,1,-0.9322314582026552,Negative
"Meta has said it will not sign up to the EU's AI code of practice, claiming it represents regulatory over-reach and ""introduces legal uncertainties"" for companies trying to build AI services in Europe. The move comes just before the next phase of the AI Act's enforcement, on August 2. ""Europe is heading down the wrong path on AI,"" Joel Kaplan, chief global affairs officer at Meta, said. ""We have carefully reviewed the European Commission's code of practice for general-purpose AI models, and Meta won't be signing it. This code introduces a number of legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act."" OpenAI has said it will sign the code, which is a voluntary set of principles intended to help companies comply with EU law. While there is no legal requirement on companies to sign up, the commission has indicated that it would be an advantage for the purposes of signalling trust and a commitment to ""ethical AI"", which could be weighed in public-procurement contracts across the EU. Companies that do not sign may also come under closer scrutiny by regulators for signs of non-compliance with the law. Mr Kaplan claimed that European businesses are largely against the EU's AI Act, pointing to a letter signed earlier this month by 44 of Europe's largest businesses, including Bosch, Siemens, SAP, Airbus and BNP. It called on the commission to pause implementation. ""We share concerns raised by these businesses that this over-reach will throttle the development and deployment of frontier AI models in Europe and stunt European companies looking to build businesses on top of them,"" Mr Kaplan said. In Ireland, some start-ups have complained that the AI Act prevents them from planning products and services that can compete with companies outside the EU. The Dublin-based founder and CEO of Naptha.ai told the Irish Independent's Big Tech Show podcast he felt forced by legal and regulatory uncertainty in Ireland and the EU to register the AI start-up in Singapore. ""In the circles I'm in, the perception of the EU is that they're going to be too restrictive and go too far when it comes to regulating AI,"" Dr Richard Blythman, a veteran start-up creator, said. ""It's the lack of clarity about what might be permissible. AI start-ups building in Europe are at a disadvantage compared to AI start-ups building internationally because you are more restricted in terms of building on top of one of the leading open-source models."" Last September, Stripe co-founder Patrick Collison and other senior European tech figures wrote an open letter to EU regulators and policymakers warning that Europe faced industrial stagnation through overzealous AI regulation. It referred to the report by former European Central Bank president Mario Draghi which argued that over-regulation was strangling EU industry. Mr Blythman said that the problem of regulatory uncertainty is a widespread fear in Irish start-ups. ""I've spoken to other founders that are registering in the US and it's just really easy for companies to move around the world. It's not so much the specific details of certain regulation, it's just a perception that Europe is not a good place to build an AI start-up,"" he said. ""AI start-ups in Europe are at a disadvantage compared to AI start-ups building internationally""",Ireland,Irish Independent; Dublin,2025-07-19,17,549,2025-07-01,2025.3,Medium,15,0,0,0,0,0,0,1,1.8214936247723132,1,0,0,0,0,0,0,0,0,0,0,0,0,10,18.214936247723134,1,1,1.8214936247723132,1,0,0,0,-1.0794725337649282,Negative
"US artificial intelligence (AI) firm Anthropic is opening an office in Dublin, it has confirmed. The company - backed by Amazon, Google and Salesforce - is one of the hottest firms in the AI sector, which has seen a surge in growth and investor interest in the past couple of years. Anthropic's fundraising spree in the past year has seen it valued at $15bn (€17.5bn). It is currently generating monthly revenue of about $8m, The New York Times reported last month. The San Francisco-headquartered firm, which employs about 160 people, said it intends to establish the presence in the coming months, having opened a small office in London last year. ""Our Dublin office will be our first EU office and we hope to have Ireland as our main establishment,"" said a spokesperson for the company. They added that Dublin is a leading European tech hub and boasts strong connections to AI centres such as London, Paris, and Berlin. ""Its diverse talent base, thriving startup sector and concentration of major tech firms make it the ideal location from which to grow our European operations,"" said the spokesperson. ""Over the coming months, we will hire staff across a range of sectors including legal, sales and people operations before expanding our recruiting plans to other areas."" Anthropic, founded by Dario Amodei and Daniela Amodei, bills itself as an AI safety and research company. Last May it raised $450m in funding from investors including Google and Salesforce. Later last year, it secured $4bn from Amazon, with Google then stumping up an additional $2bn. Last month, venture capital firm Menlo Ventures injected $750m into the company. Its flagship product is Claude, an AI assistant that it says is used by millions of businesses and consumers due to its emphasis on safety and performance. Claude.ai is not currently available in the European Union, though the company aims to roll it out in the region soon. Earlier this month, the company unveiled a new version, Claude 3, which provides AI models built for enterprises. There are three models - Claude 3 Opus, Claude 3 Sonnet and Claude 3 Haiku. Anthropic says the platforms set new industry benchmarks across reasoning, maths, coding, multi-lingual understanding and vision quality. ""Ireland offers a wealth of executive talent experienced in scaling businesses across the EU, as well as a strong workforce in science and technology, which we plan to leverage as we expand our operations,"" added the Anthropic spokesperson.",Ireland,Irish Independent; Dublin,2024-03-20,13,407,2024-03-01,2024.1,Medium,16,2,4.914004914004914,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2.457002457002457,1,-0.08269561807735355,Neutral
"Artificial intelligence technology to sniff out bribery, kickbacks and corruption within companies is being launched in Ireland by the legal firm DLA Piper. The technology, called Aiscension Bribery, claims to detect multiple forms of bribery behaviour, including ""facilitation payments"" and the circumvention of internal procedures. According to DLA Piper, whose lawyers helped to train the technology in what to look for, the software uses behavioural analysis across reams of company data, including email, chats and other kinds of communication available legally to the firm, to detect the corruption. It's aimed at being used in investigations or audits of staff behaviour, although it can also be used in a more consistent monitoring way. It is being launched in Ireland because Dublin is seen as a ""node"" for large US and European companies that do business in parts of the world that are regularly cited for egregious corruption regimes. Internationally, fines for corruption and bribery can run into the hundreds of millions. Earlier this year, Ericsson agreed to a €180m bribery fine in the US for using outside consultants to bribe officials. The Swiss mining company, Glencore, has so far been fined around €1.5bn for bribing African officials. The Aiscension Bribery software is claimed to speed up investigations or compliance reviews by a factor of 10 and ""triangulate"" data in a way that human reviewers can't to ferret out corrupt patterns and anomalies. ""White-collar crime is becoming more common in Ireland,"" said Brian Hunt, legal director at DLA Piper. ""The potential reputational damage to a business if faced with an allegation of bribery is significant. ""This utilises the power of Al to analyse vast amounts of data in order to unearth potential and real breaches."" A spokesperson for the firm added that the AI technology was intended to be used as a ""screening"" process. A previous version of the software, which was more focused on competition breaches, was trained to look for repeated use of code names and other suspicious terminology among correspondence between key executives or sales staff. ""There are a lot of tools out there in this area that call themselves advanced,"" the spokesperson said. ""The output here is very impressive compared to a more hybrid manual review that relies on existing discovery tools.""",Ireland,Irish Independent; Dublin,2023-08-25,9,374,2023-08-01,2023.3,Medium,17,0,0,0,1,2.6737967914438503,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,5.347593582887701,1,1,2.6737967914438503,1,0,0,0,-1.4299643435353724,Negative
"Dr Richard Blythman, co-founder and chief scientific officer of Naptha.ai, wanted to register his $6m AI start-up in Dublin, where he was born and where he lives. But he felt he couldn't. The rules here are too uncertain, he says. So he registered it in Singapore. According to Blythman, a veteran start-up creator whose idea of decentralised infrastructure for AI could be tapping into a new trend in the industry, his isn't the only Irish start-up choosing to register outside Ireland and the EU. ""In the circles I'm in, the perception of the EU is that they're going to be too restrictive and go too far when it comes to regulating AI,"" he says. ""It's the lack of clarity about what might be permissible. AI start-ups building in Europe are at a disadvantage compared to AI start-ups building internationally because you are more restricted in terms of building on top of one of the leading open source models."" He cites Meta's AI model, Llama, as an example. ""That's one of the leading open source models, but they released it in Europe with a much more restrictive licence compared to the US. Llama's model is quite permissive in letting you commercialise whatever you build on top of it. ""But in Europe, the license was more restrictive so that you couldn't commercialise it because Meta was worried about the implications of the EU AI Act. ""They were worried they may be held liable for any harmful applications that were built on top of it. Rather than take that risk, they just decided to basically shut down commercial activity on top of Llama so that they won't be held liable in Europe."" This is a well-worn complaint from the tech industry. Last September, Stripe co-founder Patrick Collison and an array of other senior European tech figures penned an open letter to EU regulators and policy makers warning that Europe faced industrial stagnation through overzealous AI regulation. The letter cited a critical report from former European Central Bank president Mario Draghi which argued that over-regulation was strangling EU industry. Cabinteely-born Blythman's AI startup, which recently raised a $6m (€5.2m) funding round from international venture capital firms, is one of the first to bear these warnings out. Blythman, who holds a PhD from Trinity College and has two other start-ups under his belt, doesn't think that his experience will be the last. ""From what I know, there are quite a few start-ups that are thinking the same thing,"" he says. ""I've spoken to other founders that are registering in the US and yeah, it's just really easy for companies to move around the world. It's not so much the specific details of certain regulation, it's just a perception that ""Europe is not a good place to build an AI start-up. In our case, we didn't know the specifics of how we would be restricted, but the direction that regulation was going and the overall consensus on AI in Europe led us to believe that we would be restricted in future if we built here."" Naptha's situation creates an ironic situation for Ireland, a country that is known for attracting tech companies headquartered elsewhere because of favourable regulatory or fiscal conditions here. In Blythman's case, it's now the opposite case: the brains reside in Ireland while the corporate scaffolding to allow the company's activity is based in another jurisdiction by competitive necessity. ""We're in an era where it's easier than ever to start a company abroad while being in Ireland,"" he says. ""We do need to go out to Singapore once a year for an AGM, which is an inconvenience, but it's not a huge difference compared to my start-up being registered in Ireland. So the friction with registering elsewhere has never been lower."" What about the argument that the EU is trying to be a civilising brake on things? That AI has the capacity to cause real problems and that Europe is the only democratic jurisdiction that is trying to put in place proper rules to avoid some of the excesses - not to mention political and cultural polarisation - that recent waves of technology have helped to foment? ""Sure, but how effective is that going?"" Blythman says. ""I mean, those [AI] applications are still going to get built in the US. The European Union is going to be successful at stopping those applications being built in Europe. But they're mainly going to do that by stopping all, or most, applications being built at all in Europe. I think there's another way to do it and to prevent these scary use cases of AI. But it's not by regulating, it's by building different types of technology that enforces the constraints that we want for our values. And I think there are ways to do that. If you look at GDPR pop-ups that are supposed to protect your data, how effective has that been? We've just added these pop-ups on top. But there are ways to build technology that actually keeps user data private without having these pop-ups."" Blythman cites Draghi's report, as well as recent comments from French president Emmanuel Macron, as hopeful signs that there is recognition in Europe of change needed to regain confidence in the AI sector. ""But it's still up in the air about which side will win,"" he says. And he says that while big US tech companies will always sort themselves out, the rules could be damaging to local Irish firms trying to set out in the industry. ""This is at the detriment of local startups who, I think, don't get enough attention compared to the big tech companies,"" he says. ""Ireland's approach has always been to focus on attracting big tech to Ireland. But we need to start thinking about Irish start-ups in AI now too."" To hear the full interview, listen to today's episode of the Irish Independent's Big Tech Show podcast on any podcast platform",Ireland,Irish Independent; Dublin,2025-06-26,21,989,2025-06-01,2025.2,Long,18,2,2.0222446916076846,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.0111223458038423,1,8,8.088978766430738,1,0,0,0,0,0,0,-1.2430787401353678,Negative
"Daniel Izquierdo Hijazi is well aware that artificial intelligence has become the buzziest phrase of the moment in commerce. He uses it himself. ""It's become fashionable to say: 'We use artificial intelligence in our business',"" says the 24-year-old chief executive of Micron Agritech. With grand promises of AI revolutionising virtually everything in a high-tech future, Micron is instead focusing on the practicalities of how it can be used in the here and now. The Dublin-based firm is using it to analyse medical scans, long expected to be one of the ways which AI would prove to be most useful. However, what sets the company apart is that its model is trained in looking at animal scans, rather than those of humans. ""The test is for gastrointestinal worms which live in the guts of animals. Currently samples are looked at under microscopes to identify parasites, so you have someone physically looking at one and counting the parasites up."" Izquierdo Hijazi says. ""What we do is get images of a sample which gets uploaded to a server and then our model is trained to look for the parasite eggs and count them."" Sending off a sample to a lab and waiting for test results to come back can take around five days. Micron's test, which is done by a vet using a mobile app testing kit and app, can be done on site in around five minutes. The convenience and reliability of the firm's analysis has been a key attraction for investors, helping Micron last week close a €2.7m funding round. Those participating included a variety of prominent technology backers such as Act Venture Capital, Atlantic Bridge and Yield Lab Europe. Izquierdo Hijazi says the capital could be considered seed funding, although the company ""didn't assign a name to it"". Founded as a spin-out of TU Dublin in 2019 by Izquierdo Hijazi, Tara McElligott, Sean Smith and Jose Lopez, the company has already expanded to 16 staff. It now has major plans to grow, as it plans to use the €2.7m raise to crack the UK market before moving into continental Europe. The main issue Micron was established to tackle, says Izquierdo Hijazi, is that farmers routinely medicate their animals as a preventative measure, rather than regularly test where they will have to wait days for a result. This can lead to the development of parasites which become resistant to medication, which can become a health hazard to both animals and humans. ""To counter this, we developed the Micron kit so vets can test on site and get results in minutes. It increases the efficiency of the animals and makes them healthier,"" Izquierdo Hijazi says. The technology itself is an artificial intelligence model which was trained by Micron. ""We have a team of data scientists which do machine learning training. We collect the pictures of samples and then someone who has expertise of parasites manually allocates the eggs in every single frame. We've done that hundreds of thousands of times,"" Izquierdo Hijazi says. The firm participates in industry standard testing to verify the accuracy of its results, which it says are ""at least as accurate"" as a traditional lab. Izquierdo Hijazi adds the company's testing will become even more accurate over time as its algorithm improves. ""The machine is like a child that learns. The more tests that get done on our system, the more accurate the model gets."" The way the firm's business model works is the test kit itself pairs with a phone. Once bought, vets then buy 'test packs', where one 'credit' allows the professional to carry out one test. While the tests could technically be done by farmers themselves, Micron says there are several benefits to keeping testing in the hands of the vets. ""The system is designed so anyone can do it, but vets are in the best position to educate farmers on how the tests should be used and how the animal should be medicated,"" Izquierdo Hijazi says. ""We want to move farmers away from giving all their animals medication on a blanket approach. Farmers trust vets with the health of their animals, so they're an important person to have in the loop."" The UK market is the next major step for the firm. The firm is looking to hire a software engineer in the region and Izquierdo Hijazi says he is eager to hear from those who would be interested in working for the company. ""We've achieved high penetration in Ireland, we're processing high volumes of tests, so now moving into the UK and Europe is the focus - the €2.7m is to do that,"" he says. ""We've just launched with (animal health products firm) Bimeda, which will be our distributor in the UK market. ""We're committed to having the head office in Ireland, it has worked really well for us, but we will have a local presence in each key market. They will be on the ground supporting our customers, the vets or whoever else it may be. ""We have the first hire already in the UK and that will grow. We will likely do the same in the European regions where we would have a local partner. ""The focus for the rest of the year will be growing in the Irish and UK markets and we will also be scoping out the European markets for entry there next year."" Currently, Micron is focused just on developing its intestinal test, which is targeted at commercial agriculture. However, longer term there is no reason why the firm's technology could not be used for testing for a range of diseases in a variety of animals. ""We will also eventually look at companion animals, not just agriculture,"" Izquierdo Hijazi says. ""But the big thing for us is, how do you balance the need for producing more food from animals while having less of an impact? We believe the way to do that is through technology and improving animal's health."" ""The machine is like a child that learns. The more tests that get done... the more accurate the model gets""",Ireland,Irish Independent; Dublin,2023-09-28,13,1012,2023-09-01,2023.3,Long,19,1,0.9881422924901185,1,0,0,0,0,0,0,0,0,0,1,0.9881422924901185,1,0,0,0,0,0,0,0,0,0,1,0.9881422924901185,1,1,0.9881422924901185,1,0.9260304245009218,Positive
"Retailers can overcome some of the challenges they face — such as customer retention — with innovations that enhance engagement and improve the overall customer experience. Joe Walsh — Chief Commercial Officer Ireland at global outsourcer Capita — is passionate about retail. He's equally passionate about helping retailers succeed in a competitive marketplace, with an approach that blends people, service and technology. Positive customer experience and personal interaction Walsh notes that one of the biggest issues is customer drop-out rates on retail websites. This occurs when shoppers can't find what they're looking for immediately — or have what they perceive as a negative online customer experience. That's why proactive and personalised communications are so important via, for example, pre-sales product videos and video call support facilities. In short, great customer engagement can be a deal-breaker. ""The customer journey is increasingly moving online, particularly for Gen Alphas and Gen Zs,"" he says. ""They want a personalised experience. They want a conversation."" A full 60% of consumers prefer video support for complex issues, indicating that video interactions can significantly enhance the customer experience and lead to increased retention rates. How AI can transform the retail landscape Another challenge for retailers is managing high volumes of contact during peak seasons. 'Pinch point' issues include staff availability, store and website capacity and maintaining good customer experiences. Walsh highlights that Capita can help by providing extra resources during these busy periods and by ensuring that a retailer's online channel is engineered correctly to deal with an activity surge. ""Website responsiveness is crucial,"" says Walsh. ""Because the attention span of consumers can be measured in seconds."" Walsh sees a sharp increase in AI adoption among retailers to, for example, manage high volumes, capture and analyse customer data to gain valuable insights for improvement and generally ensure smooth customer experiences. So, he advises retailers to set up innovation areas where any type of new tech can be tried and tested. ""Don't be afraid for it to fail,"" he says. ""If it works, deploy it quickly. Essentially, find out what the future of retail looks like for you — and then invest in it.""",Ireland,Irish Independent; Dublin,2024-08-29,11,356,2024-08-01,2024.3,Medium,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2.8089887640449436,1,0,0,0,1,2.8089887640449436,1,0,0,0,0.498941471131408,Positive
"Power shortages mean the country will have to choose between artificial intelligence (AI) and housing, a senior government official has warned. Oonagh Buckley, secretary general at the Department of Environment, Climate and Energy, said data centres had used up all spare electricity capacity and it was necessary to prioritise demands. ""We're having to even think about prioritising what is the social need of the demand - is it housing or is it AI?"" she said. ""We're going to have to think much more about managing demand."" Ms Buckley's comments come just days after Taoiseach Micheál Martin once again talked up the importance of data centres and AI and hit out at critics of the Government's expansion policy. Mr Martin admitted to a business gathering in Kerry: ""The AI revolution is going to pose challenges in terms of energy use."" However, he complained: ""We've had a relatively negative domestic debate on data centres which I think has been ill-informed and hasn't had perspective."" His comments echoed complaints he made in Washington last March when he accused critics of ""demonisation of data centres"". However, the remarks by Ms Buckley reflect concerns expressed by multiple external bodies. The energy regulator, the Sustainable Energy Authority of Ireland, EirGrid, ESB, the Climate Change Advisory Council, energy experts Professor Hannah Daly and Dr Paul Deane of UCC, and environmental groups have all spoken out. The regulator, the Commission for Regulation of Utilities (CRU), in particular highlighted concerns that data centres had usurped electricity put in place with the intention of supporting future housing. Ms Buckley's comments come at a critical time as the CRU is expected to decide within weeks what policy or possible restrictions should apply to new data centres seeking connection to the electricity network. Existing data centres are using 21pc of all electricity consumed in the country. In the Dublin-Meath region, the amount is 50pc. EirGrid expects the figure nationally to be 31pc by 2030. That is only for data centres with connection agreements and does not include many more at proposal stage that the Government would like to see built. Ms Buckley was speaking at an event for a new organisation, Trifecta Ireland, which aims to push for an energy policy that better pursues the aims of being clean, secure and affordable for all. She stepped in to replace Minister Darragh O'Brien, who was due to address the event but had a family bereavement. She spoke of the obligations on Ireland to reduce energy use under the EU's Energy Efficiency Directive, saying this would be ""very challenging"". EirGrid recently forecast that the country's electricity demand would grow by 45pc over the next 10 years. ""We are looking at a level of power demand that exceeds anything that we've been able to cope with before,"" she said. ""I've no problem with data centres - it's just that they consume vast amounts of power. ""Developer-led data centres cannot work any more because we can't provide the grid as fast as they need it so we're going to have to think in a more plan-led approach. ""We're going to have to think more about matching demand and supply."" The issue comes as Uisce Éireann also ramped up pressure on the Government over housing, warning that it does not have the resources to provide enough water to support all the new homes promised. Asked about Ms Buckley's remarks, the Department of the Environment acknowledged the competing electricity demands of data centres and other forms of development. It said: ""The minister is aware of the challenges around facilitating data centre connections in constrained regions, which can lead to grid constraints on the local distribution network impacting other customers' connections, including households and other commercial customers."" It said the minister was ""engaging with stakeholders"" on the upcoming decision from the CRU. The minister later doubled down on his commitment to data centres. ""We have been clear about our desire to facilitate further FDI [foreign direct investment] and indigenous investment, including the need to facilitate new technologies and data storage,"" he said. He said the Government was also prioritising ""increased and unprecedented investment in our grid and expanding our energy generation"". The Taoiseach's office said: ""The Programme for Government is committed to scaling up investment in critical infrastructure and in our electricity grid, along with addressing the challenges of the growth of data centres in a sustainable manner.""",Ireland,Irish Independent; Dublin,2025-05-29,15,729,2025-05-01,2025.2,Medium,21,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,8.23045267489712,1,1,1.371742112482853,1,5,6.858710562414266,1,-0.7827260756356216,Negative
"The latest call under the Government’s €500 million Disruptive Technologies Innovation Fund (DTIF) is now open for applications. Administered by Enterprise Ireland on behalf of the Department of Enterprise, Trade and Employment, the fund aims to encourage collaborations between industry and the research sector for the development and commercialisation of ground-breaking technologies and seed a new wave of start-ups. More than €370 million has already been allocated to 104 successful projects approved under the six previous DTIF calls since 2019. These projects cover a wide range of societally important areas including life sciences, medical devices, ICT, manufacturing and the environment. They also include 46 projects involving artificial intelligence which were awarded a total of €162.5 million between them. “What we look for is projects that are developing novel technologies that have the potential to disrupt the market,” explains Imelda Lambkin, disruptive technologies, innovation and knowledge transfer manager with Enterprise Ireland. “We are looking for exciting projects that are at the forefront rather than making incremental steps forward. Funding of €1.5 million and above is available for successful projects.” To be eligible for funding, each project team must have a minimum of three independent partners including at least one SME and one other enterprise partner along with a research performing organisation. “Each project has at least two or three companies coming together with an academic partner,” Lambkin notes. “Sometimes more partners are involved. We have high potential start-ups [HPSU] companies right the way through to multinational corporations like IBM and Analog Devices involved in projects along with high-end academics working with the teams. It’s the combination of partners that makes the difference. Everyone brings something to the table. The projects wouldn’t be successful without that.” One of the projects funded in a previous call is E-crop, which is developing tools to monitor soil and crop health, thereby improving crop productivity with reduced chemical inputs. The team comprises University College Dublin with industry partners E-Seed Crop Technology Solutions, SeqBiome and Lárionad Acmhainní Nádúrtha. Mincon Group, Subsea Micropiles and University College Dublin are working on micropile, a prototype robotic drilling system and anchor template for the installation of micropiled anchor foundations that will be load tested to prove suitability for a wide range of applications including offshore wind. In the food sector, Dawn Farm Foods, Creme Software and University College Dublin are working on Perception, a software platform to predict the human sensory quality attributes of food products for taste, health benefits and minimal waste. PerPETual aims to reduce landfill and incineration of waste plastic by creating a technology that will allow polyethylene terephthalate (PET) plastics to be continually recycled and converted into valuable resources. The project is led by researchers at TUS, in partnership with University College Cork, AvonCourt Packaging and Novelplast. The QCoIr project is developing a software platform for quantum computing technologies. The project team comprises IBM Ireland, Tyndall National Institute, University College Dublin, National University of Ireland, Maynooth, Rockley Photonics Ireland, Equal 1 Laboratories Ireland, and Mastercard Ireland. “Projects funded under the DTIF are looking at everything from crop health to quantum computing,” Lambkin notes. “A point we often make is that the projects cover everything from energy infrastructure at the bottom of the ocean to optical communications in outer space.” She points to other benefits from the programme. “One of its strengths is that the HPSUs learn how to interact and engage with large multinationals and academic partners. We have seen small companies attract investment rounds as a result of their participation in DTIF while others have been acquired. We have also seen examples of academic research teams being brought into major companies to establish new departments. The funding is helping to create new employment pipelines.” There has been a change to the way the latest call is being run. “Typically, we open calls with a three-month deadline,” Lambkin explains. “With this one we have a rolling deadline and teams can apply any time up until the end of April next year. But they can submit today if they want. The sooner the better as far as we are concerned.” The aim is to make it more user friendly. “While a deadline does focus minds, we want to give people with ideas time to engage with development advisers in Enterprise Ireland, IDA and Údarás na Gaeltachta to help them decide if the DTIF is right for them.” Another new development is a conference for DTIF participants to be held in the Printworks in Dublin Castle on September 9th. “For the first time, we are bringing the DTIF community together to discuss their experiences. We also want people who are interested in the DTIF to come along to learn about it directly from the companies and academics involved.”",Ireland,Irish Times; Dublin,2024-08-01,9,789,2024-08-01,2024.3,Medium,22,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,6.337135614702154,1,2,2.5348542458808616,1,0.3798008507321857,Positive
"As hundreds of Teachers’ Union of Ireland (TUI) representatives meet this week for the union’s annual congress in Killarney, it will come as no great surprise that the crisis in teacher recruitment and retention remains a grave concern. The latest survey carried out by the TUI’s Principals’ and Deputy Principals’ Association last October found that more than three-quarters of schools advertised positions in the previous six months for which no teacher applied, while two-thirds of schools had unfilled vacancies. This crisis is painfully real, and the continuing absence of political will to properly tackle it is unfathomable. It is clear what measures need to be taken. We will continue to campaign vigorously for contracts of full hours upon initial appointment, a restoration of posts of responsibility to boost teacher retention and a halving of the duration of the Professional Masters of Education (PME) to ensure that the profession remains affordable to those who wish to pursue it. In addition, if we are to persuade teachers to return to Ireland from jurisdictions such as Dubai and Australia, they must be awarded full incremental credit recognition for their service abroad. In September, we welcomed Minister for Education Norma Foley’s decision to shelve plans for teachers to assess their own students for State certification purposes. We have made it clear at all times and in all forums that in terms of senior cycle redevelopment, external assessment for State certification is key to the integrity of the process and must be retained. This remains our emphatic position. Minister Foley said that concerns over artificial intelligence (AI) guided her decision, which we believe was prudently made. Public trust in the Leaving Certificate cannot be gambled with. However, it is also very clear that AI is here to stay. Standing still or trying to ignore it is not an option; we must harness its potential benefits while ensuring that risks are adequately mitigated and policed. With this in mind we recently organised a conference on what AI might mean for the education system. Now, it is critical that Government departments intensify engagement with all stakeholders to ensure the education system is no way “outpaced”. Robust departmental guidelines and regulatory safeguards on this rapidly-developing area must be formalised and then regularly assessed and updated. In terms of opportunities, the potential of AI to reduce the growing administrative burden on educators should be explored. In a series of surveys, our members have consistently cited bureaucratic overload as an increasing and demoralising distraction from teaching and learning that in many cases is driving teachers and lecturers from the profession. Teachers detest the drift towards “performativity”. They want to teach, not tick boxes or attend meetings that benefit neither them nor their students. In this regard they are deeply and justifiably concerned about the usage and value of the so-called Croke Park hours in schools. At third level, following the advertisement of some senior positions in Munster Technological University (MTU) at a lower pay scale than similar positions in Dublin, TUI members gave an overwhelming mandate for industrial action over the non-adherence by the management side to a landmark collective agreement concerning the establishment of the technological university sector. Following nationwide protests and the serving of industrial action by TUI, the Department of Further and Higher Education agreed to pause the recruitment process. However, we have made clear that there must be parity of esteem across the technological university (TU) sector – we will not accept a situation where individual TUs are free to operate without regard or recourse to national negotiation. Meanwhile, in adult education, the delay in providing appropriate terms of employment to adult education tutors is an insult to the vital work they do. Of course, and inevitably, critical to many of these various issues, is funding. We as a nation are starting from a shamefully low base. The latest OECD indicators show that of the countries for which figures are provided, none spends a lower proportion of national wealth (GDP) on education than Ireland. This is even more pronounced at second level, where, at 1 per cent, investment is just half that of the OECD average. In third level colleges, the ratio of students to teaching staff in Ireland has now worsened to 23:1, a level far above the OECD average of 17:1, a legacy of the ongoing failure to address the sector’s resourcing crisis. Across all levels of education, students lose out as a result of inadequate education budgets, which result in larger class sizes, inadequate or non-existent supports for those who most need them and facilities that are ill-suited to modern, experiential teaching and learning. Our policymakers must finally match the commitment of our students and educators by providing levels of investment to our public education system that will ensure all can prosper. David Waters is president of TUI ERIC",Ireland,Irish Times; Dublin,2024-04-02,11,806,2024-04-01,2024.2,Long,23,1,1.2406947890818858,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.2406947890818858,1,0,0,0,2,2.4813895781637716,1,-0.5446902077846426,Negative
"Artificial intelligence (AI) is currently presented as a solution to all our needs and problems and education is no exception. Large language models such as ChatGPT are already used in schools, often to the dismay of teachers, which prompted OpenAI, the tool’s parent company, to release a guide on how to use generative AI in the classroom. Companies such as Khan Academy lure customers with individualised tutoring systems (like Khanmigo) that promise high-quality one-on-one teaching on an unprecedented scale. There are also hopes that AI will improve schooling by monitoring students’ mood and engagement, providing real-time feedback to teachers or even assigning grades automatically. There is no guarantee that such promises are anything more than clever marketing designed to entice schools and governments to invest significant resources into new products. After all, AI does not always work as advertised. However, let us assume that AI does indeed have the potential to revolutionise education by improving students’ performance and giving teachers more opportunities to, well, teach, rather than mark assignments and write reports. Not every school is going to equally reap these benefits and Ireland’s dual-language education system introduces unique challenges. We worry that widespread use of educational AI could advantage English-language schools and leave Gaelscoileanna behind. Training data Contemporary AI models rely on vast amounts of training data. Based on instructions from their developers, they can identify patterns which help them predict the likelihood that, for example, a given image depicts a dog rather than a turtle. In the case of ChatGPT, its makers needed to use 183,000 books, billions of web pages and numerous other texts before the model was able to formulate coherent sentences; it does so by assessing how likely given words are to follow each other in a particular context. Due to the relatively low number of native speakers, developers of AI tools operating in Irish do not have access to anywhere near the same level and variety of written source material as is available in English. This is problematic as greater amounts of data allow for the creation of more sophisticated tools. While the version of ChatGPT launched in November 2022 is estimated to have used about 570 gigabytes of training data, the newest, more advanced GPT-4 model required 10 times as much information. It is incredibly difficult to teach AI a language, because each one, including Irish, involves its own quirks and idiosyncrasies. Non-English versions of tools such as ChatGPT are already more prone to factual and grammatical errors and further advances are likely to exacerbate the differences in the availability of data. Is it possible to satiate the data hunger of Irish-language educational AI tools and maintain parity with their English-language counterparts? At the same time, we cannot just feed the model any data and hope for the best. If we expect AI to help in teaching subjects such as geography, history or physics and to do so in Irish, we need to first rely on existing materials that would demonstrate the required standards and set proper expectations. Textbooks The Government lamented the low availability of Irish-language subject-specific teaching materials. And while future AI tools might help us close this gap, they cannot create textbooks out of thin air. Assuming educational AI will be developed by private companies, is there a large enough user base for Irish-language AI that would guarantee satisfactory return on investment and incentivise developers to work on such tools? Recent data indicates that almost 52,000 pupils attend primary schools with at least some subjects taught in Irish. It is doubtful that any profits companies can extract from this cohort will offset the cost associated with the research and engineering. In addition, the decentralised nature of the Irish school system means that not every school need adopt specific tools, which further reduces the potential user base. By contrast, developers of English-language AI systems can target not only Irish pupils, but also their peers in other English-speaking countries, which in turn impacts where companies choose to invest. All these concerns lead us to worry that without a good plan, Irish-language schools will be faced with an impossible choice: embrace second-rate educational AI or forgo the potential benefits of automation altogether. Of course, general concerns related to AI and digital technologies mean that it might be a good idea to reduce our reliance on them and countries such as Sweden have already expressed similar sentiments. Countries could band together to develop their own AIs beyond the control of giant for-profit corporations which could better serve local languages. Examples under development from the Netherlands, Sweden and Finland hope to democratise AI by making it more open. We suggest that the Government should devote more attention to developments in educational AI and provide concrete resources to create Irish-language AI systems that would meet the needs of students and educators. The future of Irish-language schooling requires a proactive stance and a clear strategy on how to ensure that every school, no matter its main language, benefits from new technologies. Dr Michal Wieczorek is an IRC Government of Ireland Fellow working on ethics of educational technology and Dr Eamon Costello is an associate professor of digital learning at DCU",Ireland,Irish Times; Dublin,2024-09-03,24,863,2024-09-01,2024.3,Long,24,0,0,0,1,1.1587485515643106,1,0,0,0,0,0,0,1,1.1587485515643106,1,0,0,0,0,0,0,1,1.1587485515643106,1,3,3.4762456546929315,1,2,2.317497103128621,1,0.8199789770976924,Positive
"There is an anecdote doing the rounds among legal circles in Ireland about the potential perils of incorporating Artificial Intelligence (AI) technology, like ChatGPT, into lawyers day-to-day work. In a recent case in the United States, a law firm submitted a legal brief written by the predictive text generator ChatGPT, which cited a number of previous court rulings. The only problem was the citations were from rulings that did not exist, with a New York judge sanctioning the lawyers involved. Victor Timon, head of legal firm ByrneWallace’s technology group, said the example highlighted the pitfalls of relying on AI without checking the finished product. Speaking at a talk in the Bar Council on Wednesday, Mr Timon said tools like ChatGPT would play a “huge role” aiding legal professionals going forward. “As it improves all the time it will be far less risky, people will feel more comfortable using it,” including at least in some degree to write documents, he said. “It will be capable of generating material as long as there is human input into it,” he said. The data ChatGPT drew on to provide answers to questions was equal to “a million feet of bookspace” in a library, he said. For Michael Doran, a senior executive with legal consultants Johnson Hana, the technology will likely mean big changes for trainee solicitors or law graduates starting their careers. The tools would be able to take care of a lot of the “high volume, low complexity” work currently done by those on the first rungs of the legal ladder, such as reviewing documents and contracts, or writing early drafts, he said. “That does tend to be work the trainee solicitors or the first and second years focus a lot of their time. So naturally that’s going to beg the question are trainee solicitors needed or what does that traineeship look like,” he said. Checking Rather than replace those entry level employees, Mr Doran said AI could free them up for more rounded training at the start of their career. The tech had cut down on time spent checking a draft contract and could raise potential red flags for a legal team to investigate further. “We use it for elements of discovery and large scale contract review, where we have the ability to process large volumes of documents,” he said. Big legal or consultancy firms had been “ahead of the curve” in looking at the technology, he said. “I don’t think anyone is really lagging behind, potentially maybe you know the small mom and pop law shop because there is a cost involved in introducing it,” he said. The most impressive feature of new AI introduced in the last year was the pace at which it kept improving, he said. Barry Scannell, a consultant specialising in AI at law firm William Fry, said he felt the technology would “enhance lawyers, not replace them”. The legal field needed to be aware of the “drawbacks” and the limits on its accuracy at present, particularly if they were using it for legal research, he said. “I’m one of the biggest cheerleaders for AI that you’ll find in Ireland but I am strongly of the view that it isn’t close to being there yet in terms of accuracy,” he said. Daragh Troy, a barrister who specialises in data protection, said there would always be limits in what AI would be able to produce. “Do you want a computer to defend you, or do you want to rely on a human with expertise?,” he said. The new technology would be a “huge” help in cutting down on admin work, as well as making possible suggestions for where to make a start on a case, he said. If a barrister was defending a personal injury or criminal case, AI could hoover up the various statements involved and “pick out aspects” as prompts for cross examination, such as potential discrepancies in factual evidence, or if a claim was statute barred, he said. Human errors Carlo Salizzo, a partner at Matheson who works in their technology group, said checks could weed out any inaccuracies from AI, in the same way they caught human errors. “Part of the assurance we provide to our clients is giving them the confidence that there are two, three, four, five pairs of expert eyes on everything before it comes out of the building,” he said. “Adding AI to the equation doesn’t change that necessarily, it just means there is a slightly different path to get to the finished product,” he said. Emma Redmond, associate general counsel at OpenAI, the company behind ChatGPT, said the technology worked by “predicting what word comes next in a sentence”. Ms Redmond, who is also an adjunct professor of law at University College Dublin and trained as a barrister in Ireland, likened its impact to the “revolution that was the printing press”. “We are very clear there are safety risks with respect to AI, users have to be wary in terms of the outputs,” she told the recent Bar Council talk on the topic. Any legal documents written by AI should not be relied upon until they were “factually checked and corrected and reviewed by a legal professional,” she said.",Ireland,Irish Times; Dublin,2023-11-04,31,870,2023-11-01,2023.4,Long,25,4,4.597701149425287,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,9.195402298850574,1,0,0,0,0,0,0,-0.18178220303730383,Neutral
"There is broad agreement on the significant positive impact that artificial intelligence (AI) and generative AI (GenAI) can likely have, a survey of Irish businesses has found. Research among Irish business leaders carried out by PwC revealed that a surge in innovation and activity to enable AI adoption is happening. In June 2024 less than half (47 per cent) of respondents were engaged in experimentation. By December that had risen to two in three businesses (67 per cent). That’s a very significant increase. However, the translation of experimentation to adoption at scale remains relatively low, at less than 10 per cent. Nearly nine out of 10 (86 per cent) Irish business leaders believe that the overall impact of AI on Ireland’s economy in five years’ time will be positive while more than half (55 per cent) are of the view that GenAI will have a significant or transformative impact on their businesses over the same time period. The employment outlook is also positive with three-quarters (75 per cent) of survey respondents expecting an increase or no net impact on jobs in Ireland as a result of GenAI, up from 55 per cent when the survey was previously carried out in June 2024. Focus on productivity Those findings are broadly in line with the results of PwC’s 2025 Irish chief executive survey, which revealed an anticipated surge in AI and GenAI adoption in key business areas. On average, 89 per cent of Irish chief executives predicted that AI and GenAI will be systematically integrated into a range of key business areas in their organisation over the next three years. While there is a clear appreciation for the opportunities that AI can bring, the GenAI Survey also revealed that business leaders realise that safe and successful deployment and sustained AI adoption is a complex process that requires careful planning and co-ordination. That is reflected in the fact that just 6 per cent of respondents reported widespread adoption of AI in their organisations and this has remained below 10 per cent in all the runnings of the survey. The focus continues to be on operational efficiencies and productivity with 82 per cent of respondents believing that GenAI will deliver increased efficiency in their employees’ time at work. “There is broad agreement on the significant positive impact that AI is likely to have,” says David Lee, chief technology officer with PwC Ireland. “The overwhelming majority of respondents to our GenAI Irish Business Leaders Survey said it would have a positive impact on their business over the next five years. We do this survey every six months and that number has continued to grow.” Lee doesn’t believe that the low rate of widespread adoption should be a cause for concern. “It is not a surprise that the survey highlights that the current focus of AI-related innovation is on efficiency-related gains rather than more radical business model reinvention,” he explains. “In our experience, organisations need to build the confidence and trust in the technologies before they are willing to use them as the bedrock for more fundamental transformation.” Lee believes that the dial will move materially in the next 12 months. The findings are consistent with his experience of working with clients in the PwC GenAI Business Centre, enabled by Microsoft, he adds. “While organisations are interested in understanding the potential for AI to transform their businesses, their immediate focus is on ensuring that the right guard rails – both organisational and technical – are in place before they move beyond innovation to an adoption-at-scale mindset.” That position, however, is showing signs of improvement. “In the June 2024 GenAI Business Leaders survey, less than 30 per cent of business leaders thought that they had adequate controls and processes in place to ensure safe adoption and deployment. That went up to 56 per cent in the January 2025 survey, but more than four in 10 still believe the controls aren’t adequate so there is still some way to go.” Key inhibitor The other key inhibitor to adoption at scale is the lack of a clear business case for its deployment. “Organisations want to understand how they can monetise it, how the freeing up of capacity may be turned into commercial return from investments,” says Lee. “They need to have confidence in their ability to stand over the return on investment. Less than half (46 per cent) of business leaders are confident in their organisation’s ability to assess a return on investment from their current AI initiatives.” This slow rate of adoption and inability to translate the technology into increased earnings are only to be expected at such an early stage in the technology’s life cycle, Lee believes. “Before organisations are willing to trust technology to reinvent their existing business, they really need to be satisfied that they understand the technology and be confident that it won’t disrupt their core activities or their relationships of trust with employees and customers.” That considered approach is natural with the adoption of any new technology, he explains. Organisations tend to start with quite basic uses and as they become more confident, they start to take on and manage more complex applications. He points to Amara’s Law, named after the American scientist Roy Amara, which holds that the effect of a technology is overestimated in the short run and underestimated in the long run. In other words: once cold, hard reality bites, the true effect is understood but, from small beginnings, adoption rates accelerate quite rapidly. “It can be quite abstract when first adopting a technology,” Lee explains. He adds that when people see it in action it sparks new ideas. People start to ask if it can be used in one area, what else might it be able to do. The capability of the products is also advancing at pace. What started out as intelligent data retrieval tools have now gone much further than that. They are designed for specific use cases in areas like finance, marketing, HR and so on. They are becoming more refined and focused in terms of the personas they are trying to serve. That will have an impact on the ability to monetise the technology. Agentic AI holds out great promise in that regard. It translates knowledge into action and allows organisations to monetise the efficiency gains and time savings. And PwC and Microsoft have recently announced a strategic collaboration to transform industries with AI agents. This collaboration seeks to harness AI’s potential to drive business value, enhance customer engagement and streamline operations across various sectors. The collaboration focuses on deploying AI agents – sophisticated systems capable of performing tasks autonomously, analysing data and aiding decision-making. Together, PwC and Microsoft will provide businesses with the tools needed to embrace AI agents as strategic assets that enhance efficiency and drive innovation. Another encouraging finding of the PwC research Lee points to relates to attitudes towards the EU AI Act, with an overwhelming majority (86 per cent) of business leaders welcoming the legislation, saying it is necessary to prevent the potential negative impact of AI and build trust in the technology. “The Act provides a useful risk – classification framework,” Lee notes. “Organisations are more interested in what the Act allows them to do rather than what it doesn’t. The pitch you can play on is a lot bigger than the one you have to stay off.” Over the coming year, Lee says organisations must move faster in addressing the gaps in governance and controls, invest in understanding the business case for their own organisation that will provide the platform for wider AI adoption and reflect on the opportunities presented by developments in agentic AI.",Ireland,Irish Times; Dublin,2025-03-07,27,1277,2025-03-01,2025.1,Long,26,1,0.7830853563038371,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,1.5661707126076743,1,6,4.698512137823023,1,5,3.915426781519186,1,7,5.481597494126859,1,1.25726604710369,Positive
"Bobby Kenny, principal of Scoil Mhuire in Rosslare, Co Wexford, has unapologetically been using artificial intelligence (AI) tools in the classroom ever since they became available. “It is better to render children pupils competent digital citizens,” he says. “We use Google Gemini, which is similar to ChatGPT but, in my view, better. “Our digital learning team is experimenting with simple AI tools like auto-grading quizzes, and using AI-powered rubrics to provide more personalised feedback to pupils.” While teachers can also use it for administrative tasks, he says it helps him save time in crafting emails and newsletters, ensuring they are consistent. “I also use it for budgeting, resource management and planning. Crucially, it can also be used to create individual lesson plans for children based on their interests, strengths and weaknesses,” he says. At the recent teacher conferences, Minister for Education Norma Foley announced that her department had commissioned research into the benefits and risks of artificial intelligence in education. While the major advances in AI may have taken most of us by surprise, students and teachers from primary through to third-level have been using it for years. In most cases, it is as a learning tool or, for teachers, to help with lesson plans or correcting. But there is also a widespread concern that some students can use tools such as ChatGPT to do their homework for them – and this comes at a moment when the senior cycle reform is moving away from an emphasis on exams and towards project work that can be done at home. Dr Michal Wieczorek is an Irish Research Council Government of Ireland fellow, and he is researching emerging technologies, including AI in education and the ethical issues that it may throw up. Wieczorek, who has a background in philosophy and ethics, says there can be problems with the use of AI in education, but that it also has huge potential to improve teaching and learning. First, the negatives. “Some AI products are commercial and expensive, so not every school can afford them,” he says. “Teachers need training, and the infrastructure needs to be kept up. There is a risk that schools become dependent on these technologies and, instead of parents having to buy new textbooks every year, schools or parents have to pay for licenses.” Biases Wieczorek also says AI is not as transparent as, for instance, a textbook. It can, for example, reflect the biases of its creators and ignore the experiences of women and minorities. A recent New York Times report shows how, in the hands of big technology companies, it may breach multiple regulations to harvest data. “It’s not just bias; there can be inaccuracies, too,” Wieczorek says. “We know that ChatGPT has, on occasion, used made-up citations, and we could see this happen with AI textbooks. AI tools could promote the mainstream, and may not be as useful for, say, a non-native speaker, or for Irish-language students. “And it could lead to monitoring and behaviour of students, without regard to different ways of learning and engaging. If it monitors attention levels, for instance, this could penalise neurodivergent students.” As for the positives, Wieczorek says that AI can “visibly” perform some jobs better than teachers, including lesson plans and automated assessment. Indeed, many teachers are already using it for just these reasons. “Teachers can use analytics tools and student data to glean insights into how students are progressing. “At scale, AI can increase diversity by increasing the number of available resources, especially in schools that may not have a great library. If carefully designed, it can help neurodivergent students or students with disabilities or learning difficulties.” While Wieczorek points out some areas of concern, he believes that AI can be a positive in education – it’s just that it needs to be approached with caution. The Irish Times has interviewed educators who, already, are using AI. Broadly, they are positive about its potential. Natalie Doyle, a post-primary teacher at Chanel College in Coolock, north Dublin, says the pay-off in using AI tools is huge if they master how to craft queries and instructions for the technology. “It helps brainstorm universal design for learning strategies to engage a diverse classroom or draw on the expertise of specialists in countless fields, be it subject knowledge or specific education needs like autism and anxiety,” she says. Chanel College’s website includes links to a range of resources that teachers are using. “For someone like me, who is juggling roles as a special-education teacher, AI is invaluable, bridging gaps in my knowledge and helping me to design engaging comprehension lessons around things like Lego or Star Wars, topics that resonate with my students but aren’t my areas of expertise.” Teacher Patrick Hickey also says it is “revolutionising” the way that he teaches. “The benefits are considerable, but there is a need for adequate training, as only those who understand its full potential and risks should be employing AI.” One way in which Hickey uses it is to give prompts to AI to generate more detailed and in-depth feedback for his students – still time-consuming, but less so than without AI. Hickey is conducting training for teachers via Ireland’s network of education centres. Tailored Ultan Mac Mathúna is director of Dublin West Education Centre, which has created resources for teachers around AI. “We have worked with teacher Shanon Ahern, a teacher in St Declan’s College in Cabra, who is using ChatGPT to create personalised learning experiences in science, technology and maths education,” he says. “She has used it to generate statistics on areas that may interest her class – Formula One, for instance – and used this as a tool for teaching data analytics. “AI can be used to make learning fun and tailored to each student. It is a useful tool for creating resources and for school planning.” At Carlow Educate Together National School, principal Simon Lewis has a project called mash.plus. “It creates units of work, lesson plans and resources based on simple prompts,” he says. “For International Women’s Day, I created over 130 units of work on inspirational women in less than a day.” He says AI alone won’t make a teacher better but, if its power is harnessed, it could help teachers create original and more engaging content. “But if a teacher asks a student to produce a project, there is a risk they’ll simply go to ChatGPT and ask it to produce it – and as AI gets better, it will be more difficult for a teacher to know. However, skilled teachers will find ways.” (And, indeed, without wishing to alert students, many teachers and lecturers already have innovative techniques to catch cheats). Ireland’s first national strategy on AI, Here for Good, was published in 2021, while the EU has moved on the issue more recently, assessing the application of AI across education as high-risk. “There is a need [for] careful consideration about the use of AI in all aspects of teaching, learning and assessment,” says a Department of Education spokesperson. “Teachers and school leaders [need] the necessary resources and skills to support them.” The Department’s digital strategy for schools, covering up to the year 2027, aims to empower schools with digital literacy and competence, says the spokesperson. Its research into AI is being carried out with the State Examinations Commission and Oide, a support service for teachers and education leaders. “Reform of the curriculum is under way, and the Department continues to monitor developments in the AI space, and the impact it may have in education and assessment settings,” the spokesperson said. The Minister has asked the State Examinations Commission to research the potential role and impact of generative AI in teacher-based assessment in particular, the spokesoerson added, and emphasised that such research should consider both the opportunities and challenges with it.",Ireland,Irish Times; Dublin,2024-05-07,35,1303,2024-05-01,2024.2,Long,27,4,3.0698388334612434,1,1,0.7674597083653109,1,4,3.0698388334612434,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0.7674597083653109,1,2,1.5349194167306217,1,0,0,0,0.9008716219192605,Positive
"Artificial intelligence is already having a profound impact on the health service, improving the patient experience, protecting clinician time, drive patient safety and even prevent healthcare with its ability to detect disease before it manifests. That’s the view of Jens Dommel, who is the head of healthcare for Europe, Middle East, and Africa (EMEA) at Amazon Web Services (AWS). “AI is primed to aid healthcare in addressing some of its biggest challenges,” he states. The first goal, he says, is to improve the person’s experience of the healthcare system and offer them support in managing their own healthcare in all circumstances, whether they have a young family or are ageing. Meanwhile, from the provider’s perspective, the aim is to improve their own clinical experience – a big challenge in healthcare systems globally. “There is a big gap in the number of clinicians the health system has and what it actually needs – the World Health Organisation is forecasting a shortfall of up to 10 million in 2030,” Dommel notes. With this huge work burden, clinician burnout is very real, he adds. The cost of providing healthcare continues to mount, and, according to Dommel, AI offers the possibility of a truly “sustainable and resilient” health system. It can be deployed, he says, to increase access and address inherent inequity in the system, bringing appropriate healthcare to all those who need it. Every two years the volume of healthcare data doubles, much of which is raw and untapped; Dommel says estimates suggest as much as 97 per cent of healthcare data goes unused, largely because it is unstructured. “Even if it is digitised, it may just be in pdf format. We don’t have a lack of data, we have a lack of access to that data. We need to turn that data into value, into outcomes, and that is what AI can help us do.” AI has the power to address these big roadblocks to better healthcare and improved patient outcomes. Dommel cites a number of examples where the technology is already in use, driving efficiencies and freeing up valuable clinician time. An Irish customer of AWS is T-Pro, who have developed an ambient listening solution where the AI can summarise entire medical consultations into clinical notes, to be added to electronic healthcare records. In the area of diagnostics, another AWS customer and Irish company xWave Technologies uses AI to vet radiology referrals to ensure patients get the best test first, reducing time, cost, and getting patients diagnosed quicker. In a project carried out at St Vincent’s University Hospital and the Royal Victoria Eye and Ear Hospital in Dublin, xWave’s implementation reduced the number of CTs and MRIs needed by 61 per cent. All this and more is possible because of what Dommel calls “the democratisation of AI; we want for every company and every institution to be able to become an AI organisation” he explains. “For AWS, our vision is to make AI easy and secure to use, easy to build, affordable to run, and scale, in a trustworthy way.” AWS provides a global cloud infrastructure, including a region located in Dublin, that allows their healthcare customers to create secure, resilient, and scalable applications – and was recently awarded “Best in Klas” for public cloud in healthcare. They also provide access to foundation models from a number of AI companies and a cost effective place to train and fine-tune their customers’ own proprietary models. “Customers have the choice to use any of the different models or combine them, because there is no one size fits all model that solves everything, instead we see there are specific models for specific use cases and the quality of the outcome is key.” Dommel shares that AI has the potential to become part of every single use case in the healthcare space. “The value is in better outcomes but also better predictive ability so that we prevent diseases before they happen,” he says. “AI will help us in that, not least in Ireland where we are seeing lots of new digital programmes being introduced by the HSE that will transform how healthcare is delivered here. It’s an exciting time. We are committed to partnering on this digital transformation in Ireland.” For all AI’s promise, Dommel stresses that the purpose is not to replace doctors and nurses with technology. “It’s important to think about AI as a new member of the care team.”",Ireland,Irish Times; Dublin,2025-05-09,15,737,2025-05-01,2025.2,Medium,28,1,1.3568521031207597,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.7764159568872144,Positive
"The European Union’s AI Act comes into effect this month. It is an important step in establishing a regulatory framework for an increasingly powerful technology that has the potential to shape the future of Irish business. As the groundbreaking Act moves from legislation to implementation over the coming three years, commentary has intensified on its likely impact on innovation. Critics argue that stringent regulation could stifle Europe’s ability to compete globally, particularly against AI powerhouses such as the US and China. Some believe that it could lock Irish businesses into an “AI dark age”. In truth, the relationship between regulation and innovation is nuanced. While the US is pushing ahead with a lighter-touch approach to regulation, arguably leading to quicker technological advancements, a path without boundaries is fraught with risk. With our strong transatlantic ties and our role as the EU headquarters for many global technology firms, Ireland can help mitigate fears of an “AI slow lane” by positioning the State as the leading regulatory hub in Europe. First-mover advantage By taking the lead in how rules under the EU’s AI Act are implemented over the coming years, Ireland can have a first-mover advantage, setting out an approach that enables businesses to innovate while building trust in this evolving technology. Effective regulation can coexist with innovation. A well-crafted regulatory framework provides clarity and certainty for businesses, helping to secure investments and foster a stable environment for technological advancements. As we look to bring the new rules surrounding AI to life, we can learn from similar deployments in the past. GDPR, for example, helped to protect personal data but it’s not without its critics. Some technology firms based in Ireland have claimed GDPR has posed challenges for AI innovation by limiting data access and creating data barriers between European countries. Regulatory sandboxes are one of the tools that can be deployed to help Irish businesses to innovate responsibly. They provide a safe space in which businesses can test AI products and services under a regulator’s supervision. These sandboxes have been successfully employed in the UK fintech industry and enabled the development of autonomous vehicles in Singapore. Above all, they illustrate how innovation can thrive within a controlled, safe environment that allows for safe experimentation. However, a collaborative approach to regulation alone will not be enough to position Ireland as an AI leader. Additional steps are required to foster a world-leading AI environment that can compete with that of the US and China. The Irish Management Institute (IMI) is working closely with Irish chief executives, technology officers and HR leaders to understand the complex leadership challenges posed by AI and the ingredients needed to unlock its business benefits. While a pro-innovation approach to policy is critical in the long term for businesses adopting AI technologies, it doesn’t automatically lead to organisations and their people embracing AI at speed right now. Pivotal role Business leaders have a pivotal role to play. In fact, they are the most important stakeholder in driving change and creating a culture of innovation where employees can successfully create and adapt to evolving technologies. It is not surprising that “leadership capabilities” emerged as the top business priority in the latest IMI Leadership Barometer. The creation and deployment of successful AI initiatives will hinge not solely on technology, but also on the organisational structures and leadership strategies that guide and facilitate them. We have seen first-hand that the biggest enabler of successful digital transformation projects is not the technology itself, but the organisational and leadership structures around the project. “Skills” emerged as the second most important business priority for leaders navigating the era of AI. Our research shows only one in four business leaders feel they have the skills and capabilities to harness AI within their organisations. Even more concerning, the OECD has raised concerns about the competitive vulnerability of Irish workers to the deployment of AI and automation, with a lack of participation in lifelong learning placing many Irish workers at risk of falling behind. It is vital that Government, industry and the education and learning sector come together to close the emerging AI skills gap. Collectively, we need to ensure that every leader has the capability to lead their team through change while also ensuring every staff member has the skills and tools to understand, use and create value with and from AI. In a world increasingly shaped by AI, the most successful teams are not necessarily technology experts but their organisations place a strong emphasis on continuous learning and upskilling. The EU AI Act heralds a new era of technological governance, and Ireland stands at a crossroads. We have a real opportunity to shape the future of AI as regulatory leaders in Europe. Ireland’s approach to AI is ours to shape. The time to lead is now. By acting with clarity, courage and a collaborative spirit, Ireland can play a key role in creating real and sustainable value from the AI revolution under way. Shane O’Sullivan is chief executive of the Irish Management Institute",Ireland,Irish Times; Dublin,2024-08-12,24,837,2024-08-01,2024.3,Long,29,2,2.389486260454002,1,0,0,0,0,0,0,0,0,0,1,1.194743130227001,1,0,0,0,1,1.194743130227001,1,15,17.921146953405017,1,10,11.947431302270013,1,0,0,0,0.8196987931341709,Positive
"Artificial intelligence is no longer a distant concept confined to Silicon Valley or science fiction. It is here, now, reshaping economies, transforming industries and prompting governments to rethink everything from education and employment to ethics and regulation. The question Ireland must urgently confront is whether it is truly prepared for the scale and speed of change AI is set to unleash. There are reasons both for cautious optimism and concern. Ireland has long positioned itself as a European technology hub, hosting the headquarters of many of the world’s leading tech firms. Government agencies are actively courting AI investment. Research centres in Irish universities are leaders in developing cutting-edge AI applications. The recent launch of an AI Skills programme by the Department of Enterprise, Trade and Employment signals a growing recognition of the need to upskill the workforce. But the evidence of the last decade also suggests that Irish regulators struggle to come to terms with the challenges posed by US tech companies which are not slow to wield their considerable political influence. The establishment of an Oireachtas Committee on Artificial Intelligence, which met for the first time this week, marks a step in deepening political and legislative engagement with the issue. The committee has the potential to initiate a broader public and legislative debate on AI policy that balances innovation with the safeguarding of democratic values and public interest. But for this to succeed, it will need to act in constructive coordination with education, enterprise and civil society. Because readiness is about more than strategy documents, it must reach into the heart of how we educate, train, and govern. Schools, for example, are already grappling with how to prepare students for a new world in which traditional educational metrics are no longer credible. The reformed Leaving Certificate, with its increased emphasis on project work, now faces an unexpected challenge: how to assess students fairly in a world where generative AI can produce plausible essays and design projects at the click of a button. Equally urgent are the labour market implications. AI is set to displace roles in sectors ranging from transport and customer support to software development and legal services. Without a serious, joined-up national response, Ireland could face widening inequality, social dislocation and a growing digital divide. Nimble policymaking and innovative thinking are imperative. But this moment also requires political will and public investment. AI is not just a technological leap. It is a societal transformation. If Ireland is to thrive in this new age, it must act with speed, coherence and purpose. The future will not wait.",Ireland,Irish Times; Dublin,2025-06-12,12,429,2025-06-01,2025.2,Medium,30,0,0,0,0,0,0,1,2.331002331002331,1,0,0,0,0,0,0,0,0,0,0,0,0,3,6.993006993006993,1,4,9.324009324009324,1,2,4.662004662004662,1,-0.4990260868833398,Negative
"I’ve been sceptical that artificial intelligence (AI) will radically remake labour markets in the short term, in part because so much hype comes from the tech industry itself. But in the last couple of months, I’ve encountered some very diverse use cases for artificial intelligence that have me thinking differently. First, a psychiatrist in New York told me how mental health professionals are beginning to use AI to track clients’ word choices across sessions. Word choice can be an important factor in understanding mental illness and making diagnoses. Previously, this was dependent on a therapist’s memory and perceptions. Now, AI-driven analytics will be, in his words, a “game changer” in terms of how effectively patients can be diagnosed and treated. Second, an investor told me about a company in the US, Axon, that is leveraging AI and proximity data. One product, called Draft One, allows police in places such as Lafayette, Indiana to download body-camera images, then push a few buttons to create a first draft of the “incident reports” that take up roughly 40 per cent of their time. While other attempts at high-tech policing have come with unforeseen challenges (such as algo-racism), markets are keen. Axon’s stock is up 730 per cent over five years. Finally, weary after a long business trip, I recently got a massage in Anchorage, Alaska. My masseuse, who had dropped out of high school, had managed to turn her difficult life around in dramatic ways and wanted to write a memoir to inspire others. Discovering that I was a writer, she asked me to read her book proposal, which turned out to be as good if not better than many I’ve seen in high-end literary slush piles. Her co-author was ChatGPT. Consider how many such anecdotes – reaching across geographies and industries – there are, alongside the fact that productivity in the US is finally on the rise after flatlining during the pandemic. There may be multiple reasons for that, from low-wage immigration that allowed more skilled workers to move up the ladder into better jobs, to cyclical gains post-pandemic. But most experts agree that greater implementation of cutting-edge technologies such as AI is clearly playing some role in driving up productivity. As a recent Apollo economic outlook note put it: “The US is experiencing a surge in corporate and research spending on the back of the AI revolution – a dynamic not seen in other developing nations or even China.” There are three key lessons to take from this. First, scepticism about how frothy the US market in general, and technology stocks in particular, are is entirely understandable. But it’s hard to see how Europe and many other countries would outperform relative to the US if we are heading back to a period of technology disruption that is similar, if not exponentially more dramatic, than what we saw in the 1990s. Technology deployment Back then, investors will remember, that the US pulled ahead of Europe in terms of technology deployment and, as a result, in economic and market growth. AI would seem to be putting that trend on steroids today. That leads me to lesson two: the fortunes of countries, companies and individuals often diverge in periods of technological change. The term “jobless recovery” was coined after the 1990-91 recession, because while stock prices, corporate margins and gross domestic product (GDP) growth surged, employment growth lagged for longer than expected. While unemployment is low in the US today, it is notable that workers in areas ripe for disruption by AI – such as software development or middle management gigs – seem to be getting more redundancy notices. It obviously takes time for job markets and people to adapt to technological change. Yes, as the economists will tell us, new technologies eventually create new jobs. Big shifts But as historians, sociologists and social workers can attest, that doesn’t mitigate the pain of those going through such big shifts. Nor does it prevent the disruptive political convulsions that can result. The disruptions to manufacturing jobs, which represent only a little over 10 per cent of employment in the US and many European Union countries, brought us Donald Trump and European populism. We are about to see up to 85 per cent of the labour market disrupted to some extent by AI. What will that mean? For a start, says Nobel laureate and Massachusetts Institute of Technology professor Daron Acemoglu, today, as in the past, “it’s likely that the short- to midterm gains from AI will be distributed unequally, and will benefit capital more than labour”. This brings us to lesson three: people on most rungs of the socioeconomic ladder are anxious about the future of work. If the AI enthusiasts are right, it’s difficult to see whose job won’t be affected by the technology. That anxiety has big consequences. Jim Clark, founder of the recently launched New York-based Future of Employment and Income Institute, tells me that “economic anxiety changes behaviour and that can create major political shifts”. Think about the Luddites, for instance, whose name is still given to the backlash against tech, or, on the upside, the social welfare state, which came out of the industrial revolution in Germany. Investors, business leaders and politicians should take this to heart. What’s good for markets and even GDP growth today may not be good for politics or society tomorrow – at least not without big policy shifts to help buffer the coming disruptions. – Copyright The Financial Times Limited 2025",Ireland,Irish Times; Dublin,2025-01-13,16,916,2025-01-01,2025.1,Long,31,0,0,0,0,0,0,0,0,0,2,2.1834061135371177,1,0,0,0,1,1.0917030567685588,1,0,0,0,1,1.0917030567685588,1,2,2.1834061135371177,1,2,2.1834061135371177,1,-0.9022673826933331,Negative
"While teachers worry about students cheating with ChatGPT, doctors might be playing their own bluff with AI. In the largest study of its kind, my team surveyed more than 1,000 GPs working in Britain, and 20 per cent admitted using this and similar tools to “assist with clinical tasks”. Twenty-nine per cent used AI to generate patient documentation after appointments, 28 per cent for diagnosis and 25 per cent to suggest treatment options. Other research that I have conducted in the US also suggests doctors may be turning to “generative AI” in their droves. Exemplified by OpenAI’s GPT-3.5, and its later versions, GPT-4 and GPT-4o, as well as Google’s Bard, and Microsoft’s Bing AI, these widely available consumer tools work differently from traditional search engines such as Google. Powered by vast swathes of data that help drive their responses, these bots are often dubbed “large language models” (LLMs). Instead of typing requests for information online and receiving a list of internet pages in response, this new generation of computer interface simulates staggeringly fluent “conversations”. There’s little reason to think UK doctors are unique in this. Nor are doctors and students alone in adopting ChatGPT. In November 2023, a survey by Deloitte found that a third of people in Ireland had used the tool, with 10 per cent incorporating it into their work. So should the idea of medics using AI for diagnosis or to suggest treatment options be cause for concern? It’s easy to see why it might make us uneasy – but I would argue that embracing this shift in the longer term will be a win for patients. Body of research Already a growing body of research shows that generative AI has considerable potential in gathering patient information and helping doctors with diagnostics. For example, early research shows that GPT-4 – which isn’t even designed for medical use – can produce accurate lists of patient diagnosis, even in complex cases. Medical grade chatbots can do even better. In the United States, 1 in 6 patients may no longer be turning to Dr Google but to Dr Bot instead. Health chatbots have been shown to offer accurate, accessible information to patients with cancer who are concerned about their health. It can also read cancer scans quicker, more consistently and more accurately than the average human doctor, and with potential for patients in underserved areas of the world. Most people assume that doctors will always be needed for communication. But a clinical conversation system called AMIE can even take patient histories in greater detail and with better bedside manners than human physicians. There’s even some evidences to suggest that cold hard machines may surpass clinicians in compassion. Maintaining high levels of empathy in care can be tough, and doctors often face burnout and compassion fatigue. AI can assist with that. A study using the TalkLife mental health platform showed that responses co-written with a chatbot named “Hailey” were perceived as more empathetic than human-only responses. A study comparing ChatGPT’s responses to those of doctors using 195 real-world health questions on Reddit’s AskDocs found that the bot’s replies were 10 times more empathetic. Much is made about the risks of AI embedding “algorithmic biases” in its responses, and rightly so: some patients could be treated unfairly. Less discussed, however, is whether AI is worse than what we’ve already got with human doctors. The negative focus on AI bots also misses important new opportunities to better identify and overcome discrimination in healthcare. AI can shine a light on health disparities and prejudice too. For example, crunching through vast electronic health records, computers reveal the ways doctors tend to stigmatise some patient populations more than others. These subtle, buried or unconscious biases are to be found in the notes that physicians write after our visits. It’s also likely that de-biasing bots is easier than debugging humans of bigotry. AI can also help doctors with the dreaded burdens of administration – one Swedish study found ChatGPT was 10 times faster than doctors in generating clinical documentation. Medical grade tools could be even more efficient. Across the United States, health systems are swiftly adopting LLM-powered tools that uphold stringent privacy standards into everyday clinical practice. ‘Ambient listening’ A prominent example is “ambient listening”, where an AI system listens to patient-physician conversations and drafts clinical documentation. This technology is already in use in many clinics, and early results are promising – it boosts satisfaction for both patients and providers, without raising serious safety concerns. In Ireland, digital healthcare is a key component of the Sláintecare reforms aimed at modernising the country’s health services. As clinical records become increasingly digitised and patients gain access to them, doctors and patients will be better equipped to leverage AI. There are opportunities for Ireland to become what tech bros dub the “fast second”: to follow quickly in the footsteps of health systems that are further ahead but by learning from them and adopting better approaches. Integrating AI into patient care offers real benefits. If using chatbots feels like “cheating” on the mission to offer the best possible healthcare to patients, it might be time to rethink what “cheating” means. Dr Charlotte Blease is associate professor of health informatics at Uppsala University, Sweden, and research affiliate, digital psychiatry, Beth Israel Deaconess Medical Center, Harvard Medical School. She is the author of the forthcoming book, Dr Bot: Why Human Doctors are Failing and How AI Can Save Lives (Yale University Press, 2025).",Ireland,Irish Times; Dublin,2024-10-15,29,910,2024-10-01,2024.4,Long,32,2,2.197802197802198,1,1,1.098901098901099,1,3,3.2967032967032965,1,0,0,0,1,1.098901098901099,1,0,0,0,1,1.098901098901099,1,0,0,0,0,0,0,0,0,0,0.052417339074458606,Neutral
"The explosive arrival of ChatGPT last year marked a starting gun for the diffusion of generative artificial intelligence (AI). Transactions are already feeling the effect. “AI has been a prominent discussion point in recent times as many companies explore opportunities that digitisation and AI can bring to their business. It is no different in the M&A [mergers and acquisitions] environment, where AI has the opportunity to change the landscape in how deal-making is carried out and how target companies are evaluated,” says Rory O’Keeffe, partner of the transaction services team at professional services firm BDO. Traditionally bringing a transaction to market, negotiating a deal and running due diligence has been complex, time-consuming, and resource-intensive. “Given this, M&A professionals and due diligence advisers are actively moving forward in developing their AI capabilities in order to enhance their deal-making capabilities and to best support their clients through their M&A transactions,” he says. Due diligence For deal makers, the use of AI will allow companies to accelerate their target identification and evaluation of potential acquisitions. “We already see AI technologies used in many deal-research platforms where global M&A data is aggregated to provide insights on sectors, upcoming transactions, prospective target companies and buyer lists,” he explains. In due diligence, AI technology has the potential to enable greater efficiency in understanding and evaluating target company data, while also deepening the sophistication of diligence being carried out. “AI could analyse and write up findings and improve the quality and consistency of deliverables. This will provide the potential for significant time and cost savings through automating data analysis and quickly flagging anything that would require further review,” says O’Keeffe. “Traditionally, the due diligence process starts with long lists of data requests where significant time is spent reviewing, reconciling, analysing and summarising in order to extract key insights and metrics to be considered in the context of the transaction. “The potential benefits that AI can bring to this process are huge. However, it is not without risk as AI lacks human judgment and intuition, both of which are important attributes in evaluating financial metrics and other qualitative factors. The understanding of such can impact the due diligence findings and the overall deal success,” he adds. Ireland’s transaction market is mainly mid-market, typically firms worth anywhere from €10 million to €200 million, with profits of about €2 million to €15 million. Many are on a growth path but are not yet using AI in any significant way themselves, points out George Byron, a partner in financial advisory at Deloitte who specialises in financial due diligence. “Even larger PLCs and multinationals are just using it for the automation of repetitive tasks at present but not much more. So what businesses probably need to do in the first instance is to modernise their data,” he says. “What is really valuable is data analytics, using big data tools and visualisation tools to support due diligence work which was previously done only on Excel. This allows you go much deeper into analysis really quickly, validating with greater accuracy, to very quickly determine if there are data gaps or to establish the robustness of the existing data. It allows you make more connections, on the buy side, spotting trends that perhaps the business itself might not be seeing.” Data analytics AI-driven data analytics can support the sell side too. “Vendor due diligence is about coming to the deal with data that supports the sale of the business. That means data that supports the growth story and the equity story. In the current market, preparation is key. While there is plenty of capital out there, buyers are being choosy. There is also a valuations gap at the moment, so the more you can do to highlight the growth story, the more you are helping the process. Being able to bring the data out of your business to support that is more and more critical,” says Byron. AI has already become a “game changer” in an array of financial processes, points out Fergal McAleavey, a corporate finance partner at EY. “In areas such as risk management, fraud detection, and customer service, AI algorithms analyse vast amounts of data at speeds unimaginable with traditional methods. This not only enhances accuracy but also allows for real-time decision-making. Its application could offer several benefits, however, it’s important to be aware of the potential risks and challenge,” he says. While AI enhances efficiency, algorithms are only as good as the data they are trained on. “Biases present in historical data can lead to skewed results. Buyers need to ensure that the AI models are transparent, and they should validate the output against traditional due diligence methods. Sellers should be aware that AI-driven analyses may uncover aspects of their operations that were previously overlooked. It’s essential for sellers to conduct their own internal assessments before the due diligence process to identify and address any potential red flags. Open communication about the use of AI tools during due diligence is also crucial,” says McAleavey. Charlie Carroll, corporate and M&A partner at A&L Goodbody and head of ALG Solutions, believes generative AI will become an integral part of M&A transactions soon. As such, law firms that invest in its capabilities ahead of the cycle will gain a competitive edge. “Modern-day M&A transactions require clients and their advisers to handle, assess and report on ever-increasing amounts of data, and against a breadth of metrics that would have been unachievable even a decade ago,” says Carroll. “Clients need to access, manage and analyse this data effectively to support their strategic decisions and achieve their deal goals. Forward-thinking law firms recognise that manual resources alone are no longer the singular solution to these challenges. Leading M&A law firms are therefore harnessing the power of AI technology to enhance their capabilities and efficiency for the benefit of their clients.” Risks and limitations Clients expect their legal advisers to provide them with the highest quality legal advice and market knowledge. Law firms want to empower their practitioners to deliver faster and deeper insights into the data, he explains. “These are complementary efforts that, when done correctly, benefit both the law firm and the client.” His firm has been testing Generative AI for the past year. “Generative AI can help create new insights, content, and data based on existing or new M&A transaction data, such as contracts, financial statements, data room repositories and online reports; and can marry that capability with generative output for queries on legislation, rules, codes of governance and so on. These next generative AI products are coming to the fore across many sectors, including in M&A transactions. We recently onboarded Harvey, a bespoke generative AI solution for the legal market and we are the first Irish law firm to do so,” he adds. “Generative AI products like Harvey have?the power not only to help it to automate and optimise some of the tasks that are repetitive or complex, such as identifying key terms and clauses or flagging potential issues or red flags, but just as importantly, has the power to put at the fingertips of our expert practitioners transaction information which empowers our clients to do deals.” It is, of course, well aware of the risks and limitations of Generative AI, such as the quality, reliability, security and ethics of the generated content or data, as well as the oft-cited hallucinations that can occur. “As we continue to evaluate the use of Generative AI in our business, we are careful to consider the training needs of our practitioners,?the need to monitor and validate the outputs of Generative AI, and to use it as a tool to complement and augment our legal expertise and judgement — not replace it,” he says. Early days for AI Despite the hype cycle, it is still early days concerning AI and in particular generative artificial intelligence (GAI) of the kind unveiled by ChapGPT. Artificial general intelligence (AGI), which for some tech types is the Holy Grail, is as yet the stuff of fantasy — but only just. It’s why the race is on to see how the accelerating pace of tech development might affect a range of business services and processes, including M&A due diligence. “AI is still relatively new, and the possibilities of GAI really only arrived into the public last year with the launch of ChatGPT, so it is still a work in progress,” says Elena Lillo, regulatory affairs executive at Financial Services Ireland, an Ibec group. It is for sure transitioning from early adopters to mainstream integration. But we don’t have any data on its adoption at the moment.” There is no regulatory framework either, although all eyes are on the EU’s forthcoming AI act, which will plug that gap. Existing GDPR and data security regulations will also hold sway, she points out. “We know that the use of AI can identify patterns and analyse larger data sets, which would take more time and more people to otherwise do. But for interpreting results and making the final decision, there should always be a human behind it, and not just in relation to corporate transactions but in general,” she says. “It is an evolving landscape and because of that, there are a lot of uncertainties. Firms know it will be good for them but there must also be transparency. And both sides of the deal will have to know in advance if AI technology has had some part in the decisions being taken, to safeguard against legal challenges.”",Ireland,Irish Times; Dublin,2024-02-16,40,1581,2024-02-01,2024.1,Very Long,33,7,4.427577482605946,1,0,0,0,2,1.2650221378874131,1,0,0,0,4,2.5300442757748263,1,0,0,0,2,1.2650221378874131,1,11,6.957621758380772,1,4,2.5300442757748263,1,4,2.5300442757748263,1,2.1197705575300176,Positive
"DANIELLE BARRON It is just over two years since the launch of ChatGPT, and while artificial intelligence (AI) has undoubtedly had a massive impact on the stock markets since then, observers agree the technology has largely failed to live up to the early hype that surrounded it. The truth is AI has been around for 75 years in one form or another, points out Séamas Kelly, who is professor of organisation, technology and society at the UCD School of Business. Many “AI summers” and “AI winters” have played out over that time, he says. “During the summers, there is a sense of we are going to solve it, we will create a super intelligent machine and there is a huge amount of excitement and investment, but then it hits the barrier, then an AI winter follows that, where there is a real sense of letdown and a lack of investment,” he says. “This is a very familiar type of cycle, and the hype around AI in recent times has been outlandish.” Kelly believes that, as a society, we are gripped by “techno optimism” and as a result we are poor at questioning any technological change, not just AI. “When you have this kind of culture that sees technology as always being progressive when you add that to something like AI, it can lead to hysteria.” According to Rory Timlin, data and AI lead with KPMG in Ireland, we are still in a hype bubble surrounding AI, but he notes the focus is now shifting from hype to finding meaningful value from AI – “benefits over buzzwords”. “Given the noise around AI, it is important to be aware of the different kinds of AI systems and the different ways of integrating AI into an organisation’s digital capability,” he says, noting that, while Generative AI (GenAI) has recently dominated the airwaves, it is only one form of AI being employed in real-world application. “There are sound proven use cases for AI and where AI is integrated into end-to-end business processes, it has a higher potential to create meaningful value,” he notes. Machine learning systems have been successfully employed in areas including in supply chain optimisation, manufacturing efficiency, cyber security, digital marketing, media content recommendation and healthcare, while GenAI is now gaining traction in domains including customer service, digital content generation and marketing, he says. KPMG’s recent Global Tech Report found that, while 70 per cent of organisations say AI investments are delivering business value, only 31 per cent reported delivering value at scale. “We see AI investment decisions moving from fast-following to being more return on investment focused, based on evidence and data,” Timlin says. He echoes Kelly by noting that it is “vitally important” for an organisation to have the right processes and controls in place to deploy AI in a trustworthy and ethical manner. “A challenge with GenAI solutions is that they are extremely fluent in creating content that reads well. Without sufficient guardrails to guide appropriate use, to specifically train AI on accurate data and to verify the output for accuracy; people can over-trust the outputs GenAI creates.” Timlin also agrees that aspirations of creating AI systems with human-level general intelligence are still a long way off. Eoin O’Reilly, EY Ireland partner and head of data, analytics and AI, agrees GenAI had its “break out” moment with the launch of ChatGPT. He believes that, while the hype cycle around GenAI is slowly beginning to subside, from a business perspective the surface has only been scratched when it comes to its potential for value creation. “At EY we fundamentally believe that AI is changing everything, because AI is becoming part of everything,” he states. “The next decade will see a fundamental transformation of organisations, business models and value creation all underpinned by AI in combination with continued advances in digital, cloud and metaverse solutions.” And despite the hype, O’Reilly warns that “waiting is not a winning strategy”. “Delaying AI will not stop AI,” he says. “And ‘toe dipping’ experimentalism and incrementalism will only mean competitive advantage is being surrendered.” The AI and data team at EY Ireland is working with a host of organisations to embed and harness AI in their operations, including partnering with organisations to improve efficiency in both clinical laboratory and pharmaceutical manufacturing settings and working with State agencies to support them to illuminate and improve their efficiency. “Once you step away from the hype cycle and actually begin working with organisations, it quickly becomes clear that flaws are less around the capability of the technology and more around data quality and implementing a change management process to effectively harness AI,” O’Reilly says. Another concern that organisations may have is trusting the quality and accuracy of the output. Kelly says there is a disconnect between what large language models (LLMs) can achieve and what people think they can achieve, noting that “it’s a very different type of intelligence”. Those who believe a truly super intelligent machine is around the corner “are way off”, he adds. “Just because they do things like a human doesn’t mean intelligence – can a submarine swim, for example? Is a calculator super intelligent? A lot of hype has included some of the really important questions like what it would take to produce something really intelligent, but reliability is a huge problem that people are aware of too.” AI “hallucinations”, where it provides false or misleading information, are years away from being solved, he adds. Yet Kelly is keen to emphasise how “extraordinary” the capabilities of LLMs are from a technical point of view. “They’ve really constituted an exciting leap forward in natural language processing and have surprised a lot of people,” he says. “This doesn’t mean that we shouldn’t treat the very hyperbolic claims made for them, for example, that we are approaching forms of artificial intelligence with the scepticism that they deserve. Technologies always have their politics – they are not neutral, and the implications associated with them are not inevitable – so we need to be discussing these and responding appropriately.”",Ireland,Irish Times; Dublin,2024-12-19,27,1011,2024-12-01,2024.4,Long,34,1,0.9891196834817012,1,0,0,0,1,0.9891196834817012,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0.9891196834817012,1,1,0.9891196834817012,1,7,6.9238377843719086,1,1.227301971594368,Positive
"EDEL CORRIGAN Artificial intelligence (AI) has the potential to change the landscape in how deal-making is carried out and how target companies are evaluated. M&A professionals and due diligence advisers are increasingly using the new technology to enhance their deal-making capabilities. AI has become an influential force in M&A, enhancing the efficiency and accuracy of various processes. AI-powered tools are now employed to conduct due diligence and origination, where they can analyse vast amounts of data at unprecedented speeds, says Grit Young, EY Ireland technology industry leader and valuations partner. “Moreover, AI can assist in the valuation of companies by generating data-driven insights to support the review of financial forecasts and identifying synergies between organisations. Through machine-learning models, AI can help predict the future performance of a target company based on historical data, alternative data and market trends. “Overall, the arrival of AI has improved the accuracy and efficiency of M&A, but also uncovers new dimensions of value creation and risk management.” AI is beginning to make inroads into the delivery of M&A advice, but it is early days in the adoption across the industry, agrees James McMenamin, partner, corporate finance, PwC Ireland. “We see transaction services providers using AI to prepare initial data analysis quickly and it appears to be able to do it to a reasonable base level standard. That said, it requires significant input from specialists to provide any thoughtful analysis and conclusions.” There is a saying in the industry that “AI won’t take my job, but someone proficient in the use of AI will”, says McMenamin. “It is essential that M&A professionals get proficient with the capabilities and limitations of AI because if they aren’t using it, their competitors will, and they will be at a distinct disadvantage. Excitement and apprehension “There is a sense of excitement about AI because we are only just starting to see the potential to increase productivity in the M&A sector.” The reaction to AI in M&A is a mix of excitement and apprehension, agrees Young. “On one hand, professionals can see the potential of AI to revolutionise their workflows, providing enhanced analytics, deeper insights and more precise decision-making capabilities. They recognise the power AI holds in transforming the landscape of M&A, making the process more dynamic and insightful. In the area of origination, it can be a very powerful tool to identify potential opportunities for buyers and accelerate the funnel. “On the other hand, there is a degree of caution. Some individuals fear the unknown, worrying about the implications of relying heavily on AI, such as an over-dependence on technology and diminished human judgment making processes vulnerable to system failures.” Despite these concerns, the overall sentiment leans towards optimism and our clients are asking us more and more about it, says Young. Greater confidence in AI should be fostered as more success stories emerge and the technology continues to evolve. The pros of using AI in M&A include improved data analysis, time savings and enhanced decision-making, says McMenamin. “The main con is the potential for people to rely on the output without doing a careful review and fact check. Human judgment cannot be replaced. “AI in M&A requires a balanced approach, combining advanced technology with human expertise. Organisations must focus on data quality and invest in training to maximise AI’s benefits and mitigate associated risks.” Young agrees that AI has improved the accuracy and efficiency of M&A, but also uncovers new dimensions of value creation and risk management. “AI can help an acquisitive company identify a potential M&A target with the click of a button, a target that may not have been found through the more traditional methods of desktop research and contacting peers/international colleagues. “Routine tasks such as document analysis and compliance checks can be automated with more time to focus on strategic activities. “Paradoxically perhaps, AI can also help assess the risk of a target company being disrupted by AI and can help an expert with scenario planning when assessing forecasts.” On the other hand, one concern Young hears across the industry and from clients is the reliance on the quality of the input data. “Another big factor is the initial implementation costs, but these are starting to reduce as more competition arrives and CEOs, CFOs and CTOs are starting to see the long-term value added and, therefore, return on investment.” Is it the future? AI has the potential to revolutionise our industries and position Ireland and Europe as leaders in this technology frontier, says Young. “We are yet to see significant investment in Ireland’s AI sector, but it could become a European hub given our access to quality talent, location and business-friendly policies.”",Ireland,Irish Times; Dublin,2025-02-21,27,776,2025-02-01,2025.1,Medium,35,5,6.443298969072164,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.288659793814433,1,1,1.288659793814433,1,1,1.288659793814433,1,6,7.731958762886598,1,0.6262297036981702,Positive
"DANIELLE BARRON Last year, the Irish Funds FinTech Specialist Group identified significant opportunities offered by both generative artificial intelligence (GenAI) and more traditional machine learning models. They highlighted a wide range of use cases where the new technology can be utilised but experts say there are significant challenges when it comes to adopting AI within the funds industry. According to Thomas Conlon, professor of finance in the School of Business at University College Dublin, elements of AI are already being used across the investment management sector. “For example, machine learning is being used to help generate excess returns, to optimise portfolios and to manage risk,” he says. “Natural language processing is employed to understand market sentiment, to summarise information from large documents and to automate reporting, and generative AI, such as ChatGPT, is used to create content, to write code and to do scenario analyses.” This, Conlon notes, highlights the wide-ranging applicability of AI to the funds industry but he believes fund management will struggle with adopting AI due to the centralised approach being taken. “While not limited to the US, much of the innovation is coming from Silicon Valley and there has been a focus by many of the big players on black-box models,” he says. This creates challenges for fund managers in protecting their intellectual property (IP) and makes it difficult for them to truly differentiate themselves. ‘Highlighted’ Mary Ruane, partner and asset and wealth management leader with PwC Ireland, also highlights a number of use cases for the technology within the funds industry, noting that these range from simple administrative tasks to free up time, to predictive analytics that offer new insights. “AI and especially GenAI has been highlighted for its role in real-time data analysis, uncovering deeper market insights and improving decision making,” she says. “AI-driven tools streamline operations by automating repetitive tasks such as fund administration, compliance reporting and data reconciliation. Predictive analytics powered by AI aids in portfolio optimisation, proactive risk management and scenario planning.” Additionally, AI simplifies compliance processes by automating regulatory filings and monitoring changes in real time, while the combination of GenAI and blockchain is also revolutionising private markets through tokenisation. Ruane says Ireland’s funds and asset management industry is progressively embracing AI and GenAI, with notable advancements in operational efficiency, data analysis and decision making, but we need to do more to keep pace with this rapidly evolving field. For example, following on from GenAI, the next evolution in artificial intelligence is agentic AI, building on those capabilities of GenAI by introducing autonomous decision making and continuous feedback loops. “In the funds industry agentic AI is seeking to transform operations by enabling real-time data analysis, adaptive investment strategies and automated risk management,” Ruane explains. This shift will allow financial institutions to enhance efficiency, reduce costs and improve customer experiences, positioning agentic AI as a pivotal force in navigating the complexities of modern finance. According to Ruane, continued investment in AI infrastructure, workforce upskilling and fostering innovation across public and private sectors is essential for Ireland when it comes to embracing disruptive technology. “Ireland’s strong tech ecosystem and being at the forefront of the funds and asset management industry provides a solid foundation for future growth, but strategic efforts are needed to fully realise AI’s potential in the industry,” she says. Disruptive technologies such as AI and GenAI, as well as distributed ledger technology (DLT), big data and cloud computing, remain crucial for boosting efficiency and performance and Ruane says their influence is growing. “These innovations are not only enhancing operational capabilities but also transforming offerings, revenue models and business frameworks within the industry,” she says. “A remarkable 80 per cent of the 264 asset and wealth managers we surveyed as part of our PwC Asset and Wealth Management Revolution 2024 Report noted that disruptive technology is fuelling revenue growth – nearly as many as those who observe improvements in operational efficiency. “What’s more, seven out of 10 highlighted the pivotal role of disruptive tech in driving product and service innovation, as these technologies transition from supporting back-office functions to playing a more prominent role in customer interactions.” The danger is, of course, being left behind. “For early-stage adopters there was a competitive advantage to AI but this is rapidly becoming a strategic necessity,” says Conlon. “The prevalence of inexpensive exchange traded funds means that fund managers are now competing on cost and embracing AI allows companies to pass on the benefits of AI-driven automation to customers where it really matters – their pockets.”",Ireland,Irish Times; Dublin,2025-05-21,19,753,2025-05-01,2025.2,Medium,36,3,3.9840637450199203,1,1,1.3280212483399734,1,0,0,0,0,0,0,1,1.3280212483399734,1,0,0,0,0,0,0,3,3.9840637450199203,1,4,5.3120849933598935,1,9,11.952191235059761,1,0.7234447757089367,Positive
"DANIELLE BARRON As the home of the European headquarters of many of the world’s largest technology, digital and cyber companies, Ireland is ideally positioned to be a global digital and AI hub for both innovation and regulation. Recognising the significant opportunity that exists in digital, AI and cyber, businesses are increasingly investing in these areas. According to a recent American Chamber of Commerce Ireland survey, 44 per cent of respondents said they expect that AI transformation will directly result in companies increasing investment in Ireland in the next five years. But how are companies innovating with AI, digital and cyber right now? Is it already transforming their work and what does the future hold? According to Owen Lewis, head of AI and management consulting at KPMG in Ireland, organisations are embracing AI, digital and cyber technologies, making them significant investment priorities. “Our recent CEO Outlook 2024 report indicates that top CEOs are betting big on AI over the next three years, with generative AI (GenAI) in particular ranking in the top three strategies for achieving growth objectives,” he says, adding that this trend underscores the importance of digital transformation in the current business landscape. “CEOs in Ireland recognise that these technologies are not just tools for efficiency but are critical for staying competitive and resilient in a rapidly changing environment.” AI and digital tools are already reshaping business operations and the everyday work of employees, Lewis points out. “We’ve seen AI implemented to enhance decision-making processes, automate routine tasks, and improve customer experiences, and this allows teams to focus on more strategic and creative tasks, increasing overall productivity and job satisfaction,” he says. And looking to the future, the opportunities are huge, he believes: “As AI continues to evolve, it will open new avenues for innovation and growth. Irish businesses that invest in the technology will be better positioned to develop new business models, enhance customer engagement and drive operational efficiencies.” Essential, not optional “Investment in AI, digitalisation and cybersecurity is now essential, not optional,” agrees Leo Moore, partner and head of William Fry’s technology group. These technologies drive agility, efficiency and cybersecurity, crucial for competitive advantage, he says. “Our clients, especially in regulated sectors, invest heavily in AI for operational resilience and regulatory compliance.” Emerging AI applications – for example, agentic AI, digital twins, on-device processing and hyper-personalised customer engagement – offer immense potential for innovation, productivity, efficiency and competitive advantage across many sectors such as healthcare, finance, retail and logistics, says Moore. Carmel Condon, senior director of tech, at Lilly, says: “Our organisation has adopted a digital-first approach to our new manufacturing facilities and is driving an innovative digital transformation agenda in existing manufacturing facilities. “Key focus areas include paperless operations, automated processes and data utilisation to drive efficiency, enhance compliance and drive greater process insights.” The company also leverages AI in many forms across the business. “At its most basic level, for co-authoring of documents locally here in Kinsale, it has become most valuable in providing advanced analytics for greater process insights,” says Condon. Lilly is investing more than $2 billion (€1.88 billion) in Lilly Limerick, set to be one of the most digitally advanced plants in the Lilly manufacturing world, and it is on track to be making medicine in 2026. “The Limerick manufacturing site will be a fully integrated digital site with the latest machine automation, manufacturing execution and data technologies designed to ensure product and data quality,” says Condon. Companies embracing digital transformation tend to achieve superior productivity, adaptability and regulatory compliance, adds Barry Scannell, partner in the William Fry Technology Group. He says those lagging in digital engagement and/or transformation will encounter technological disadvantages and may even face regulatory compliance challenges, “in either case risking competitive disadvantage”. “With AI’s rapid technological progression, slower adopters may struggle to adapt to imminent advancements,” says Scannell. “Digital transformation isn’t just current best practice; it builds critical agility, as we see at William Fry through our own AI initiatives.” But apprehension is understandable and trust in AI cannot be assumed – it must be earned, says Emmanuel Adeleke, partner in technology and transformation engineering, AI and data at Deloitte. This is where transparency and regulation come into play. “The EU’s AI Act is one of the most comprehensive efforts to ensure AI is used responsibly,” explains Adeleke. “By categorising AI systems based on their risk and requiring transparency about how they work, the Act aims to build a more robust framework for AI deployment. It’s a very necessary step to address concerns around privacy, data security and bias.” According to David Kirton, partner in IP, technology and DP at Eversheds Sutherland, the use of AI tools by Irish businesses, and particularly the use of generative AI, has skyrocketed. Foster innovation “Deployed effectively, AI technologies can deliver huge efficiencies, foster innovation and support human intelligence in a way that we as a society have never before experienced. “At one end of the scale, many Irish businesses have created new roles entirely focused on delivering AI and helping their organisations take advantage of the benefits and opportunities it offers,” he says. “But other organisations have taken a much more cautious approach and their level of AI adoption is currently very limited.” From a regulatory standpoint, Kirton says there is a huge opportunity for Ireland to become a standard-setter. “A number of the pioneers in this space have their European headquarters in Ireland,” he says. “This means that Irish regulators, such as the Data Protection Commission, have a vital role to play in fostering innovation and allowing organisations to reap the huge benefits of AI, while ensuring that these technologies are designed and deployed in a way that respects fundamental rights.” Businesses must also take a proactive role in ensuring trustworthy AI adoption, he stresses, adding that ensuring AI is used in conjunction with – rather than in place of – human judgment remains crucial. “Every single tool, whether it’s a hammer or an algorithm, can be used in a benign or malignant way,” says Kirton. “Building trust in AI will continue to remain a human challenge. Some of the big concerns we hear are about the potential for AI to create misinformation. However, these are human-created algorithms. This is about enhancing human expertise, not replacing it.” Jason Ward is vice-president EMEA North and managing director of Dell Technologies Ireland. He says AI is no longer a distant concept and is already reshaping Ireland’s business landscape at an unprecedented pace. “The shift towards accelerated computing is revolutionising productivity levels, much like the industrial revolution, with potential gains of 20-40 per cent,” he says. “AI is also modernising the workplace, enabling employees to focus on higher-value tasks. But it will require continuous innovation and investment in skills and infrastructure to fully unlock the opportunities of our AI era.” Dell is helping businesses and organisations to unlock the power of this transformative technology, says Ward. For example, Dell’s Customer Solutions Centre Innovation Lab in Limerick provides a space for customers to see how GenAI can benefit their business. “GenAI can create virtual prototypes, allowing engineers and designers to evaluate and refine suggested designs before moving to physical prototyping,” says Ward. “This approach can significantly reduce time and costs associated with the traditional product development cycle.” The blistering pace of AI evolution means the near future holds even more promise. AI at the edge presents significant opportunities for enhancing the capabilities and performance of humanoid robots, Ward explains: “Edge AI allows humanoid robots to process data and make decisions locally, without relying on cloud connectivity. Quick processing of sensor data enables robots to detect and respond to potential hazards or unexpected obstacles in real time, enhancing overall safety in human-robot interactions.” Workhuman’s investment in AI is leading to quite dramatic efficiency gains. “This year, we’ve continued to drive internal innovation, implementing an advanced AI solution for our Customer Service teams,” says Shmulik Barel, vice-president of product engineering at the company. “Our latest AI application has automated 50 per cent of administrative tasks, transforming the efficiency of our email and phone case management. By streamlining these processes, our teams can now focus more on delivering high-quality customer support while handling cases faster and with greater accuracy.” This new AI system allows agents to instantly categorise incoming inquiries, prioritise urgent cases and respond to common questions with AI. “Other key features included sentiment analysis, case routing and automated resolutions, enabling our agents to focus on higher-value tasks,” Barel adds. “This has not gone unnoticed; our team is a finalist in the AI Ireland Awards for best application of AI in consumer/ customer service.” Analytic solutions AI has also transformed how WorkHuman serves its clients. “We offer our own AI-powered analytic solutions that help them leverage data to make strategic decisions about their workforce, improve employee engagement and ultimately achieve better outcomes in their organisations,” says Barel. “With that, our operations are now all AI-focused, externally and internally, making us a truly AI-first company.” ServiceNow’s AI platform helps customers across all industries keep pace with innovation and transformation for their business, including AI, digital and cyber technologies. “They’re not just tools – they’re the lifeblood of our business,” says Mark Cockerill, senior vice-president, legal, at ServiceNow. “As our own ‘customer zero’, we’re at the forefront of using our own solutions to integrate AI, digital and cybertechnologies across all business units and teams.” This includes enhancing customer service through generative AI-driven case summaries and chatbots with Now Assist, to fortifying its security and risk management with advanced cybertechnologies such as machine learning for threat detection. “Now Assist for Security Operations (SecOps) harnesses the power of AI to accelerate response times and swiftly contain security threats,” says Cockerill, noting that more than 85 per cent of Fortune 500 companies have used this platform. Across industry, AI is taking over mundane tasks, freeing people to focus on more complex and impactful work, he adds. As part of ServiceNow’s last platform release, Xanadu, the company rolled out powerful new data enhancements that transform how employees work. “The RaptorDB high-performance database empowers teams to quickly filter and process the data that matters most, turning even the most complex queries into lightning-fast responses,” says Cockerill. “Early-use cases have seen phenomenal results and transaction times have sped up by 53 per cent.” A crucial aspect of this technological revolution is the need to reskill teams for the future of work, he adds: “ServiceNow recently shared its commitment to reskill three million learners globally by the end of 2027 through AI learning tools. We all need to play a part in supporting teams through this rapid pace of change.” KPMG’s Lewis warns, however, that the focus on cybersecurity will become even more critical as digital threats continue to evolve: “Leaders who prioritise cybersecurity will not only protect their assets but also build trust with their customers and stakeholders.” The impact AI is having on cybersecurity and the wider technology ecosystem cannot be understated, agrees Carla Baker, senior director, public policy UK and Ireland at Palo Alto Networks. “We are seeing first hand how AI is amplifying the scale and speed of attacks for threat actors, as well as revolutionising threat detection and defence strategies, and giving cyber defenders the upper hand against malicious actors,” she says. “This process of continuous discovery and analysis allows threat detection to stay ahead of the adversary. This real-time awareness of the threat landscape allows our company to block over 11.3 billion attacks each day. This would simply not be possible without AI.” Of course, cyber adversaries are already using AI to advance their tradecraft. “Today, threat actors are leveraging AI to conduct reconnaissance, enhance social engineering, develop malicious code, automate vulnerability exploitation, create deepfake attacks and launch prompt injection attacks,” says Baker. “With increases in cybersecurity threats and greater use of digital technology, there is a continued focus on cybersecurity advancement, and ongoing investment is a key part of that,” says Lilly’s Condon. “The cybersecurity landscape will continually evolve. Opportunities in this area include enhancing our cybersecurity infrastructure to protect against evolving threats, implementing advanced threat detection and response systems, and investing in continuous upskilling of staff to be alert to cyber potential risks.”",Ireland,Irish Times; Dublin,2024-11-21,59,2036,2024-11-01,2024.4,Very Long,37,23,11.296660117878194,1,1,0.49115913555992136,1,3,1.4734774066797642,1,0,0,0,3,1.4734774066797642,1,0,0,0,2,0.9823182711198427,1,13,6.385068762278978,1,23,11.296660117878194,1,16,7.858546168958742,1,2.3423517529364815,Positive
"DANIELLE BARRON Whether you are excited by it or frightened of it, artificial intelligence (AI) is well on its way to transforming how we do business. Many US multinationals based in Ireland are engaged in AI research which will help improve insights into what these changes may look like. However, a recent survey found that 69 per cent of AmCham members have been experiencing a skills gap in Ireland, with 40 per cent experiencing a skills gap in digital or AI-related fields. Experts say Ireland must quickly build momentum in research and development in this critical area, while seeking to address the skills gap as a priority. For years the perception of artificial intelligence was that it was “futuristic” but now the future is here and AI is already in use in many larger organisations, says Barry Scannell, consultant in the technology department at William Fry. “And if not already deployed across the organisation, those systems are in development. We have seen what a number of multinationals are working on in terms of AI and the direction of travel is clearly ubiquitous AI deployment,” says Scannell. Bigger organisations “have the deeper pockets”, allowing them to buy into these systems during the early development stages and manage risk more comfortably, he points out. AI was previously trained on task-specific data to perform a narrow range of functions but now large-scale, adaptable AI models, known as foundation models, have entered popular use – ChatGPT being a notable example. “The future is likely going to have myriad foundation models incredibly fine tuned to very specific tasks which will be deployed in every single business, in addition to the major foundation models that the likes of OpenAI, Meta and Google are developing,” says Scannell. That said, we remain in the “embryonic” stages of this technology, which means it may not always be deployed where it is most useful, Scannell explains; many companies are still figuring out use cases and how AI could help with their business. ‘Buzz word’ “The risk with AI is that, as it’s such a buzz word, boards wielding an AI hammer may see every process as an AI nail,” he adds. Having an appropriate AI framework in place is the best position from which to start, Scannell advises. “This means assessing the technology, its intended use and possible consequences of use [and] everything from fundamental rights to data protection to liability,” he says. “Part of this process is ensuring that existing legal frameworks such as contracts, policies, etc are updated to accommodate this new technology. The AI Act is also looming large on the horizon and companies need to start addressing potential obligations now as they may not have enough time when the legislation comes into force.” With the EU’s Artificial Intelligence Act imminent, Scannell says there is “absolutely no reason” why Ireland should not become the global hub for AI regulations. “Ireland has the potential to establish itself as a global hub and centre of excellence for the next AI-powered evolution but that potential may go unrealised if we fail to develop the skills required to compete,” says Sinead Keogh, the director of the Ibec’s medtech and engineering sectors. Ibec’s own research has found that more than half of companies within the health-technology sectors have identified AI as a current and future skills need. Keogh points to the potentially transformative impact that AI will have in the life-sciences sector. US-headquartered companies such as Amgen, Pfizer, MSD, Janssen and Eli Lilly are just a few organisations exploring AI for drug discovery, clinical trials, diagnosing diseases and manufacturing. “Practically, AI is augmenting human productivity, improving decision making, reducing manual groundwork, automating processes and freeing up time for more valuable work,” she says. Having the talent to navigate the rapidly evolving regulatory environment will be critical, she adds. “Skills across the value chain will be incredibly important, especially an understanding of the complex regulatory and reimbursement environment within the EU and US, in particular, for digital solutions,” says Keogh. Goldman Sachs estimates that AI could increase productivity by 1.5 per cent annually and S&P profits by 30 per cent or more over the next 10 years. “With that kind of prize on offer it’s imperative that Ireland is positioned as a leader in these skills. We have great foundations in place that provide the opportunity to realise this goal,” says Kieran Towey, applied intelligence lead at KPMG. Results from the KPMG CEO Outlook 2023 highlighted how global CEOs are making generative AI a top investment priority; 70 per cent are investing heavily in generative AI as their competitive edge for the future, with most (52 per cent) expecting to see a return on their investment in three to five years. “Virtually all organisations are carrying out internal research or working with external partners, including the university sector, through organisations like CeADAR, Insight and SFI’s Machine Learning Labs, and this will continue,” says Towey. Organisational success But, again, when it came to the barriers to organisational success caused by generative AI, chief executives surveyed by KPMG as part of CEO Outlook 2023 cited a lack of technical capability and skills to implement it. “In fact, a lack of technical capability and skills to implement AI was flagged as an issue by exactly half of CEOs worldwide, and 60 per cent of Irish CEOs,” he says. Yet Towey points out that there has been “an explosion” in third-level courses devoted to data, analytics and AI, which are attracting both domestic and international students. “Ireland is also an attractive destination from Europe and beyond for those with these hot skills and, for our size, we are very much punching above our weight,” he adds. However, with such nascent and dynamic technology, organisations may currently lack the more senior visionary resources to effectively design and deliver solutions. “I expect to see organisations look to concentrate their limited resources into a critical mass of capability, working in a hybrid environment and coming together in person when required,” says Towey.",Ireland,Irish Times; Dublin,2023-11-23,24,1002,2023-11-01,2023.4,Long,38,2,1.996007984031936,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.998003992015968,1,4,3.992015968063872,1,5,4.990019960079841,1,5,4.990019960079841,1,1.6674216091350818,Positive
"Scarcely six months after the world’s first comprehensive AI legislation entered into force, heads of state, chief executives from companies such as Google, Microsoft and OpenAI, senior government officials and computer scientists from around the world gathered in Paris last February to attend the AI Action Summit. “I’m not here this morning to talk about AI safety,” US vice-president JD Vance said on his first overseas trip. “I’m here to talk about AI opportunity.” Taking aim at the Act, he set out the new administration’s position: “We believe that excessive regulation of the AI sector could kill a transformative industry just as it’s taking off.” At the centre of the Act is a voluntary code of practice for general-purpose AI models, which is being drawn up by independent experts and based on submissions from industry stakeholders, academics, civil society, representatives of member states, as well as European and international observers. For AI models that may carry systemic risks, it sets out how providers should assess and mitigate these risks. It is also intended to spell out how developers should respect intellectual property rights when training AI systems. But finalising the code has become a slow-motion tug of war. Behind closed doors, some in Brussels acknowledge that US pressure, and growing nervousness inside certain EU capitals, has complicated efforts. Poland’s recent call for a “stop-the-clock” pause on parts of AI Act implementation reflects this unease. Warsaw’s intervention is widely seen as an early warning sign: some member states fear the EU may be moving too far, too fast, or in ways that could trigger trade disputes to add to those it can already ill afford. Meanwhile, Europe’s creative sectors feel increasingly abandoned. Artists, musicians, writers, journalists and other creators see their work ingested to train generative AI systems, systems that can now produce convincing imitations of human-created music, images and text in seconds. This is done without their consent, credit or compensation. Copyright law The European Commission insists it is bound by existing copyright law, arguing that the AI Act cannot go beyond the EU copyright directive which provides for an exception whereby copyrighted works can be used for scientific research purposes without permission from copyright holders. As there is no basis for them to object, there is no basis for copyright holders to request payment for the use of their material. Although that directive was only passed in 2019, copyright holders say nobody had AI in mind at the time. Many creators placed their hopes on the AI Act delivering accountability for how models are trained on their works. As that possibility recedes, we now hear promises that a broader review of copyright rules may come next year. In the absence of legislative intervention, it will fall to the courts to determine whether AI companies should be allowed to train models on copyrighted works without permission, and if not, what rules on transparency, licensing or opt-outs will ensure creators are fairly treated. In the US, a similar, so-called “fair use” exemption is the subject of increasing litigation, including a New York Times case against OpenAI which is the most high-profile example. Indian news outlets and book publishers who say the firm uses their content without permission to help train its ChatGPT have taken a high court challenge that could reshape how the sector operates there. When it comes to the media sector, the stakes are not just economic but democratic. AI-generated content stripped of attribution threatens the ability of news publishers to generate any revenue from their content and, by extension, the media pluralism indissociable from a democratic society. While some larger, stronger news publishers, including the New York Times are signing licensing agreements with AI, others who lack their heft are being pushed out. The Reuters Institute’s annual Digital News Report, which was released last week, found that a growing number of people are using AI chatbots to read headlines and get news updates. While only 7 per cent overall say they use AI chatbots to find news, that number rises to 15 per cent of under-25s. The commission deserves credit for having had the courage to propose regulation to mitigate the risk posed by AI, as pointed out by respected computer scientists both in academia and industry. But the legislation fails to determine definitively how the balance should be struck between the development of European AI, trained on European data so as to reflect European values and culture, and protecting Europe’s creative and media industries. Fragile compromise The code of practice was meant to offer at least partial answers. Today, it risks becoming a fragile compromise text pulled between Silicon Valley warnings of regulatory overreach, and rights holders warning of economic erosion already under way. The European Parliament sits uneasily in the middle. After a hard-fought compromise, it is naturally inclined to defend the legislative package it passed so recently. But we are also being asked by citizens, creators and member states alike to fill the legal grey zones that are already opening. And to do so in the face of the prevailing zeitgeist, which is one of deregulation and simplification. The window to get this right is narrowing. Copyright is not a mere technical issue. It strikes at the heart of Europe’s cultural sovereignty and democratic resilience in the AI era. Michael McNamara MEP is co-chair of the European Parliament’s working group on the Implementation and Enforcement of the AI Act",Ireland,Irish Times; Dublin,2025-06-24,25,904,2025-06-01,2025.2,Long,39,6,6.6371681415929205,1,0,0,0,2,2.2123893805309733,1,0,0,0,0,0,0,0,0,0,0,0,0,7,7.743362831858407,1,3,3.3185840707964602,1,1,1.1061946902654867,1,-1.4810719043673786,Negative
"JILLIAN GODSIL Artificial intelligence (AI) is not one thing. It now appears in distinct forms: Generative artificial intelligence (gen AI), agentic AI, multimodal AI, small language models (SLMs), and self-improving models. Each serves a specific function, from content generation to co-ordination of agents or processing diverse data types. Irish organisations are adopting different forms based on accuracy, control, and practical deployment. Rory Timlin, Data and AI Practice lead at KPMG in Ireland, points to early success with gen AI. “In customer operations, gen AI enhances query understanding and response formulation,” he says. “It improves first-time resolution, prompts next-best actions and automates post-call summarisation.” Beyond customer service, Timlin highlights uses such as “content generation, localisation, translation and document summarisation,” where point solutions are already active and producing measurable gains in speed and consistency. Alessia Paccagnini, associate professor at UCD’s Smurfit Business School, highlights the benefits of SLMs in settings with limited computing power. “SLMs offer faster processing speeds and lower computational costs,” she says. These features make SLMs especially suitable for edge deployment, that is, running locally on a device instead of relying on the cloud. Environments such as wearable health monitors, factory sensors and customer service kiosks are prime candidates for this type of model. Ivan Jennings, senior manager for Solution Architecture at Red Hat, describes how SLMs function in enterprise systems. “SLMs are easier to manage, customise and control,” he says, also stressing their role in structured orchestration. “Each one performs a specific step. For example, summarising, classifying, or routing. You can inspect the model weights, set rules and adapt prompts which is essential for enterprise use.” Jennings underlines the need for transparency in environments that demand compliance and operational traceability. Multimodal AI brings a different capability: the ability to process more than one type of input simultaneously. “These systems process and combine multiple types of data, text, images, audio, and video, for more integrated tasks,” says Paccagnini. She identifies high-potential areas such as education, customer support, healthcare and media. Emmanuel Adeleke, partner in Technology and Transformation at Deloitte Ireland, agrees: “They integrate and process diverse data types enabling a richer and more nuanced understanding of complex scenarios.” In academic and business contexts, multimodal AI is being explored for its ability to unlock insights that single-mode models cannot reach. Adeleke and Paccagnini also focus on self-improving models. These systems refine their performance over time without explicit reprogramming. “Self-improving AI models continuously learn from data and feedback,” Adeleke says. “They enable personalised experiences and process optimisation, especially in dynamic environments.” This continual learning loop is valuable in sectors such as logistics, risk management and finance, where conditions change rapidly. Paccagnini says: “Self-improving AI models can adapt and learn continuously from new data or user feedback, leading to improved performance over time without human intervention”. She lists practical applications such as recommendation systems, fraud detection, predictive maintenance and dynamic pricing strategies. These use cases rely on the system’s ability to adjust, respond and improve based on patterns it encounters in real-world usage. Richard Blythman, founder of NapthaAI, an Irish AI start-up, focuses on agentic AI where systems composed of decentralised agents work together to complete tasks. “There is a lot of confusion over agentic AI, generative AI and multimodal AI,” he says. “Many terms have been introduced very recently and often get bundled together. You can have a generative agent or an agent that performs tasks that are not generative.” Blythman’s point is that agentic AI is not defined by content output but by autonomous co-ordination and task execution. At NapthaAI, he is building what he describes as “an internet of agents.” These agents are designed to operate across devices and co-operate through a networked structure. “Agents run on many devices, often geographically dispersed, yet co-operate with each other across the network.” Blythman cites education as an example: “The tutor agent might work together with coding agents making it a simple example of a multi-agent system,” he says. Each agent has a clear role and operates as part of a broader structure, rather than relying on a centralised model. Each AI type plays a different role. Generative AI handles content creation, agentic AI manages co-ordination and multimodal AI brings together diverse inputs. Self-improving models evolve based on use. SLMs bring these capabilities closer to the user with lower cost and greater customisation. The experts agree that modularity and control are emerging as defining principles in this next phase of deployment. “Each one performs a specific step,” says Jennings. In tightly governed organisations, the ability to inspect, modify and isolate each stage of an AI pipeline is non-negotiable. Timlin notes that even initial deployments are delivering clear operational benefits. Adeleke and Paccagnini also emphasise the long-term adaptability and autonomy, while Blythman points to the growing importance of co-operation between agents. These are not speculative developments. Across public and private sectors, models are already being embedded, not to replace full workflows but to enhance parts of them. The idea of one model to do everything is being replaced by precise orchestration: each tool performing one function well, integrated into a system that is observable, tunable and evolving. The strength of this approach lies in clarity. Businesses now choose AI tools based not on novelty but on alignment – does the model fit the task, can it be governed, and will it adapt? Artificial intelligence is not just changing. It is specialising.",Ireland,Irish Times; Dublin,2025-07-25,18,898,2025-07-01,2025.3,Long,40,1,1.1135857461024499,1,0,0,0,1,1.1135857461024499,1,0,0,0,1,1.1135857461024499,1,0,0,0,1,1.1135857461024499,1,4,4.4543429844097995,1,0,0,0,0,0,0,1.1245928634310174,Positive
"BARRY MCCALL The hype surrounding all things AI following the launch of ChatGPT in late 2022 has abated somewhat and organisations and commentators are taking a more realistic view of the technology. While most surveys indicate that a majority of businesses intend to adopt it in the coming years, many organisations are still struggling to identify practical use cases for it. The pace of AI adoption is relatively slow, and doesn’t match the high expectations for the technology, according to David Lee, chief technology officer with PwC Ireland. “PwC’s 2025 GenAI Business Leaders survey showed that 86 per cent of Irish business leaders believe that the overall impact of AI on Ireland’s economy in five years’ time will be positive; while 82 per cent believe that AI will deliver increased efficiency in their employees’ time at work. However, scale adoption is at a slow pace – while two thirds (67 per cent) are either at the testing or partial implementation stages of AI adoption, only 6 per cent of respondents in the same survey reported widespread adoption of AI. “The gap we are seeing here is reflective of the maturity of adoption in Irish organisations, and as more organisations progress through their journeys from testing AI through to wider-scale adoption, we expect to see this gap decrease.” This gap between enthusiasm and application is quite natural and has been witnessed in previous waves of technology, according to Liam McKenna, partner in the consulting practice in Forvis Mazars, who points out that the time taken to bridge the gap is reducing. “It took a long time from the development of the PC to when the spreadsheet emerged as the killer app,” he notes. “And it took 10 to 15 years for businesses to make money on the internet for the first time. The cycle was much shorter after the introduction of the smartphone. The cycle is similar, but the time span is reduced. We expect that within the next three to five years we will see fundamental change in business and organisations of all types as a result of AI.” McKenna refers to the Gartner definition of the “hype cycle”, which is composed of five phases: the technology trigger, the peak of inflated expectations, the trough of disillusionment, the slope of enlightenment, and the plateau of productivity. “We are going through it now with AI, but it will be much shallower. There are practical use cases at the moment. For example, we are working quite a lot with hospitals where it is being used to enable doctors to spend more time with patients. We are also seeing organisations with really clear and well understood business cases which have been able to roll out the technology successfully.” The majority of organisations are still at the innovation and exploration phase of their AI journeys, adds Martin Duffy, head of GenAI with PwC Ireland. “Business leaders are approaching AI adoption in a considered manner,” he says. “Businesses have worked hard to establish relationships of trust with their staff and customers, and they want to ensure that these are sustained on their AI journey. So, they are cautiously experimenting with the technology. They are learning from their innovation activity that the safe and successful deployment and sustained adoption of AI is a complex process that requires planning and co-ordination across the organisation.” There are variations in AI adoption depending on geographical location, firm size and by economic activity, says Erik O’Donovan, head of digital economy policy with Ibec. “The EU lags the USA in relation to AI adoption and there are geographical divides in adoption across the EU itself. AI is beginning to deliver financial impact across business functions, but most companies are early in their journeys. In Ireland, AI adoption in SMEs and the public sector tends to lag [behind] large enterprise. AI use is currently more intensive in the ICT sector, professional, scientific and technical service activities.” Economic enthusiasm for AI is based on productivity and trade opportunities, he adds. “However, that means enabling conditions where opportunities can be understood and realised are not a given. Regulatory uncertainty; lack of capacity, including skills; and access to capital are challenges.” Prof Andreas Hoepner of the UCD School of Business isn’t sure that the gap is as big as many people think. He points out that there is a difference between using AI in areas such as marketing or customer support and deploying it for new product development or operational efficiencies. “Maybe they don’t want to talk about that until they are sure they are ahead of the competition,” he says. “It is similar to IP [intellectual property]. If it is good IP, they want to keep it secret for as long as possible. There may be a lot more going on beneath the surface than we know about.” On much the same note, David O’Sullivan, director, consulting, with Forvis Mazars, points out that the number of active users of GenAI is increasing and that there is still growth in shadow use, where people in organisations are using it without necessarily having authorisation to do so. “A report by Trinity College Dublin and Microsoft showed a lot of use without policies being in place in organisations,” he says. “Organisation-wide adoption hasn’t reached the level people thought would happen, though.” He believes this is likely to change quite quickly. “The technology is advancing very quickly. Every day there are announcements about new products and capabilities. For example, agentic AI is an enhanced version of robotic process automation and people will be able to understand it better and build it into workflows.” Successful AI adoption should start slowly, says PwC’s Lee: “Businesses should walk before they run with AI and identify the use cases that are easy wins. Look at your workflows and see where AI can enhance or streamline a process by either focusing on increased accuracy, efficiency or reduced time or cost. Once you have tackled that, you can move on to more complex processes. The key thing is that the AI adoption journey involves all of your people to help identify where AI can make a difference.” O’Donovan also emphasises the people aspect. “AI adoption and a supporting data governance culture are more than an IT implementation process; this is about change management and people,” he says. “Organisations, management and employees must understand the AI value proposition and any risks and learn new ways of working and skills to empower them to deliver that proposition.”",Ireland,Irish Times; Dublin,2025-07-25,26,1081,2025-07-01,2025.3,Long,41,1,0.9250693802035153,1,0,0,0,0,0,0,0,0,0,1,0.9250693802035153,1,0,0,0,2,1.8501387604070305,1,3,2.7752081406105455,1,5,4.6253469010175765,1,4,3.700277520814061,1,0.5353606373788468,Positive
"EDEL CORRIGAN With GenAI tools such as ChatGPT, Gemini and Perplexity becoming household names, and business adoption increasing, how can organisations – and their employees – ensure that GenAI is safe to use in the workplace? Generative AI is revolutionising the workforce by augmenting how we work rather than replacing the human role, says Nicola Flannery, digital trust and transparency lead at Deloitte Ireland. “This technology is transforming job functions and creating new roles, requiring a reskilling of 40 per cent of the global workforce. While 300 million jobs globally are impacted, the emergence of 97 million new roles is anticipated. “The success of integrating GenAI hinges on a renewed focus on human-centric approaches, ensuring that technology complements rather than competes with human capabilities.” Generative AI, if done properly, can have a significant impact on the workforce, agrees David Sullivan, director, privacy, digital trust and AI governance, at Forvis Mazars. “Many are using GenAI for various aspects of their role and at varying levels. Some roles are more appropriate for GenAI than others, but it can have its place nearly everywhere. We are seeing it across multiple sectors, including the public sector, healthcare, financial services and others.” O’Sullivan says a multitude of use cases is being seen, the most successful being specific and designed to solve a particular problem. Adoption of GenAI is increasing year on year; however, many organisations still have either no policy at all or a restrictive one that does not allow for the use of GenAI, says O’Sullivan. “The cat is out of the bag in many ways, so despite policy or lack thereof, many of the workforce are using GenAI anyway, known as ‘shadow AI’.” In Ireland, 73 per cent of adults are aware of Gen AI, with almost half having used it, according to the most recent Deloitte Digital Consumer Trends report, Flannery says. “However, usage remains sporadic, with 46 per cent using it less than monthly due to concerns and issues.” Despite this, two-thirds of those using GenAI for work report a productivity boost, although only a quarter of companies actively encourage its use, she points out. Businesses are increasingly recognising the potential of Gen AI, yet many lack formal policies to guide its integration, says Flannery. While 24 per cent of companies encourage its use, there is a need for strategic planning to harness its benefits fully. Business leaders are urged to develop AI policies, redesign roles to incorporate a human/AI mix, and identify new skills required. “Effective change management, including training and support for innovation, is crucial for smooth integration. Transparency and communication are key to addressing concerns about job displacement, as 60 per cent of respondents worry about AI reducing job availability.” Many businesses are still early in their AI journey and are only starting to think of the different ways they can use it as they try to understand the risks, how to manage them and what governance is needed, says O’Sullivan. “As AI agents become more common, it is possible we will see a change in how workflows are designed as they try and leverage the technology in more detail.” The benefits depend on the use case, but there are clear advantages in many areas, including research, data analysis, creating new content, reviewing content, and even getting started with building new strategies, he adds. “For individuals, proper use of GenAI could free up time to focus on more value-adding work that is more fulfilling and meaningful.” Key risks are a lack of AI literacy/fluency leading to over-caution in the use of AI or misuse and over-reliance without understanding the risks, cautions Flannery. “A lack of data governance, including data integrity, can also lead to misinformation or hallucinations, as well as data privacy and security risks. Other risks can stem from bias in training data, lack of transparency and ethical and legal challenges.” The key to implementing Gen AI safely is to ensure a risk-benefit analysis is carried out, and having a transparent AI strategy hinged on the value that can be realised from Gen AI. “Clear usage policies are important, [as is] ensuring a trust-by-design approach is taken to any AI development or vendor policies,” says Flannery. “Other important considerations are investment in upskilling and training, strong data governance and controls, sandboxes for safe experimentation, use cases and proofs of concept that build in human-centricity and transparency, and risk analyses which take bias, ethics, data privacy and security into account.” A starting point is to examine the principles of trustworthy AI from the European Commission that the Irish Government adopted in its guidelines on the use of AI in the public service, says O’Sullivan. “Making sure there is a strong policy, good governance and effective training are all key to safely using GenAI in business. If done properly, this will allow accountability for decision making and use of AI, compliance with various laws and alignment with corporate strategy. “Since February this year, AI literacy training has been a requirement under the EU AI Act, but despite compliance requirements, good training is very beneficial to ensuring that organisations maximise their investment while managing risk.”",Ireland,Irish Times; Dublin,2025-07-25,26,855,2025-07-01,2025.3,Long,42,11,12.865497076023393,1,3,3.5087719298245617,1,8,9.35672514619883,1,0,0,0,0,0,0,0,0,0,0,0,0,12,14.035087719298247,1,3,3.5087719298245617,1,3,3.5087719298245617,1,0.7122350082650398,Positive
"DEANNA O’CONNOR With a booming tech ecosystem, world-class universities and many of the world’s leading technology companies already operating here, Ireland has the potential to be a world leader in research and development (R&D) in artificial intelligence (AI). Yet while the foundations are solid, experts agree that real leadership in this critical field will depend on strategic action, now. We have a strong base, but there is no room for complacency in this fast-moving field. “Ireland is already a natural home for research and development around artificial intelligence,” says Dr Patrick Mannion, lecturer in computer science at the University of Galway and programme director of its postgraduate certificate in AI for managers. He points to the presence of data centres, engineering labs and multinationals with strong R&D operations, noting that “there is a lot of AI-related investment here already”. According to Ibec’s head of digital economy policy, Erik O’Donovan, the Irish ICT sector made up a striking 35 per cent of the Republic’s gross value added in 2022, employing around 90,000 people. When digitally intensive sectors are included – those that rely heavily on digital technologies – the jobs figure triples, he estimates. But for the State to move from a fast follower to a global leader, public policy needs to create a supportive environment. “Despite our strengths, we cannot be complacent,” warns O’Donovan. “Industry investment in innovation makes up most of the total expenditure on research and innovation (R&I), while public investment lags.” In 2021, ICT accounted for 43 per cent of total business R&D expenditure and 45 per cent of total R&D personnel. This imbalance, he argues, must be addressed. “Supports for research and innovation are underutilised by industry due to the administrative burden and challenges in navigating the system,” says O’Donovan. “Ireland must enhance regulatory capacity, co-ordination and simplicity to support certainty and innovation.” O’Donovan outlines several key steps to strengthen the State’s AI capacity: boost public investment in strategic digital R&I; make R&D tax credits more accessible to the private sector; launch a new programme for research in third level institutions; and channel National Training Fund resources into research talent development. Crucially, he stresses the need for a “strategic approach to addressing gaps in AI skills that mobilises and co-ordinates the whole education and training system”. Mannion echoes this view. “We need to emphasise upskilling and reskilling the workforce for the types of new AI-related roles that will emerge in the coming years,” he says. While Ireland has one of the highest third-level education completion rates in the world, the speed of AI development means ongoing learning must be central to the national approach. Although the Republic has an enviable track record in technology, to lead in AI it needs to focus its efforts. Rory Timlin, data and AI lead at KPMG Ireland says, “it is important for Ireland to pick the right fields and sectors in AI in which we will play strongly and differentiate ourselves.” He sees the greatest opportunities in sectors where Ireland already has global standing: pharmaceuticals, medtech, agri-food, and tech operations. “Many of the factors that have helped Ireland’s position in the modern cloud-based technology sector hold for growing AI R&D,” says Timlin. The State’s emerging role as a centre for digital regulation in the EU, combined with its position between the US and European markets, gives it “a natural advantage” in navigating the future of AI governance. Talent remains a defining factor in AI progress – but also a potential stumbling block. AI Ireland founder Mark Kelly warns that competition for top researchers and engineers is fierce, with leading experts “always open to the next move” to stay at the cutting edge. Developing a healthy AI ecosystem also requires supporting emerging talent – not just elite researchers. “One of the challenges is probably at the junior level,” Kelly says. “Getting roles as entry-level data scientists is very, very difficult, because people want experience. It’s a kind of catch 22: how do I get experience if I don’t already have it?” His advice to budding data scientists: personal projects. More companies like Intercom, which is building out a strong generative AI team in Ireland, are needed to grow the AI ecosystem. “If you have more of those companies here, then the ecosystem gets stronger and stronger,” he says. However, without deliberate effort to support the formation of clusters, Ireland risks losing out. “It’s really important that these AI roles are based here,” Kelly stresses. “Otherwise, you’ll see talent migrating to countries like Portugal, Spain or Finland, where clusters are forming.” Beyond talent, data infrastructure is another area where Kelly sees a need for urgent action. “We need a better, more systematic approach to data management in Ireland,” he says. Without it, Ireland’s future job market and innovation capacity may suffer. AI, he adds, could even play a role in optimising Ireland’s energy grid to support this. The complexities of developing a national AI strategy are considerable, and Kelly is clear that “there’s no quick or easy solution.” But there is hope. “There are a lot of really positive things to point to – strong talent, national strategy, global players already based here. That gives me heart.” With AI now reshaping industries across the globe, the State is well-positioned – and the advice from the experts consistent: build on our strengths, simplify and scale support for innovation, invest in skills, foster clusters of excellence, and target the areas where we can lead globally.",Ireland,Irish Times; Dublin,2025-07-25,21,911,2025-07-01,2025.3,Long,43,1,1.0976948408342482,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,5.488474204171241,1,11,12.074643249176729,1,4,4.390779363336993,1,1.198069362928937,Positive
"The advent of AI, and generative AI (GenAI) in particular, has the potential to drive quite dramatic productivity gains and create new industries that do not exist today, but there is also the risk that it will create a new digital divide between the technological haves and have-nots. Many jobs will disappear while new ones will be created. The question for society is how to maximise the benefits of the new technology while ensuring that no one is left behind when it comes to availing of them. “I think the discussion on the impact on jobs is more nuanced than a binary creator-or-destroyer conversation,” says David Lee, chief technology officer with PwC Ireland. “The impact will vary across both job types and industry. In looking to understand the impact I think it is helpful to distinguish between an overall job and the tasks that currently make up that job. Undoubtedly AI will help to automate many of the tasks that make up many existing jobs, but this does not imply that the job is destroyed, but rather that it will need to evolve.” Cait Mulcahy, head of capability and automation with Three Ireland takes an optimistic view. “Technological leaps have historically displaced certain roles while creating new opportunities. For instance, the rise of mobile telecoms and the internet over the past 20-25 years has generated vast new industries and employment opportunities. Thus, it’s reasonable to expect that AI will be a net job creator in the long run. AI will lead to the emergence of new roles – some already foreseeable, like AI specialists, and others yet to be imagined – thus fostering job growth and economic development.” She points to new roles already being created by the technology. “At Three, we now employ robotic process automation specialists, messaging bot developers, and data scientists – positions that didn’t exist a decade ago. Moving forward, AI will undoubtedly generate numerous roles in Stem fields. Additionally, the ethical implications of AI will necessitate roles focused on ethics, training, and education.” PwC’s 2024 AI Jobs Barometer indicates that openings for jobs that require specialist AI skills have grown 3.5 times faster than openings for all jobs since 2016, according to Lee. “New roles such as AI ethicists will be crucial in ensuring that AI systems operate within ethical boundaries and do not perpetuate biases,” he points out. “It is also expected that roles such as AI-generated work auditors will emerge, tasked with scrutinising content for quality, accuracy and ethical compliance. We expect to see roles such as data curators for managing and organising the vast amounts of data required to train sophisticated AI models. It is important to note that judgement and human skills will be pivotal in an AI world and that includes communication skills.” It is not simply a case of accommodating new roles in the workforce. “To prepare for these changes organisations should focus on skills and redesign work,” says Deloitte Ireland consulting partner on human capital, Gary Notley. “The first step to this is to understand what ‘work’ is and the types of jobs in the organisation and examine whether the impact of AI is simple, like automation that machines do best, or complex, requiring machines and humans to work together. After that, deconstruct jobs into tasks and skills. Understand the impact of skills on each job, upskill and reskill the workforce to create new value, and foster a culture of continuous learning. Then, redesign work and workflows to effectively integrate generative AI.” Lee believes there is a societal responsibility to ensure that AI does not create or widen any existing divisions within the labour market. “For people within the existing workforce there is, based on our research, a strong recognition among employers about the need to invest in the upskilling of their existing workforces to take advantage of the AI opportunity. However, the ability to fund this upskilling will vary across organisations. In particular, it is important that the various state agencies tasked with supporting the SME sector are alert and reactive to this need.” Equipped He also points to the need to ensure that people outside of the workforce and those yet to enter it are equipped with the skills required to participate in an AI-enabled labour market. This is not limited to AI-related technical skills but also “the creative thinking, data literacy and continuous learning mindset that we will need to instil via our first- and second-level education sector”. “Change is inevitable, but we can support those at risk of displacement by helping them leverage new opportunities,” says Mulcahy. “At Three, we run initiatives like our Digital Academy and Citizen Developer programmes, which encourage the adoption of new technologies, including AI and automation. These programmes provide our employees with the time and resources to learn new skills and apply them in their roles. By fostering a culture of continuous learning and skill development, we ensure that everyone has the opportunity to thrive in an evolving technological landscape. This approach can serve as a model for other organisations looking to support their workforce through similar transitions.” But organisations may not be paying enough attention to the skills issue, according to the 2024 Deloitte Human Capital Trends report, which showed that 75 per cent of organisations globally intend to accelerate their use of AI over the next five years while only 13 per cent of workers have been offered AI-related skills training in the past year. “Organisations should invest in AI literacy programmes, ensuring that all employees have a foundational understanding of AI’s capabilities and implications,” says Notley. “Organisations should also foster a culture that celebrates exploration and learning, accelerating the adoption and mastery of generative AI. These are safe spaces that allow people to experiment and explore new ways of working, which in turn helps build trust and confidence while they build new skills and innovate.” Lee concludes by highlighting another significant societal issue. “With the increased focus that AI adoption will place on Stem-related skills it is important that this does not widen the existing gender imbalance within the Stem arena. It is important we adopt a proactive approach to prevent this occurring.”",Ireland,Irish Times; Dublin,2024-07-26,26,1027,2024-07-01,2024.3,Long,44,2,1.9474196689386563,1,0,0,0,4,3.8948393378773125,1,0,0,0,4,3.8948393378773125,1,0,0,0,1,0.9737098344693281,1,1,0.9737098344693281,1,4,3.8948393378773125,1,1,0.9737098344693281,1,1.5073794371432496,Positive
"The launch of ChatGPT in November 2022 led to the fastest growing consumer software application uptake in history. A host of competing products were launched in its wake, and it is widely credited with spawning the AI boom that continues to this day. Ken Anderson, CTO of Tashi Gaming, points to the importance of how we approach and use AI. “AI as an assistant is fine, but some people are treating AI as a source of truth, or an educator or a teacher. AI is more like an assistant who has access to a lot of information quickly but isn’t necessarily as smart as a human would be with that same information,” he says. There are a number of concerns about AI and one of them is hallucinations, where AI has been trained on bad data and is convinced that it’s correct and will give you the wrong answer. If people start to depend on those answers, they can very quickly be led astray. Anderson is aware of how plausible AI can sound. “AI can give you a very reasonable answer and can also articulate how it came to that answer in a way that would convince a human. We call those hallucinations. There was one recently that was a forced hallucination that Reddit basically played a prank on open AI, specifically Google’s AI, in their search.” In this example, Reddit seeded a bunch of conversations with bad data that led to “If you searched ‘for how long can you stay in the air when running off a cliff?’”, the answer was “As long as you don’t look down”. The other issue is the feedback loop of AI, as Anderson explains: “People are using AI to build content. Now, at what point is AI consuming its own output to train itself on the next generation of outputs? So, if AI is generating the content that we’re seeing today, and then AI is also listening to all of the social posts and consuming and processing, then at some point it’s just AI talking to itself and humans aren’t even involved. “And that is not necessarily a good thing, because AI needs more human input than AI input, to get better.” Anderson gives the example of mature coders over newer ones. Mature coders can use AI to write code and specs but they would have the experience of having manually written code to spot mistakes and good code over bad. “New coders don’t have that experience to be able to judge good or bad code.” In terms of perspective Martin Duffy, head of generative AI (GenAI) at PwC Ireland believes the pace of AI and GenAI adoption is set to increase. “According to our most recent GenAI Irish Business Leaders survey, there is significant innovation and activity afoot to enable a surge in AI adoption in the years ahead: for example, 86 per cent of survey respondents confirmed that they are either at the early stages of exploration, testing or partial implementation stages of AI adoption, up from 54 per cent just six months ago. Many organisations are realising the opportunities that AI and GenAI can bring and are looking to embed the technologies into their business operations but are also realising it takes time and can be a complex process. “According to the survey, key uses for GenAI in the next 12 months will be cyberdefence, IT development, improving collaboration, sales and marketing, and enhancing supply chains,” says Duffy. PwC’s latest GenAI Business Leaders survey highlights the threats of AI/GenAI: an overwhelming majority (91 per cent) of Irish business leaders believe that GenAI will increase cybersecurity risks in the year ahead; nearly three-quarters (74 per cent) of survey respondents are of the view that GenAI will not enhance their organisation’s ability to build trust with shareholders in the next 12 months, and less than three out of 10 (28 per cent) stated they are confident that the processes and controls over GenAI in their organisation lend themselves to safe and secure outcomes. GenAI also has a role to play in organisations’ cyberdefences. Cyber security software providers are increasingly deploying GenAI-based capabilities in their products to help organisations protect themselves from external threats. Indeed, the use of GenAI as part of cyberdefence was one of the most popular use cases highlighted in our recent GenAI Business Leaders Survey,” says David Lee, CTO of PwC Ireland. Another issue facing AI is the centralised nature of the software. Richard Blythman, co-founder of NapthaAI, sees decentralisation as the key to reducing potential manipulation. He had worked for a number of large AI companies and was concerned about issues such as data surveillance, which led him to co-found a decentralised AI company, NapthaAI, which has recently raised more than $6 million (€5.5 million) in pre-seed funding internationally. “I use ChatGPT all the time and it definitely improves my productivity in terms of writing emails, code and even ideation and brainstorming. But on the down side, every time you use open AI your data becomes part of the system and the AI will eventually be able to do your job better than you can. The importance of decentralising AI lies in keeping your data and intellectual property private. “Decentralised AI has its benefits, but it’s also not without its own risks. For example, some people fear that AI could become very powerful and act against human interests. If such an AI were decentralised, it would be much harder to shut down, similar to how Bitcoin operates. There’s no kill switch for Bitcoin, and a decentralised AI could be equally resilient. It’s crucial to consider both the positive and negative implications and to develop this technology thoughtfully,” says Blythman.",Ireland,Irish Times; Dublin,2024-07-26,32,951,2024-07-01,2024.3,Long,45,5,5.257623554153523,1,1,1.0515247108307044,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1.0515247108307044,1,1,1.0515247108307044,1,2,2.103049421661409,1,1,1.0515247108307044,1,0.1903043749361939,Neutral
"Office life was always going to evolve. The Covid-19 pandemic merely made that evolution more obvious than expected. In the years since, employers in Ireland and across Europe have struggled to keep up with the rapidly evolving world of work. The sheer breadth of influences is causing headaches for employers. From remote work to an ageing workforce through to the increased influence of artificial intelligence (AI), jobs as we know them have changed. Barbara Gerstenberger, head of the working life unit at Eurofound, says that any effective use of technology in the workplace must begin with thoughtful planning. “Job redesign is crucial if workers are supposed to use technology effectively and co-operate with advanced machines and robots,” she says. “As regards dealing with demographic change and managing an ageing workforce, overcoming age stereotypes is key. These stereotypes with respect to new technologies and digital skills, for example, are often reinforced and even internalised by older people themselves, which can result in an underestimation of their own performance or technology skills.” Rather than viewing change as a cost, Gerstenberger says it can be approached more practically. “Job quality has so many dimensions; it is not necessarily difficult or expensive to improve aspects of it. Companies that want to be prepared for the future must turn their attention to the quality of jobs they offer.” The question of skills and career mobility is also front of mind for Irish employers. Martina Fitzgerald, chief executive of Scale Ireland, says there is a growing value to broad, transferable capabilities. “There will always be a need for expertise in key strategic areas, but employers are also now placing a stronger value on transversal skills. That focus will facilitate less rigid career paths and people transitioning to different sectors,” she says. “Problem solving, creative thinking, leadership and communication skills are valuable and valued.” Technology is reshaping the nature of work itself, especially with the growing role of AI, according to Dr John Lonsdale, chief executive of CeADAR. “AI is reshaping work in virtually every sector. In manufacturing, AI-driven robotics and predictive maintenance systems are augmenting human labour on factory floors. In healthcare, AI is helping clinicians with diagnostics and resource allocation,” he says. “Investing in people’s skills is essential for employers seeking to adapt to these shifts. Companies that succeed in the AI era actively upskill and reskill their workforce. Big companies are spending significant sums on AI literacy programmes covering the fundamentals of AI, responsible AI use, and advanced topics like generative AI.” Nessa McEniff, centre director at the Learnovate Centre, says strong leadership will play a vital role in navigating this shift. “As the value of skills continues to increase in our knowledge-based economy, every leader, starting with the chief executive, must champion a culture of learning. When leadership values learning, employees are far more likely to invest their time and energy into developing their capacity to continuously learn,” McEniff says traditional training models no longer suffice. “In today’s demanding work environment, organisations must also ensure learning is accessible anytime, anywhere, personalised to individual needs and embedded into the daily flow of work.” The flexibility of remote work remains one of the most tangible changes since the pandemic. Its success is reliant on structure and active communication from managers, says Dónal Kearney, community manager at Grow Remote. “Being clear about what remote means is important. Does that mean fully at home or are you expected to be in the office sometimes? If so, when are you expected to be in the office? The best companies tell employees why they are expected in the office.” He says the best outcomes are achieved when managers focus on results rather than employee visibility. “The managers who are most flexible and who listen to employees but keep the focus on output and productivity rather than presenteeism are most likely to succeed,” he adds. The greatest risk to employers, says Kearney, is in failing to adequately support managers who are trying to navigate the changing world of work. “The risks, when you don’t adapt, are centred around poor management. Managers need to be supported. “There’s a gap right now where there are huge benefits available for staff at large, but managers are struggling to adapt.” l",Ireland,Irish Times; Dublin,2025-06-26,13,707,2025-06-01,2025.2,Medium,46,2,2.828854314002829,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2.828854314002829,1,1,1.4144271570014144,1,1.1849801899329437,Positive
"DANIELLE BARRON Ireland’s attractiveness as a destination for international investment is set to be examined as we endeavour to become a global hub for AI development. An overwhelming majority (94 per cent) of respondents to the AmCham FDI Insights Survey said the Republic’s position as a regulatory hub within the EU is important to the State becoming a leader in digital and artificial intelligence (AI), with 60 per cent saying it is extremely or very important to their company. In addition, 96 per cent of respondents said EU investment in AI is important to ensure Ireland remains a destination of choice for investment. Many are already walking the walk in this regard. Darryl Williams is chief executive and founder of Partsol, a US AI company that made the decision to relocate its global headquarters from the United States to Ireland in recent months. “Partsol’s move to Ireland didn’t happen despite the global uncertainty or because of who sits in the White House,” he wrote in The Irish Times earlier this year. “It happened because of Ireland’s stability, its deep alignment with European regulatory clarity and its position as a bridge between the best of American innovation and the rigour of European accountability and regulation.” Rhonda Doyle is country president of Schneider Electric Ireland, which is working to integrate AI into systems that optimise energy use, automate processes and reduce consumption. It is her view that the Irish ecosystem is not only supportive of AI development but is “actively driving” it. “Ireland has become a highly attractive location for AI companies, underpinned by a booming tech sector, a pro-business environment and a deep culture of innovation,” says Doyle. The State is particularly well-positioned to lead in AI, with adoption projected to contribute more than €250 billion to the economy by 2035. Start-ups are playing a vital role, Doyle says: “Ireland boasts a strong R&D-driven start-up culture, with 63 per cent of start-ups already adopting AI, more than double the European average.” In parallel, the presence of big global tech players continues to bolster the ecosystem, she adds. “These companies are not only advancing AI innovation but also investing in workforce development through upskilling programmes and strategic partnerships. Together, this dynamic mix of innovation, investment and collaboration creates a fertile environment for AI to thrive in Ireland, making it one of the most attractive locations for AI development in Europe and beyond.” Noesis is a global IT services company. Its business development manager, Conor Bartley, says the firm is focused on expanding its global AI capabilities to help clients drive innovation and unlock business value through advanced technologies. “We see Ireland as a strategic part of our global business, with a strong talent pool that has the potential to lead the way for AI in Europe,” says Bartley. “Our centre of excellence in Dundalk plays a key role, leveraging close ties to top universities and strategic time zone alignment with the UK and US. Noesis recognises this opportunity and is committed to investing in Ireland to turn that potential into impact.” João Martins, data analytics and AI senior manager with the organisation, says this is due to several factors: “It’s the maturity of the market, the high concentration of companies operating in Ireland and the specific challenges these organisations face – which, in many cases, mirror those we have already addressed in other geographies where Noesis operates.” Ireland is also a market that demonstrates both financial capacity and strategic vision, with a medium- to long-term perspective, Martins adds. “Many organisations have moved beyond early-stage proof of concept or value validation,” he says. “They are now engaging in full-scale production projects where AI plays a central role. Ireland also attracts highly skilled professionals and we have observed a parallel maturity in mindset and openness to innovation.” As a result, Martins says the Irish ecosystem is primed to be a global hub for AI development. “When we see the multinational companies that have already chosen Ireland as a key European hub, alongside the strength of the local talent pool – we see a powerful combination of factors enabling various sectors to grow in a robust, consistent and forward-looking manner.” Yet Doyle points out that the State’s ageing energy infrastructure risks deterring investment, as the rapid growth of AI data centres, which require a significant amount of energy, has raised concerns around grid resilience and energy supply. “To maintain Ireland’s appeal as a tech hub, we need more investment in grid modernisation and incentives for businesses to adopt ‘prosumer’ solutions like microgrids that offer a reliable, localised energy supply, helping businesses withstand disruption while easing pressure on the grid,” says Doyle. He says continued collaboration between Government and industry is needed to fast-track energy-efficient technologies. Beyond infrastructure, talent development is equally crucial. “While Ireland has a strong workforce, there is a growing Stem skills gap that threatens progress in key sectors like AI and clean tech,” says Doyle. “Government initiatives are necessary to support education and training on a country level, but businesses must also invest in developing talent and upskilling employees to meet evolving demands.” Schneider Electric is taking matters into its own hands and last year it launched a UK and Ireland training programme focused on automation, AI, regulation and energy management.",Ireland,Irish Times; Dublin,2025-06-27,22,879,2025-06-01,2025.2,Long,47,2,2.2753128555176336,1,0,0,0,1,1.1376564277588168,1,0,0,0,1,1.1376564277588168,1,0,0,0,0,0,0,4,4.550625711035267,1,16,18.20250284414107,1,6,6.825938566552901,1,2.0226838808171306,Positive
"Artificial Intelligence, or AI, seems to be everywhere at the moment. You may have already heard of apps such as ChatGPT, a text-based chatbot that allows you to write essays, or AI image generators such as Midjourney, that create realistic images based on the instructions you give it. But did you know that AI is also now being used in Irish hospitals and supporting medical professionals in providing patient care? Much like the introduction of any revolutionary technology, the increased use of AI goes hand in hand with valid fears and concerns around its use – from an ethical, legal and moral standpoint. All aspects of life stand to be impacted by AI and, when it comes to our health and personal healthcare data, we can’t be too careful. Dr Conor Judge is a consultant nephrologist at Saolta University Health Care Group (which comprises six hospitals across seven locations in the west and northwest of Ireland) and senior lecturer of applied clinical data analytics with the University of Galway. He recently spoke at the Irish Medical Organisation’s AGM in Kerry on the potential impact of AI on healthcare delivery in Ireland. Dr Judge believes that AI interventions in healthcare must be treated in the same way as medications. This means they must be tested in large phase-three clinical trials. “So this is how we test aspirin, how we test blood pressure medications, and anything short of that for artificial intelligence is going to be a costly mistake,” he told The Irish Times. Last year, Ireland’s first Institute for Clinical Trials was established by the University of Galway with a focus on benefit and impact for patients. According to the institute, it “will transform lives by ensuring patients get access to the latest medicines and treatments in a timely way”. Speaking at the launch of the institute, Prof Martin O’Donnell, dean of the college of medicine, nursing and health sciences at University of Galway, said: “This institute will consolidate our areas of considerable strength in clinical trials, ensure the rapid translation of medical discoveries to clinical evaluation and ultimately improve the lives of patients and health of our population, leveraging the academic strengths of our university and its healthcare partner Saolta University Healthcare Group.” Dr Judge said the new institute would look at innovations in randomised clinical trials which would include testing new medicinal products, medical devices and AI interventions. One of the stated goals was to “support emerging domains, such as Artificial Intelligence in healthcare, by creating a cluster of academic and industry investigators aligned to evaluating digital health interventions”. Dr Judge has recently secured funding from the Health Research Board to explore the potential of AI to improve the management and treatment of hypertension or high blood pressure. Often known as the silent killer, hypertension affects more than 1.4 billion people worldwide and is a major risk factor for heart attack and stroke. Treatment decisions Currently, some patients with high blood pressure may need to make frequent visits to their GP to ensure optimal control of their condition which is time-consuming for both the patient and the doctor. This is where Artificial Intelligence Clinical Decision Support Systems (AICDSS) for hypertension could help. The research project will analyse two extensive clinical trials on blood pressure treatment to train a computer program to make treatment decisions similar to doctors who specialise in hypertension. Safety features will then be integrated into the AI program to ensure reliable recommendations, especially in unfamiliar medical scenarios. The AI tool would, therefore, aim to help GPs provide the best treatment for a patient’s blood pressure to help bring it under control quicker. Interestingly, the project will also carry out comprehensive surveys with clinicians and patients to gauge their perceptions of AI-driven treatment. The final step involves a thorough evaluation of the AI program’s efficacy in recommending blood pressure treatments in a real-world setting. “The overarching goal is to personalise hypertension management, thereby bridging the existing care gap and significantly reducing the global burden of hypertension-related complications. The project’s findings could potentially set a precedent for employing AI in managing other critical health conditions, marking a significant stride towards integrating AI in routine clinical practice,” Dr Judge said. There are several AI-assisted interventions already in use in Irish hospitals, mainly in the area of medical imaging, and Dr Judge paid credit to the work of the HSE National Intelligence Group for its work in this area. One of these currently in place is an assisted decision-making tool used in the management of stroke. When a patient has had a stroke, one treatment option that may be suitable for them is called a thrombectomy. Thrombectomy is a life-saving treatment where the clot causing the stroke is mechanically removed from the brain. To decide if a patient is eligible for thrombectomy, a radiologist examines CT scans of the patient’s brain to calculate a score called the ASPECTS score. Dr Judge explained that this was a time-consuming task and this was where an AI tool developed by a company called Brainomix comes in. The Brainomix 360 stroke platform uses AI algorithms to support doctors by providing real-time interpretation of brain scans. The AI tool calculates the ASPECTS score for the radiologist which facilitates the decision on appropriate treatment more quickly, making the process more efficient. This is particularly important where every minute matters and time is crucial for improved patient outcomes. Dr Judge said that, ultimately, it is the physician who decides on the best course of treatment for patients and AI tools can help augment the decision-making process. The practice of medicine has evolved over centuries, shaped by new advancements from the introduction of vaccines and anaesthesia and transplant surgery to modern medicines. However, the one thing that has remained constant is the doctor-patient relationship, the humanity or art of medicine. Changing the relationship According to Dr Judge, the doctor-patient relationship is not under threat from AI, but rather could potentially benefit from it. “We spend a lot of our time doing administration work, where we’re trying to collect pieces of information from multiple different data sources – they’re often in siloed systems, you have to have a username and password to log into all of them and then we have to make a treatment decision about that. “And then there’s very little time to talk to the patient and explain it to the patient. So if we had systems that enhance the ability to put information in one place it could really change the relationship for now we have more time to speak to the patient, to explain to the patient, to allow them to ask and answer questions. Framing it in that way there is immense potential to improve the doctor-patient relationship.” Dr Judge believes that AI has the potential to transform healthcare, but only if the correct safety measures are put in place. He said that patients should demand that AI interventions be rigorously tested to the same level as medicines. “We don’t ask patients to take medications that haven’t been tested in randomised clinical trials . . . So patients need to demand that that’s the thing that happens for AI interventions. “I’m very positive about it . . . but we just have to do it right. Because if we don’t do it right, well, then it could transform it . . . for the worse. It has to be done with the correct guardrails in place . . . It goes back to the first ‘do no harm’ and we need to take that approach as well. But I’m very positive about the transformative nature of it.”",Ireland,Irish Times; Dublin,2024-05-28,33,1274,2024-05-01,2024.2,Long,48,5,3.9246467817896384,1,0,0,0,2,1.5698587127158556,1,0,0,0,1,0.7849293563579278,1,0,0,0,0,0,0,2,1.5698587127158556,1,3,2.3547880690737832,1,0,0,0,1.1983495468924585,Positive
"‘Data is the new oil,” said Barry Scannell, an AI law expert with William Fry, “and AI companies are going to be the new refineries.” He was addressing an audience of tech industry professionals in Trinity College at a summit on artificial intelligence. “What we’re doing now will have ripples for the future,” Scannell continued, and as he spoke on a panel shared with OpenAI and Logitech, attendees diligently took notes. Ireland’s 300-odd indigenous AI companies, more than half of them based in and around Dublin, and their multinational competitors have seen an explosion in interest in AI over the past 18 months after products such as ChatGPT opened the world’s eyes to the potential of the technology. Tech companies big and small are scrambling to develop the best software to get an edge over rivals, and chip manufacturing companies, such as Nvidia, have seen huge market gains as they struggle to keep up with demand for microchips capable of running AI. “Generative AI will drive a paradigm shift in our interaction with technology,” Google’s Sebastian Haire told the Dublin summit, adding that the world was entering a fourth industrial revolution spearheaded by AI. He joked that even his presentation was out of date in the time between designing and delivering it. If industry representatives are to be believed, virtually every company is either integrating some form of AI into its systems or is beginning an “exploratory phase” to assess how it can help their staff improve productivity. By 2025, worldwide spend on AI will reach $204 billion (€188 billion), according to Haire. That’s next year. It is expected that trillions will be pumped into the technology in the years to come. At the Trinity College conference, AI experts mingled, trying each other’s branded cupcakes at industry stands, exchanging niceties and whatnot. But below the surface there’s a race going on, and it’s one in which nobody wants to fall behind. For companies that miss the boat, “they’re probably going to miss out on some significant productivity gains”, said James Croke, business development officer at Version 1. “To get the most from AI, companies need to get their data right first. They need to migrate to the cloud. It’s a challenge that they need to play catch-up on but when you look at the potential impact of AI for SMEs in particular, it could allow them to leapfrog rivals on productivity. This could be a once-in-a-generation chance for SMEs.” Markham Nolan, a former journalist and co-founder of Noan, which helps companies utilise artificial intelligence, said AI was “a time machine for small businesses”. “Our users save five to 10 hours a week by using AI,” he said. “If you get the prompts right, whatever AI creates will be 80-90 per cent to where it needs to be: you just have to do the polishing. ‘Hurdle’ “The AI has learned [our clients’] brand. It has learned to speak for their brand. So rather than having to sit down and write an email from scratch, they just say, ‘I need this email to be about this, to this company’, and five seconds later they’ll have a fully fleshed-out email in their voice. “So many businesses hit a hurdle, a point they can’t get over because they don’t have the revenue. AI is a bridging technology that allows people to add capacity without cost, and to grow to become the companies they have the potential to be,” Nolan said. Speaking to those attending the conference, you get the sense that AI no longer rests its promise upon pie-in-the-sky transformations in the future, but offers deliverable ones in the short term. Still, there are some innovations that remain difficult to picture in Ireland any time soon. Matthew Nicholson, a researcher at Trinity’s Adapt Centre, was showcasing Swedish company Furhat’s social robot. Looking a bit like a bust from Will Smith’s I, Robot, with an internal projection displaying various facial expressions, it might act like a robot concierge in a hotel room, Nicholson suggested, though for now it seems a little too bizarre to wake up to. Other AI applications were more obvious in their benefits for humanity. Unicef’s AI lead, Dr Irina Mirkina, suggested that in the future AI will help predict cyclical natural disasters, disease outbreaks and how much aid is needed for emergencies at a pace far faster than any human can calculate. Chris Hokamp, a scientist with Quantexa, sees a future in which there is likely to be “another species of AIs”. AI should remain a tool for humans for as long as possible, he said, adding: “We don’t want to give it emotions.” There’s a curious casualness about sweeping predictions such as Hokamp’s. It’s an almost unsurprising prediction in these circles nowadays. He told the conference that humans must ensure that when AI reaches a superhuman level of intelligence, it must act ethically and adhere to regulations. He takes comfort in the knowledge that bad actors are unlikely to unleash a malevolent superintelligence on the world that they themselves would be unable to control. Regulation was a key theme of the summit and there was much talk of the EU’s proposed AI Act, which seeks for the first time to put in place a legal framework within which AI can operate in the bloc. Onur Korucu, managing partner at GovernID, a privacy firm, said AI must be regulated in the EU – not to stifle innovation but “to put a frame around the innovation” and “democratise” the use of AI. Mark Kelly, the founder of AI Ireland, argued that a framework from which to develop AI would encourage companies to green-light new AI projects. His advice for Irish AI start-ups was not to “go competing with the likes of OpenAI with video tools ... but if [a smaller Irish company] can go down and solve a niche industry-specific issue [you will succeed].” He spoke of one company that documented 20,000 client questions over the last 20 years, and created a language model around it to save time in answering queries. It led to a 25 per cent increase in clients within one year. Skillnet Ireland’s Tracey Donnery said the number of women in AI is small but is increasing, and she appeared optimistic about AI’s ability to augment and change jobs rather than solely replacing them. “Hopefully it won’t be as dramatic as described by the naysayers,” she said. Dark side There is, of course, also a dark side to AI. Dan Purcell, founder of Ceartas, an AI-powered company that takes down deepfakes from the internet, told the conference that “sextortion” is a growing issue, with young men increasingly accessing technology that uses AI to “de-clothe” women. On deepfakes, UCD’s Dr Brendan Spillane believes the technology poses a “serious risk to society” including “the integrity of elections”. He said states are using deepfakes to sow social distrust and unrest and that it is becoming more common for states to outsource the service to private companies. On the same subject, Rigr AI founder Edward Dixon helps law enforcement agencies around the world, including An Garda Síochána, to find the likely location of sensitive media. When police receive media depicting crimes against children, for example, they might receive hundreds of thousands of files. “Crimes like terrorism are noisy, public and generally have a lot of bystander imagery and accounts,” he told The Irish Times. “The kind of crimes we focus on happen quietly.” The company’s tools suggest plausible locations based on the photo or video’s environment, languages spoken, named mentioned – ultimately saving hundreds of hours for investigators and potential psychological damage from examining sensitive media. It is just one more example of how the long-predicted AI revolution now appears to be here. Whether you are worried, enthused or just baffled, it is hard not to feel as though the AI tide is coming in relentlessly.",Ireland,Irish Times; Dublin,2024-02-29,40,1314,2024-02-01,2024.1,Long,49,1,0.76103500761035,1,1,0.76103500761035,1,0,0,0,0,0,0,0,0,0,1,0.76103500761035,1,0,0,0,6,4.5662100456621,1,3,2.28310502283105,1,4,3.0441400304414,1,-0.6381116576751608,Negative
"Demis Hassabis and Mustafa Suleyman, who both grew up in London, feared a corporate rush to build artificial intelligence. Now they’re driving that competition at Google and Microsoft. Mustafa Suleyman grew up in subsidized housing in one of London’s roughest areas. His father, a Syrian immigrant, drove a taxi. His mother was a nurse with the National Health Service. When the prestigious Queen Elizabeth’s School accepted him at the age of 11, the family moved into a safer, leafier neighborhood a few miles north. There, he met 20-year-old Demis Hassabis, after becoming friends with his younger brother. Demis was a chess prodigy and video game designer whose parents — one a Greek Cypriot, the other a Singaporean — ran a London toy store. Today, they are two of the most powerful executives in the tech industry’s race to build artificial intelligence. Dr. Hassabis, 47, is the chief executive of Google DeepMind, the tech giant’s central research lab for artificial intelligence. Mr. Suleyman, 39, was recently named chief executive of Microsoft AI, charged with overseeing the company’s push into A.I. consumer products. Their path from London to the executive suites of Big Tech is one of the most unusual — and personal — stories in an industry full of colorful personalities and cutting rivalries. In 2010, they were two of the three founders of DeepMind, a seminal A.I. research lab that was supposed to prevent the very thing they are now deeply involved in: an escalating race by profit-driven companies to build and deploy A.I. Their paths diverged after a clash at DeepMind, which Google acquired for $650 million in 2014. When the A.I. race kicked off in late 2022 with the arrival of the ChatGPT online chatbot, Google put Dr. Hassabis in charge of its A.I. research. Mr. Suleyman took a rockier route — founding another A.I. start-up, Inflection AI, that struggled to gain traction before Microsoft unexpectedly hired him and most of his employees. “We’ve always seen the world differently, but we’ve been aligned in believing that this is going to be the next great transition in technology,” Mr. Suleyman said of his old family friend in an interview. “It is always a friendly and respectful rivalry.” Microsoft’s push into artificial intelligence with its partner, OpenAI, the maker of ChatGPT, has rattled Google. The two companies are now fighting to control what many experts see as the next dominant computing platform, a battlefield as important as the web browser and the smartphone before it. Dr. Hassabis is driving the creation of Google’s A.I. technology, while Mr. Suleyman works to put Microsoft’s A.I. in the hands of everyday people. Though Mr. Suleyman sees their relationship as a cordial rivalry, Dr. Hassabis believes any talk of rivalry is overblown. He does not see Mr. Suleyman as a major threat, because competition in A.I. was already so high, with so many formidable companies. “I don’t think there is much to say,” he said in an interview with The New York Times. “Most of what he has learned about A.I. comes from working with me over all these years.” When the two first met, Mr. Suleyman was in grade school and Dr. Hassabis had started a computer science degree at the University of Cambridge. While Dr. Hassabis was competing in the annual Varsity Chess Match between Cambridge and Oxford, his younger brother, George, and Mr. Suleyman were teaching chess to local children at a Wednesday night math school run by the Hassabis family in North London. Mr. Suleyman later studied philosophy and theology at Oxford, before dropping out to help start a mental health help line for Muslim teenagers and working as a human rights officer for the mayor of London. Dr. Hassabis founded a video game company, before returning to academia for a doctorate in neuroscience. But they shared an interest in high-stakes poker. “We are both quite good,” Mr. Suleyman often says. In 2010, after sitting down for a game at the Victoria Casino in London, they discussed how they could change the world. Dr. Hassabis dreamed of building technologies of the future. Mr. Suleyman aimed to change society right away, improving health care and closing the gap between the haves and the have-nots. “Demis had the pure-science moonshot aspiration,” said Reid Hoffman, a Silicon Valley venture capitalist and Microsoft board member who helped found both OpenAI and Mr. Suleyman’s Inflection AI. “He convinced Mustafa this science could be a high-order bid for making things better for society — for humanity.” Dr. Hassabis was finishing postdoctoral work at the Gatsby Computational Neuroscience Unit, a University College London lab that combined neuroscience (the study of the brain) with A.I. (the study of brainlike machines). Seeing Mr. Suleyman as a hard-charging personality who could help build a start-up, he invited him to the Gatsby for meetings with a philosophically minded A.I. researcher, Shane Legg. In the afternoons, they would huddle at a nearby Italian restaurant, cultivating a belief that A.I could change the world. By the end of 2010, after engineering a meeting with Peter Thiel, the Silicon Valley venture capitalist, the three of them had secured funding for DeepMind. Its stated mission was to build artificial general intelligence, or A.G.I., a machine that could do anything the human brain can do. They were also determined to build the technology free from the economic pressures that typically drive big business. Those pressures, they believed, could push A.I. in dangerous directions, upend the job market or even destroy humanity. As Dr. Hassabis and Dr. Legg (who is still with DeepMind) pursued intelligent machines, it was Mr. Suleyman’s job to build products and find revenue. He and his team explored an A.I. video game, an A.I. fashion app and even whether A.I. could help a company, Hampton Creek, making vegan mayonnaise, a former colleague said. Dr. Hassabis told employees that DeepMind would remain independent. But as its research progressed and tech giants like Facebook swooped in with millions of dollars to poach its researchers, its founders felt they had no choice but to sell themselves to Google. DeepMind continued to operate as a largely independent research lab, but it was funded by and answered to Google. For years, DeepMind employees had whispered about Mr. Suleyman’s aggressive management style. That came to a head in early 2019 when several employees filed formal complaints accusing Mr. Suleyman of verbally harassing and bullying them, six people said. Former employees said he had yelled at them in the open office and berated them for being bad at their jobs in long text-message threads. Mr. Suleyman later said of his time at DeepMind: “I really screwed up. I remain very sorry about the impact that that caused people and the hurt that people felt there.” He was placed on leave in August 2019, with DeepMind saying he needed a break after 10 hectic years. Multiple people told Dr. Hassabis that the punishment should go further, two people with knowledge of the conversations said. Months later, Mr. Suleyman moved into a job at Google’s California headquarters. Privately, Mr. Suleyman felt that Dr. Hassabis had stabbed him in the back, a person with knowledge of their relationship said. Mr. Suleyman’s new Google position had a big title — vice president of A.I. product management and A.I. policy — but he was not allowed to manage employees, two people said. He disliked the role, a friend said, and soon left to start Inflection AI. When OpenAI released ChatGPT less than a year later, sparking an industrywide race to build similar technologies, Google responded forcefully. Last April, the company merged its homegrown A.I. lab with DeepMind and put Dr. Hassabis in charge. (The New York Times sued OpenAI and Microsoft in December for copyright infringement of news content related to A.I. systems.) For a time, Mr. Suleyman remained an independent voice warning against the tech giant and calling for government regulation of A.I. An opinion piece that he wrote with Ian Bremmer, a noted political scientist, argued that big tech companies were becoming as powerful as nation states. But after raising more than $1.5 billion to build an A.I. chatbot while pulling in practically no revenue, his company was struggling. In March, Inflection AI effectively vanished into Microsoft, with Mr. Suleyman put in charge of a new Microsoft business that will work to inject A.I. technologies across the company’s consumer services. Mr. Suleyman, who splits his time between Silicon Valley and London, officially became a rival to Google DeepMind, opening a new Microsoft office in London to compete for the same talent. Dr. Hassabis expressed frustration to his staff that Mr. Suleyman was positioning himself as a prominent A.I. visionary, a colleague said. They still text each other on occasion. They might meet for dinner if they are in the same city. But Dr. Hassabis said he does not worry much about what Mr. Suleyman or any other rival is up to. “I don’t really look to others for what we should be doing,” Dr. Hassabis said. By Cade Metz and Nico Grant",USA,"The New York Times, International edition; New York",2024-05-02,35,1509,2024-05-01,2024.2,Very Long,50,2,1.3253810470510272,1,0,0,0,0,0,0,0,0,0,1,0.6626905235255136,1,0,0,0,0,0,0,3,1.9880715705765406,1,0,0,0,3,1.9880715705765406,1,-0.7005921922649997,Negative
"Enlarge this image. SAN FRANCISCO — Elon Musk celebrated his 44th birthday in July 2015 at a three-day party thrown by his wife at a California wine country resort dotted with cabins. It was family and friends only, with children racing around the upscale property in Napa Valley. This was years before Twitter became X, and Tesla had a profitable year. Musk and his wife, Talulah Riley, were a year from throwing in the towel on their second marriage. Larry Page, a party guest, was still CEO of Google. And artificial intelligence had pierced the public consciousness only a few years before, when it was used to identify cats on YouTube — with 16% accuracy. AI was the big topic of conversation when Musk and Page sat down near a fire pit beside a swimming pool after dinner the first night. The two billionaires had been friends for more than a decade. But the tone that clear night soon turned contentious as the two debated whether AI would ultimately elevate humanity or destroy it. As the discussion stretched into the chilly hours, it grew intense, and some of the more than 30 partyers gathered closer to listen. Page, hampered for more than a decade by an unusual ailment in his vocal cords, described his vision of a digital utopia in a whisper. Humans would eventually merge with artificially intelligent machines, he said. One day there would be many kinds of intelligence competing for resources, and the best would win. If that happens, Musk said, we’re doomed. The machines will destroy humanity. With a rasp of frustration, Page insisted his utopia should be pursued. Finally, he called Musk a “specieist,” a person who favors humans over the digital life-forms of the future. That insult, Musk said later, was “the last straw.” Eight years later, the argument between the two men seems prescient. The question of whether AI will elevate the world or destroy it — or at least inflict grave damage — has framed an ongoing debate among Silicon Valley founders, chatbot users, academics, legislators and regulators about whether the technology should be controlled or set free. That debate has pitted some of the world’s richest men against one another: Musk, Page, Mark Zuckerberg of Meta, tech investor Peter Thiel, Satya Nadella of Microsoft and Sam Altman of OpenAI. All have fought for a piece of the business and the power to shape it. At the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth. Musk and Page stopped speaking soon after the party that summer. A few weeks later, Musk dined with Altman, who was then running a tech incubator, and several researchers in a private room at the Rosewood hotel in Menlo Park, California. That dinner led to the creation of a startup called OpenAI later in the year. Backed by hundreds of millions of dollars from Musk and other funders, the lab promised to protect the world from Page’s vision. Thanks to its ChatGPT chatbot, OpenAI has fundamentally changed the technology industry and has introduced the world to the risks and potential of artificial intelligence. OpenAI is valued at more than $80 billion, according to two people familiar with the company’s latest funding round, although Musk and Altman’s partnership didn’t make it. The two have since stopped speaking. “There is disagreement, mistrust, egos,” Altman said. “The closer people are to being pointed in the same direction, the more contentious the disagreements are. You see this in sects and religious orders. There are bitter fights between the closest people.” Last month, that infighting came to OpenAI’s boardroom. Rebel board members tried to force out Altman because, they believed, they could no longer trust him to build AI that would benefit humanity. Over five chaotic days, OpenAI looked as if it were going to fall apart, until the board — pressured by giant investors and employees who threatened to follow Altman out the door — backed down. The drama inside OpenAI gave the world its first glimpse of the bitter feuds among those who will determine the future of AI. But years before OpenAI’s near meltdown, there was a little-publicized but ferocious competition in Silicon Valley for control of the technology that is now quickly reshaping the world. The New York Times spoke with more than 80 executives, scientists and entrepreneurs, including two people who attended Musk’s birthday party in 2015, to tell that story of ambition, fear and money. The Birth of DeepMind Five years before the Napa Valley party and two before the cat breakthrough on YouTube, Demis Hassabis, a 34-year-old neuroscientist, walked into a cocktail party at Thiel’s San Francisco town house and realized he had hit pay dirt. There in Thiel’s living room was a chessboard. Hassabis had once been the second-best player in the world in the under-14 category. “I was preparing for that meeting for a year,” Hassabis said. “I thought that would be my unique hook in: I knew that he loved chess.” In 2010, Hassabis and two colleagues, who all lived in Britain, were looking for money to start building “artificial general intelligence,” or AGI, a machine that could do anything the brain could do. At the time, few people were interested in AI. Still, some scientists and thinkers had become fixated on the downsides of AI. Many, including the three young men from Britain, had a connection to Eliezer Yudkowsky, an internet philosopher and self-taught AI researcher. Yudkowsky was a leader in a community of people who called themselves Rationalists or, in later years, effective altruists. Thiel had become enormously wealthy through an early investment in Facebook and through his work with Musk in the early days of PayPal. He had developed a fascination with the singularity, a trope of science fiction that describes the moment when intelligent technology can no longer be controlled by humanity. With funding from Thiel, Yudkowsky had expanded his AI lab and created an annual conference on the singularity. Years before, one of Hassabis’ two colleagues had met Yudkowsky, and he snagged them speaking spots at the conference, ensuring they’d be invited to Thiel’s party. Yudkowsky introduced Hassabis to Thiel. Charmed, Thiel invited the group back the next day. The three made their pitch, and soon Thiel and his venture capital firm agreed to put 1.4 million British pounds (roughly $2.25 million) into their startup. He was their first major investor. They named their company DeepMind, a nod to “deep learning,” a way for AI systems to learn skills by analyzing large amounts of data; to neuroscience; and to the Deep Thought supercomputer from the sci-fi novel “The Hitchhiker’s Guide to the Galaxy.” By the fall of 2010, they were building their dream machine. They wholeheartedly believed that because they understood the risks, they were uniquely positioned to protect the world. “I don’t see this as a contradictory position,” said Mustafa Suleyman, one of the three DeepMind founders. “There are huge benefits to come from these technologies. The goal is not to eliminate them or pause their development. The goal is to mitigate the downsides.” Having won over Thiel, Hassabis worked his way into Musk’s orbit. About two years later, they met at a conference organized by Thiel’s investment fund, which had also put money into Musk’s company SpaceX. Hassabis secured a tour of SpaceX headquarters. Afterward, the two men lunched in the cafeteria and talked. Musk explained that his plan was to colonize Mars to escape overpopulation and other dangers on Earth. Hassabis replied that the plan would work — so long as superintelligent machines didn’t follow and destroy humanity on Mars, too. Musk was speechless. He hadn’t thought about that particular danger. Musk soon invested in DeepMind alongside Thiel. Flush with cash, DeepMind hired researchers who specialized in neural networks, complex algorithms created in the image of the human brain. A neural network is essentially a giant mathematical system that spends days, weeks or even months identifying patterns in large amounts of digital data. First developed in the 1950s, these systems could learn to handle tasks on their own. After analyzing names and addresses scribbled on hundreds of envelopes, for instance, they could read handwritten text. DeepMind took the concept further. It built a system that could learn to play classic Atari games to illustrate what was possible. This got the attention of another Silicon Valley powerhouse, Google, and specifically Page. The Talent Auction In the fall of 2012, Geoffrey Hinton, a 64-year-old professor at the University of Toronto, and two graduate students published a research paper that showed the world what AI could do. They trained a neural network to recognize common objects such as flowers, dogs and cars. Scientists were surprised by the accuracy of the technology built by Hinton and his students. One who took particular notice was Yu Kai, an AI researcher who had met Hinton at a research conference and had recently started working for Baidu, a giant Chinese internet company. Baidu offered Hinton and his students $12 million to join the company in Beijing, according to three people familiar with the offer. Hinton turned Baidu down, but the money got his attention. “We did not know how much we were worth,” Hinton said. He consulted lawyers and experts on acquisitions and came up with a plan: “We would organize an auction, and we would sell ourselves.” The auction would take place during an annual AI conference at the Harrah’s hotel and casino on Lake Tahoe. Google made an offer. So did Microsoft. DeepMind quickly bowed out. The industry giants pushed the bids to $20 million and then $25 million. As the price passed $30 million, Microsoft quit, but it rejoined the bidding at $37 million. Then Microsoft dropped out a second time. Only Baidu and Google were left, and they pushed the bidding to $42 million, $43 million. Finally, at $44 million, Hinton and his students stopped the auction. The bids were still climbing, but they wanted to work for Google. And the money was staggering. It was an unmistakable sign that deep-pocketed companies were determined to buy the most talented A.I. researchers — which was not lost on Hassabis at DeepMind. He had always told his employees that DeepMind would remain an independent company. That was, he believed, the best way to ensure its technology didn’t turn into something dangerous. But as Big Tech entered the talent race, he decided he had no choice: It was time to sell. By the end of 2012, Google and Facebook were angling to acquire the London lab, according to three people familiar with the matter. Hassabis and his co-founders insisted on two conditions: No DeepMind technology could be used for military purposes, and its AGI technology must be overseen by an independent board of technologists and ethicists. Google offered $650 million. Zuckerberg of Facebook offered a bigger payout to DeepMind’s founders but would not agree to the conditions. DeepMind sold to Google. The Lost Ethics Board When Musk invested in DeepMind, he broke his own informal rule — that he would not invest in any company he didn’t run himself. The downsides of his decision were already apparent when, only a month or so after his birthday spat with Page, he again found himself face to face with his former friend and fellow billionaire. The occasion was the first meeting of DeepMind’s ethics board, on Aug. 14, 2015. The board had been set up at the insistence of the startup’s founders to ensure that their technology did no harm after the sale. The members convened in a conference room just outside Musk’s office at SpaceX, according to three people familiar with the meeting. But that’s where Musk’s control ended. When Google bought DeepMind, it bought the whole thing. Musk was out. Three Google executives now firmly in control of DeepMind were there: Page; Sergey Brin, a Google co-founder and Tesla investor; and Eric Schmidt, Google’s chair. Among the other attendees were Reid Hoffman, another PayPal founder; and Toby Ord, an Australian philosopher studying “existential risk.” The DeepMind founders reported that they were pushing ahead with their work but that they were aware the technology carried serious risks. Eight months later, DeepMind had a breakthrough that stunned the AI community and the world. A DeepMind machine called AlphaGo beat one of the world’s best players at the ancient game of Go. The game, streamed over the internet, was watched by 200 million people across the globe. Most researchers had assumed that AI needed another 10 years to muster the ingenuity to do that. The Breakup Convinced that Page’s optimistic view of AI was dead wrong, and angry at his loss of DeepMind, Musk built his own lab. OpenAI was founded in late 2015. In late 2017, he hatched a plan to wrest control of the lab from Altman and the other founders and transform it into a commercial operation that would join forces with Tesla and rely on supercomputers the car company was developing, according to four people familiar with the matter. When Altman and others pushed back, Musk quit and said he would focus on his own AI work at Tesla. In February 2018, he announced his departure to OpenAI’s staff on the top floor of the startup’s offices in a converted truck factory, three people who attended the meeting said. OpenAI suddenly needed new financing in a hurry. Altman flew to Sun Valley for a conference and ran into Satya Nadella, Microsoft’s CEO. A tie-up seemed natural. The deal closed in 2019. Altman and OpenAI had formed a for-profit company under the original nonprofit, they had $1 billion in fresh capital, and Microsoft had a new way to build AI into its vast cloud computing service. The Reveal After OpenAI received another $2 billion from Microsoft, Altman and another senior executive, Greg Brockman, visited Bill Gates at his sprawling mansion on the shores of Lake Washington, outside Seattle. Over dinner, Gates told them he doubted that large language models could work. He would stay skeptical, he said, until the technology performed a task that required critical thinking — passing an Advanced Placement biology test, for instance. Five months later, on Aug. 24, 2022, Altman and Brockman returned and brought along an OpenAI researcher named Chelsea Voss. Voss had been a medalist in an international biology Olympiad as a high schooler. On a huge digital display on a stand outside Gates’ living room, the OpenAI crew presented a technology called GPT-4. Brockman gave the system a multiple-choice advanced biology test, and Voss graded the answers. There were 60 questions. GPT-4 got only one answer wrong. Gates sat up in his chair, his eyes opened wide. In 1980, he had a similar reaction when researchers showed him the graphical user interface that became the basis for the modern personal computer. He thought GPT was that revolutionary. By that October, Microsoft was adding the technology across its online services, including its Bing search engine. And two months later, OpenAI released its ChatGPT chatbot, which is now used by 100 million people every week. OpenAI had beat the effective altruists at Anthropic. Page’s optimists at Google scurried to release their own chatbot, Bard, but were widely perceived to have lost the race to OpenAI. Three months after ChatGPT’s release, Google stock was down 11%. Musk was nowhere to be found. But it was just the beginning. This article originally appeared in The New York Times. Enlarge this image. Enlarge this image. Enlarge this image.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2023-12-03,64,2620,2023-12-01,2023.4,Very Long,51,6,2.290076335877863,1,0,0,0,0,0,0,0,0,0,3,1.1450381679389314,1,0,0,0,1,0.38167938931297707,1,8,3.0534351145038165,1,6,2.290076335877863,1,3,1.1450381679389314,1,-0.36743446049116674,Neutral
"The Chinese company DeepSeek seemed to have come out of nowhere this week when it upturned markets. Here’s what to know about Liang Wenfeng, the engineer who started it. In technology, many entrepreneurs get only one defining act. Liang Wenfeng, the founder of DeepSeek, is already on his second. The engineer, described by colleagues as introspective, first made his mark in China’s investment world in the late 2010s, cofounding a hedge fund that used artificial-intelligence models to deliver strong returns and attracted billions of dollars in capital. Buoyed by profits and wary of Beijing’s tightening grip on speculative trading, Mr. Liang pivoted in 2023. He poured money into artificial intelligence, betting on A.I. chips and assembling a team to build China’s answer to the Silicon Valley front-runner OpenAI. Now, just two years later, DeepSeek has upended the global tech landscape. Here is what you need to know about Liang Wenfeng. He’s a deeply technical engineer. That puts him in a line of other successful Chinese tech executives. When Chinese technologists debated why the country’s biggest investors and tech firms failed to anticipate the rise of generative A.I., many pointed to a single culprit: China’s companies were obsessed with quick returns in a fiercely competitive market. Armed with those lessons — and backed by his own trading windfall — Mr. Liang has made it clear his ambitions lie far beyond commercial applications. His focus, he has said, is on what he sees as China’s only real chance to catch up with the United States. That means taking bold, idealistic swings at fundamental A.I. challenges. His primary ambition is to create artificial general intelligence or A.G.I. — the elusive goal of building machines that can think and learn like humans. When DeepSeek undercut its Chinese competition last year by offering its model at bargain prices, forcing larger rivals into their own price cuts, Mr. Liang dismissed the significance. “To be honest, we didn’t really care — it was just something we did along the way,” he said in a widely shared interview with 36Kr, a Chinese tech outlet. “Providing cloud services is not our main goal. Our aim is still to achieve A.G.I.” (DeepSeek has remained mostly silent this week and has not responded to requests for comment.) In his convictions that superhuman artificial intelligence is just around the corner, Mr. Liang sounds a lot like OpenAI’s chief executive, Sam Altman. But the similarities end there. Mr. Liang, a low-profile executive with a deep technical background in A.I. engineering, more fits the mold of Pony Ma, a co-founder of China’s Tencent, than Silicon Valley’s charismatic visionaries. He started as a hedge fund trader. And then he turned to pure A.I. research. In many ways, Mr. Liang’s career traces the major shifts in China’s technology landscape. His 2010 thesis at Zhejiang University tackled what would soon become one of the hottest topics in Chinese A.I.: improving intelligent tracking algorithms for surveillance cameras. Later, the hedge fund he co-founded was buffeted by regulatory pressures, eventually forcing the closure of one of its main investment products, according to Peter Alexander, managing director of Z-Ben Advisors, a market consultancy, who researches Chinese hedge funds. “Between 2019 and 2023, they wanted to have this side project so that their Ph.D.s felt like they had something to do and so DeepSeek originated from that,” Mr. Alexander said. “But it really went into overdrive when their primary investment product had to shut down in February of 2024,” he said. In a way, it was China’s crackdown on the private sector that nudged DeepSeek toward long-term A.I. research. He’s willing to try things other entrepreneurs will not. He even hired lit majors. If you polled China’s A.I. experts on who would deliver the country’s first major generative A.I. breakthrough, few would have picked Mr. Liang. That included the Chinese government. DeepSeek was a private company with no apparent state backing, no big-name alliances and none of the institutional heft of players like Baidu, the search giant. In a system that favored insiders, Mr. Liang was not one. Yet, there’s precedent for that. Some of China’s most disruptive tech companies — Huawei, Alibaba, ByteDance — started outside the spotlight, only to redefine their industries. Mr. Liang’s approach has been as unconventional as his company’s rise. He has emphasized intellectual exploration over sheer grind. His hiring philosophy is equally unorthodox — DeepSeek’s engineering teams have been joined by literature buffs to help refine the company’s A.I. models. “Everyone has their own unique journey and brings their own ideas with them, so there’s no need to push them,” he said in the 36Kr interview. In a tech culture defined by grueling hours and hierarchy, that outlook borders on Bohemian. Yet, Mr. Liang insists change is necessary if China wants to lead in frontier A.I. innovation. “When ChatGPT came out, the entire industry in China lacked the confidence to pursue frontier innovation,” he said. “Innovation starts with confidence — and we often see that more from young people.” Alexandra Stevenson contributed reporting from Hong Kong. Alexandra Stevenson contributed reporting from Hong Kong. By Paul Mozur",USA,"The New York Times, International edition; New York",2025-02-04,18,851,2025-02-01,2025.1,Long,52,0,0,0,2,2.3501762632197414,1,0,0,0,0,0,0,1,1.1750881316098707,1,1,1.1750881316098707,1,0,0,0,1,1.1750881316098707,1,4,4.700352526439483,1,4,4.700352526439483,1,0.6138214013241878,Positive
"When the Chinese artificial intelligence firm DeepSeek shocked Silicon Valley and Wall Street with its powerful new A.I. model, Marc Andreessen, the Silicon Valley investor, went so far as to describe it as “A.I.’s Sputnik moment.” Presumably, Mr. Andreessen wasn’t calling on the federal government to start a massive new program like NASA, which was our response to the Soviet Union’s Sputnik satellite launch; he wants the U.S. government to flood private industry with capital, to ensure that America remains technologically and economically dominant. As an antitrust enforcer, I see a different metaphor. DeepSeek is the canary in the coal mine. It’s warning us that when there isn’t enough competition, our tech industry grows vulnerable to its Chinese rivals, threatening U.S. geopolitical power in the 21st century. Although it’s unclear precisely how much more efficient DeepSeek’s models are than, say, ChatGPT, its innovations are real and undermine a core argument that America’s dominant technology firms have been pushing — namely, that they are developing the best artificial intelligence technology the world has to offer, and that technological advances can be achieved only with enormous investment — in computing power, energy generation and cutting-edge chips. For years now, these companies have been arguing that the government must protect them from competition to ensure that America stays ahead. But let’s not forget that America’s tech giants are awash in cash, computing power and data capacity. They are headquartered in the world’s strongest economy and enjoy the advantages conferred by the rule of law and a free enterprise system. And yet, despite all those advantages — as well as a U.S. government ban on the sales of cutting-edge chips and chip-making equipment to Chinese firms — America’s tech giants have seemingly been challenged on the cheap. It should be no surprise that our big tech firms are at risk of being surpassed in A.I. innovation by foreign competitors. After companies like Google, Apple and Amazon helped transform the American economy in the 2000s, they maintained their dominance primarily through buying out rivals and building anticompetitive moats around their businesses. Over the last decade, big tech chief executives have seemed more adept at reinventing themselves to suit the politics of the moment — resistance sympathizers, social justice warriors, MAGA enthusiasts — than on pioneering new pathbreaking innovations and breakthrough technologies. There have been times when Washington has embraced the argument that certain businesses deserve to be treated as national champions and, as such, to become monopolies with the expectation that they will represent America’s national interests. Those times serve as a cautionary tale. Boeing was one such star — the aircraft manufacturer’s reputation was so sterling that a former White House adviser during the Clinton administration referred to it as a “de facto national champion,” so important that “you can be an out-and-out advocate for it” in government. This superstar status was such that it likely helped Boeing gain the regulatory green light to absorb its remaining U.S. rival, McDonnell Douglas. That 1997 merger played a significant role in damaging Boeing’s culture, leaving it plagued with a host of problems, including safety concerns. On the other hand, the government’s decision to enforce antitrust laws against what is now AT&amp;T Inc., IBM and Microsoft in the 1970s through the 1990s helped create the market conditions that gave rise to Silicon Valley’s dynamism and America’s subsequent technological lead. America’s bipartisan commitment to maintaining open and competitive markets from the 1930s to the 1980s — a commitment that many European countries and Japan did not share — was critical for generating the broad-based economic growth and technological edge that catapulted the United States to the top of the world order. While monopolies may offer periodic advances, breakthrough innovations have historically come from disruptive outsiders, in part because huge behemoths rarely want to advance technologies that could displace or cannibalize their own businesses. Mired in red tape and bureaucratic inertia, those companies usually aren’t set up to deliver the seismic efficiencies that hungry start-ups can generate. The recent history of artificial intelligence demonstrates this pattern. Google developed the groundbreaking Transformer architecture that underlies today’s A.I. revolution in 2017, but the technology was largely underutilized until researchers left to join or to found new companies. It took these independent firms, not the tech giant, to realize the technology’s transformative potential. At the Federal Trade Commission, I argued that in the arena of artificial intelligence, developers should release enough information about their models to allow smaller players and upstarts to bring their ideas to market without being beholden to dominant firms’ pricing or access restrictions. Competition and openness, not centralization, drive innovation. In the coming weeks and months, U.S. tech giants may renew their calls for the government to grant them special protections that close off markets and lock in their dominance. Indeed, top executives from these firms appear eager to curry favor and cut deals, which could include asking the federal government to pare back sensible efforts to require adequate testing of models before they are released to the public, or to look the other way when a dominant firm seeks to acquire an upstart competitor. Enforcers and policymakers should be wary. During the first Trump and then the Biden administrations, antitrust enforcers brought major monopolization lawsuits against those same companies — making the case that by unlawfully buying up or excluding their rivals, these companies had undermined innovation and deprived America of the benefits that free and fair competition delivers. Reversing course would be a mistake. The best way for the United States to stay ahead globally is by promoting competition at home. Lina M. Khan served as chair of the Federal Trade Commission in the Biden administration. The Times is committed to publishing a diversity of letters to the editor. We’d like to hear what you think about this or any of our articles. Here are some tips . And here’s our email: letters@nytimes.com . Follow the New York Times Opinion section on Facebook , Instagram , TikTok , WhatsApp , X and Threads . By Lina M. Khan",USA,"The New York Times, International edition; New York",2025-02-06,14,1012,2025-02-01,2025.1,Long,53,4,3.952569169960474,1,0,0,0,1,0.9881422924901185,1,0,0,0,0,0,0,0,0,0,1,0.9881422924901185,1,4,3.952569169960474,1,12,11.857707509881422,1,1,0.9881422924901185,1,0.30802476526660205,Neutral
"China’s tech industry recently gave the U.S. tech industry — and along with it, the stock market — a rude shock when a start-up called DeepSeek unveiled an artificial intelligence model that performs on par with America’s best but that may have been developed at a small fraction of the cost and despite trade restrictions on A.I. chips. Since then there have been a lot of frantic attempts to figure out how DeepSeek did it and whether it was all above board. Those are not the most important questions, and the excessive focus on them is an example of precisely how we got caught off guard in the first place. The real lesson of DeepSeek is that America’s approach to A.I. safety and regulations — the concerns espoused by both the Biden and Trump administrations, as well as by many A.I. companies — was largely nonsense. It was never going to be possible to contain the spread of this powerful emergent technology, and certainly not just by placing trade restrictions on components like graphics chips. That was a self-serving fiction, foisted on out-of-touch leaders by an industry that wanted the government to kneecap its competitors. Instead of a futile effort to keep this genie bottled up, the government and the industry should be preparing our society for the sweeping changes that are soon to come. The misguided focus on containment is a belated echo of the nuclear age, when the United States and others limited the spread of atomic bombs by restricting access to enriched uranium, by keeping an eye on what certain scientists were doing and by sending inspectors into labs and military bases. Those measures, backed up by the occasional show of force, had a clear effect. The world hasn’t blown up — yet. One crucial difference, however, is that nuclear weapons could have been developed only by a few specialized scientists at the leading edge of their fields. The core idea that powers the artificial intelligence revolution, on the other hand, has been around since the 1940s. What opened the floodgates was the arrival first of vast data sets (via the internet and other digital technologies) and then of powerful graphics processors (like the ones from Nvidia), which can compute A.I. models from those data troves. Another difference: Each nuclear weapon has to be constructed out of steel and fissile material. Some A.I. models, on the other hand, can fit on a USB stick, and can be endlessly replicated and built upon just by plugging that stick into new laptops. Initially developing a new model, like ChatGPT, is a very costly process, but it’s the output, known as the model weights, that are so valuable, and so replicable. Companies like OpenAI, which has loudly proclaimed that A.I. poses an existential threat to humanity, kept these model weights to themselves, lest others piggyback on all that expensive development work to produce something even more powerful. And if those protection-minded companies made a lot of money because of the U.S. government’s defensive measures? Well, that’s just the price of keeping humanity safe, right? Those companies had an ally in President Joe Biden — especially, said his deputy chief of staff for policy, Bruce Reed, after he watched “Mission: Impossible — Dead Reckoning Part One,” a story of A.I. gone rogue. Having already signed one executive order restricting the sale of those crucial chips to China, Biden signed another to establish safety and security mandates. The Trump administration is operating under the same faulty logic. Just one day into his new term, President Trump and OpenAI’s chief executive, Sam Altman (fresh off his $1 million pledge to Trump’s inaugural fund), announced a vast computing infrastructure venture. Called StarGate, it is billed as a multi-hundred-billion-dollar bid to retain U.S. advantage in the fast-growing industry. DeepSeek chose the very next day as the moment to publish a paper letting the world in on its great coup. At least it’s having fun, I guess. The company says it spent little of what OpenAI and others spent, because it was able to optimize its software and train its model more efficiently. Advances like that have allowed many other technologies to become cheaper and more widely available. Still, not everyone believes that account, especially given questions about China’s respect for intellectual property rights and trade restrictions. Could the company have amassed a forbidden stash of Nvidia chips? Maybe. Could the cost of developing the model have been higher than was disclosed? Some estimates suggest so. OpenAI says that DeepSeek may have stolen some of its work. I’m gutted for the company that built a commercial product by hoovering up a big chunk of the internet then claiming it was “fair use.” (The New York Times has sued OpenAI and Microsoft over whether the use of news content in their A.I. systems is a fair use.) But whatever DeepSeek did, it, and others, can keep doing it. Already, many A.I. companies are building on DeepSeek’s model. Individuals are downloading it or querying it for only a tiny fraction of what OpenAI charges. Within the industry, there’s a popular trope that the real turning point will be the development of A.G.I., or artificial general intelligence, when A.I. reaches human-level intelligence and potentially becomes autonomous. The implication, then, is that what’s happening now is just a kind of warm-up, which no one needs to worry too much about. That’s a convenient falsehood. We have reached the other A.G.I. turning point: artificial good-enough intelligence — A.I. that is fast, cheap, scalable and useful for a wide range of purposes — and we need to engage with what’s happening now. Many observers have described this as a Sputnik moment. That’s incorrect: America can’t re-establish its dominance over the most advanced A.I. because the technology, the data and the expertise that created it are already distributed all around the world. The best way this country can position itself for the new age is to prepare for its impact. If the inevitable proliferation of A.I. endangers our cybersecurity, for example, instead of just regulating exports, it’s time to harden our networked infrastructure — which will also protect it against the ever-present threat of hacking, by random agents or hostile governments. And instead of fantasizing about how some future rogue A.I. could attack us, it’s time to start thinking clearly about how corporations and governments could use the A.I. that’s available right now to entrench their dominance, erode our rights, worsen inequality. As the technology continues to expand, who will be left behind? What rights will be threatened? Which institutions will need to be rebuilt and how? And what can we do so that this powerful technology with so much potential for good can benefit the public? It is time, too, to admit that the interests of a few large, multinational companies aren’t good proxies for the interests of the people facing such a monumental transformation. Whatever else DeepSeek may have done to get us here, perhaps forcing that realization is something we can be grateful for. The Times is committed to publishing a diversity of letters to the editor. We’d like to hear what you think about this or any of our articles. Here are some tips . And here’s our email: letters@nytimes.com . Follow the New York Times Opinion section on Facebook , Instagram , TikTok , WhatsApp , X and Threads . By Zeynep Tufekci",USA,"The New York Times, International edition; New York",2025-02-07,19,1235,2025-02-01,2025.1,Long,54,7,5.668016194331984,1,0,0,0,1,0.8097165991902834,1,0,0,0,0,0,0,0,0,0,0,0,0,3,2.4291497975708505,1,4,3.2388663967611335,1,0,0,0,-0.3009734196584068,Neutral
"It was a sunny Saturday afternoon, and dozens of people sat in the grass around a backyard stage where aspiring founders of tech start-ups talked about their ideas. People in the crowd slouched over laptops, vaping and drinking strawberry Frappuccinos. A drone buzzed overhead. Inside the house, investors took pitches in the kitchen. It looked like Silicon Valley, but it was Liangzhu, a quiet suburb of the southern Chinese city of Hangzhou, which is a hot spot for entrepreneurs and tech talent lured by low rents and proximity to tech companies like Alibaba and DeepSeek. “People come here to explore their own possibilities,” said Felix Tao, 36, a former Facebook and Alibaba employee who hosted the event. Virtually all of those possibilities involve artificial intelligence. As China faces off with the United States over tech primacy, Hangzhou has become the center of China’s A.I. frenzy. A decade ago, the provincial and local governments started offering subsidies and tax breaks to new companies in Hangzhou, a policy that has helped incubate hundreds of start-ups. On weekends, people fly in from Beijing, Shanghai and Shenzhen to hire programmers. Lately, many of them have ended up in Mr. Tao’s backyard. He helped found an A.I. research lab at Alibaba before leaving to start his own company, Mindverse, in 2022. Now Mr. Tao’s home is a hub for coders who have settled in Liangzhu, many in their 20s and 30s. They call themselves “villagers,” writing code in coffee shops during the day and gaming together at night, hoping to harness A.I. to create their own companies. Hangzhou has already birthed tech powerhouses, not only Alibaba and DeepSeek but also NetEase and Hikvision. In January, DeepSeek shook the tech world when it released an A.I. system that it said it had made for a small fraction of the cost that Silicon Valley companies had spent on their own. Since then, systems made by DeepSeek and Alibaba have ranked among the top-performing open source A.I. models in the world, meaning they are available for anyone to build on. Graduates from Hangzhou’s Zhejiang University, where DeepSeek’s founder studied, have become sought-after employees at Chinese tech companies. Chinese media closely followed the poaching of a core member of DeepSeek’s team by the electronics company Xiaomi. In Liangzhu, many engineers said they were killing time until they could create their own start-ups, waiting out noncompete agreements they had signed at bigger companies like ByteDance. DeepSeek is one of six A.I. and robotics start-ups from the city that Chinese media calls the “six tigers of Hangzhou.” Last year, one of the six, Game Science, released China’s first big-budget video game to become a global hit, Black Myth: Wukong. Another firm, Unitree, grabbed public attention in January when its robots danced onstage during the Chinese state broadcaster’s televised annual spring gala. This spring, Mingming Zhu, the founder of Rokid, a Hangzhou start-up that makes A.I.-enabled eyeglasses, invited the six founders to his home for dinner. It was the first time they had all met in person, Mr. Zhu said. Like him, most of the six had studied at Zhejiang University or worked at Alibaba. “When we started, we were small fish,” Mr. Zhu said. “But even then, the government helped out.” He said government officials had helped him connect with Rokid’s earliest investors, including Jack Ma, the founder of Alibaba. But some said the government support for Hangzhou’s tech scene had scared off some investors. Several company founders who asked not to be named so they could discuss sensitive topics said it was difficult for them to attract funds from foreign venture capital firms, frustrating their ambitions to grow outside China. The nightmare situation, they said, would be to end up like ByteDance, the Chinese parent of TikTok, whose executives have been questioned before Congress about the company’s ties to the Chinese government. Founders described choosing between two paths for their companies’ growth: Take government funding and tailor their product to the Chinese market, or raise enough money on their own to set up offices in a country like Singapore to pitch foreign investors. For most, the first was the only feasible option. Another uncertainty is access to the advanced computer chips that power artificial intelligence systems. Washington has spent years trying to prevent Chinese companies from buying these chips, and Chinese companies like Huawei and Semiconductor Manufacturing International Corporation are racing to produce their own. So far, the Chinese-made chips work well enough to help companies like ByteDance provide some of their A.I. services in China. Many Chinese companies have created stockpiles of Nvidia chips despite Washington’s controls. But it is not clear how long that supply will last, or how quickly China’s chip makers can catch up to their American counterparts. A seemingly inescapable concept in Hangzhou is “agentic A.I.,” the idea that an artificial intelligence system could be directed to act on its own. Qian Roy, another Hangzhou entrepreneur, has developed an A.I.-enabled digital companion for young people that responds to their moods based on information from the Myers-Briggs personality test, which is popular among young people in China. His team programmed his app, All Time, using publicly available A.I. systems, including those made by DeepSeek, Alibaba and Anthropic, an American start-up. Mindverse, the company co-founded by Mr. Tao, who hosted the backyard event, is working on a product that would use A.I. to help people manage their lives. It can send supportive daily emails to colleagues, for example, or regular text messages to parents reminiscing about family vacations. “I don’t want the A.I. to just handle tasks, but to actually give you more mental space so you can unplug,” Mr. Tao said. Many in the crowd in Mr. Tao’s backyard said the atmosphere in Hangzhou, set on the banks of a lake that was muse to generations of Chinese poets and painters, fueled their creativity. Lin Yuanlin started his company, Zeabur, while studying at Zhejiang University. His company provides back-end systems to people who are making apps and websites by “vibecoding,” or using A.I. tools to program without deep software knowledge. Liangzhu is the perfect testing ground for his product, Mr. Lin said. He can lean over to someone in a coffee shop or wander into a neighbor’s living room and learn what kind of support they need for their start-ups. Mr. Lin found himself going to Liangzhu so often that he moved there. Liangzhu villagers have been hosting film nights. They had recently gathered to watch “The Matrix.” Afterward, they decided the movie should be required viewing, Mr. Lin said. Its theme — people finding their way out of a vast system controlling society — provided spot-on inspiration. Aspiring founders in Liangzhu, even those who did not go to top universities, believe they could start the next world-changing tech company, Mr. Tao said. “Many of them are super brave to make a choice to explore their own way, because in China that is not the common way to live your life.” Siyi Zhao contributed research. Siyi Zhao contributed research. By Meaghan Tobin",USA,"The New York Times, International edition; New York",2025-07-08,24,1176,2025-07-01,2025.3,Long,55,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,2.5510204081632653,1,1,0.8503401360544217,1,0,0,0,0.12442301228168334,Neutral
"Enlarge this image. SAN FRANCISCO — Almost every day, Grant Lee, a Silicon Valley entrepreneur, hears from investors who try to persuade him to take their money. Some have even sent him and his co-founders personalized gift baskets. Lee, 41, would normally be flattered. In the past, a fast-growing startup like Gamma, an artificial intelligence company he helped establish in 2020, would have constantly looked out for more funding. But like many young startups in Silicon Valley today, Gamma is pursuing a different strategy. It is using AI tools to increase its employees’ productivity in everything from customer service and marketing to coding and customer research. That means Gamma, which makes software that lets people create presentations and websites, has no need for more cash, Lee said. His company has hired only 28 people to get “tens of millions” in annual recurring revenue and nearly 50 million users. Gamma is also profitable. “If we were from the generation before, we would easily be at 200 employees,” Lee said. “We get a chance to rethink that, basically rewrite the playbook.” The old Silicon Valley model dictated that startups should raise a huge sum of money from venture capital investors and spend it hiring an army of employees to scale up fast. Profits would come much later. Until then, head count and fundraising were badges of honor among founders, who philosophized that bigger was better. But Gamma is among a growing cohort of startups, most of them working on AI products, that are also using AI to maximize efficiency. They make money and are growing fast without the funding or employees they would have needed before. The biggest bragging rights for these startups are for making the most revenue with the fewest workers. Stories of “tiny team” success have now become a meme, with techies excitedly sharing lists that show how Anysphere, a startup that makes the coding software Cursor, hit $100 million in annual recurring revenue in less than two years with just 20 employees, and how ElevenLabs, an AI voice startup, did the same with about 50 workers. The potential for AI to let startups do more with less has led to wild speculation about the future. OpenAI CEO Sam Altman has predicted there could someday be a one-person company worth $1 billion. His company, which is building a cost-intensive form of AI called a foundational model, employs more than 4,000 people and has raised more than $20 billion in funding. It is also in talks to raise more money. With AI tools, some startups are now declaring that they will stop hiring at a certain size. Runway Financial, a finance software company, has said it plans to top out at 100 employees because each of its workers will do the work of 1.5 people. Agency, a startup using AI for customer service, also plans to hire no more than 100 workers. “It’s about eliminating roles that are not necessary when you have smaller teams,” said Elias Torres, Agency’s founder. The idea of AI-driven efficiency was bolstered in January by DeepSeek, a Chinese AI startup that showed it could build AI tools for a small fraction of the typical cost. Its breakthrough, built on open-source tools that are freely available online, set off an explosion of companies building new products using DeepSeek’s inexpensive techniques. “DeepSeek was a watershed moment,” said Gaurav Jain, an investor at the venture firm Afore Capital, which has backed Gamma. “The cost of compute is going to go down very, very fast, very quickly.” Jain compared new AI startups to the wave of companies that arose in the late 2000s, after Amazon began offering cheap cloud computing services. That lowered the cost of starting a company, leading to a flurry of new startups that could be built more cheaply. Before this AI boom, startups generally burned $1 million to get to $1 million in revenue, Jain said. Now, getting to $1 million in revenue costs one-fifth as much and could eventually drop to one-tenth, according to an analysis of 200 startups conducted by Afore. “This time, we’re automating humans as opposed to just the data centers,” Jain said. But if startups can become profitable without spending much, that could become a problem for venture capital investors, who allocate tens of billions to invest in AI startups. Last year, AI companies raised $97 billion in funding, making up 46% of all venture investment in the United States, according to PitchBook, which tracks startups. “Venture capital only works if you get money into the winners,” said Terrence Rohan, an investor with Otherwise Fund, which focuses on very young startups. He added: “If the winner of the future needs a lot less money because they’ll have a lot less people, how does that change VC?” For now, investors continue to fight to get into the hottest companies, many of which have no need for more money. Scribe, an AI productivity startup, grappled last year with far more interest from investors than the $25 million it wanted to raise. “It was a negotiation of what is the smallest amount we could possibly take on,” said Scribe CEO Jennifer Smith. She said investors were shocked at the size of her staff — 100 people — when compared with its 3 million users and fast growth. Some investors are optimistic that AI-driven efficiency will spur entrepreneurs to create more companies, leading to more opportunities to invest. They hope that once the startups reach a certain size, the firms will adopt the old model of big teams and big money. Some young companies, including Anysphere, are already doing that. Anysphere has raised $175 million in funding, with plans to add staff and conduct research, according to the company’s president, Oskar Schulz. Other founders have seen the perils of the old startup playbook, which kept companies on a fundraising treadmill where hiring more people created more costs that went beyond just their salaries. Bigger teams needed managers, more robust human resources and back-office support. Those teams then needed specialized software, along with a bigger office with all the perks — and so on, which led startups to burn through cash and forced founders to constantly raise more money. Many startups from the funding boom of 2021 eventually downsized, shut down or scrambled to sell themselves. Turning a profit early on can change that outcome. At Gamma, employees use about 10 AI tools to help them be more efficient, including Intercom’s customer service tool for handling problems, Midjourney’s image generator for marketing, Anthropic’s Claude chatbot for data analysis and Google’s NotebookLM for analyzing customer research. Engineers also use Anysphere’s Cursor to more efficiently write code. Gamma’s product, which is built on top of tools from OpenAI and others, is also not as expensive to make as other AI products. (The New York Times has sued OpenAI and its partner, Microsoft, claiming copyright infringement of news content related to AI systems. The two companies have denied the suit’s claims.) Other efficient startups are taking a similar strategy. Thoughtly, a 10-person provider of AI phone agents, turned a profit in 11 months, thanks to its use of AI, said co-founder Torrey Leonard. Payment processor Stripe created an AI tool that helps Leonard analyze Thoughtly’s sales, something he would have previously hired an analyst to do. Without that and AI tools from others to streamline its operations, Thoughtly would need at least 25 people and be far from profitable, he said. Thoughtly will eventually raise more money, Leonard said, but only when it is ready. Not worrying about running out of cash is “a huge relief,” he said. At Gamma, Lee said he planned to roughly double the workforce this year to 60, hiring for design, engineering and sales. He plans to recruit a different type of worker from before, seeking out generalists who do a range of tasks rather than specialists who do only one thing, he said. He also wants “player-coaches” instead of managers — people who can mentor less experienced employees but can also pitch in on the day-to-day work. Lee said the AI-efficient model had freed up time he would have otherwise spent managing people and recruiting. Now, he focuses on talking to customers and improving the product. In 2022, he created a Slack room for feedback from Gamma’s top users, who are often shocked to discover that the CEO was responding to their comments. “That’s actually every founder’s dream,” Lee said. This article originally appeared in The New York Times.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-03-09,46,1417,2025-03-01,2025.1,Long,56,1,0.7057163020465773,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.7057163020465773,1,17,11.997177134791814,1,0.48868486567013103,Positive
"Tech start-ups typically raised huge sums to hire armies of workers and grow fast. Now artificial intelligence tools are making workers more productive and spurring tales of ""tiny team"" success. Almost every day, Grant Lee, a Silicon Valley entrepreneur, hears from investors who try to persuade him to take their money. Some have even sent him and his co-founders personalized gift baskets. Mr. Lee, 41, would normally be flattered. In the past, a fast-growing start-up like Gamma, the artificial intelligence start-up he helped establish in 2020, would have constantly looked out for more funding. But like many young start-ups in Silicon Valley today, Gamma is pursuing a different strategy. It is using artificial intelligence tools to increase its employees' productivity in everything from customer service and marketing to coding and customer research. That means Gamma, which makes software that lets people create presentations and websites, has no need for more cash, Mr. Lee said. His company has hired only 28 people to get ""tens of millions"" in annual recurring revenue and nearly 50 million users. Gamma is also profitable. ""If we were from the generation before, we would easily be at 200 employees,"" Mr. Lee said. ""We get a chance to rethink that, basically rewrite the playbook."" The old Silicon Valley model dictated that start-ups should raise a huge sum of money from venture capital investors and spend it hiring an army of employees to scale up fast. Profits would come much later. Until then, head count and fund-raising were badges of honor among founders, who philosophized that bigger was better. But Gamma is among a growing cohort of start-ups, most of them working on A.I. products, that are also using A.I. to maximize efficiency. They make money and are growing fast without the funding or employees they would have needed before. The biggest bragging rights for these start-ups are for making the most revenue with the fewest workers. Stories of ""tiny team"" success have now become a meme, with techies excitedly sharing lists that show how Anysphere, a start-up that makes the coding software Cursor, hit $ 100 million in annual recurring revenue in less than two years with just 20 employees, and how ElevenLabs, an A.I. voice start-up, did the same with around 50 workers. The potential for A.I. to let start-ups do more with less has led to wild speculation about the future. Sam Altman, the chief executive of OpenAI, has predicted there could someday be a one-person company worth $ 1 billion. His company, which is building a cost-intensive form of A.I. called a foundational model, employs more than 4,000 people and has raised more than $ 20 billion in funding. It is also in talks to raise more money. With A.I. tools, some start-ups are now declaring that they will stop hiring at a certain size. Runway Financial, a finance software company, has said it plans to top out at 100 employees because each of its workers will do the work of 1.5 people. Agency, a start-up using A.I. for customer service, also plans to hire no more than 100 workers. ""It's about eliminating roles that are not necessary when you have smaller teams,"" said Elias Torres, Agency's founder. The idea of A.I.-driven efficiency was bolstered last month by DeepSeek, the Chinese A.I. start-up that showed it could build A.I. tools for a small fraction of the typical cost. Its breakthrough, built on open source tools that are freely available online, set off an explosion of companies building new products using DeepSeek's inexpensive techniques. ""DeepSeek was a watershed moment,"" said Gaurav Jain, an investor at the venture firm Afore Capital, which has backed Gamma. ""The cost of compute is going to go down very, very fast, very quickly."" Mr. Jain compared new A.I. start-ups to the wave of companies that arose in the late 2000s, after Amazon began offering cheap cloud computing services. That lowered the cost of starting a company, leading to a flurry of new start-ups that could be built more cheaply. Before this A.I. boom, start-ups generally burned $ 1 million to get to $ 1 million in revenue, Mr. Jain said. Now getting to $ 1 million in revenue costs one-fifth as much and could eventually drop to one-tenth, according to an analysis of 200 start-ups conducted by Afore. ""This time we're automating humans as opposed to just the data centers,"" Mr. Jain said. But if start-ups can become profitable without spending much, that could become a problem for venture capital investors, who allocate tens of billions to invest in A.I. start-ups. Last year, A.I. companies raised $ 97 billion in funding, making up 46 percent of all venture investment in the United States, according to PitchBook, which tracks start-ups. ""Venture capital only works if you get money into the winners,"" said Terrence Rohan, an investor with Otherwise Fund, which focuses on very young start-ups. He added, ""If the winner of the future needs a lot less money because they'll have a lot less people, how does that change V.C.?"" For now, investors continue to fight to get into the hottest companies, many of which have no need for more money. Scribe, an A.I. productivity start-up, grappled last year with far more interest from investors than the $ 25 million it wanted to raise. ""It was a negotiation of what is the smallest amount we could possibly take on,"" said Jennifer Smith, Scribe's chief executive. She said investors were shocked at the size of her staff -- 100 people -- when compared with its three million users and fast growth. Some investors are optimistic that A.I.-driven efficiency will spur entrepreneurs to create more companies, leading to more opportunities to invest. They hope that once the start-ups reach a certain size, the firms will adopt the old model of big teams and big money. Some young companies, including Anysphere, the one behind Cursor, are already doing that. Anysphere has raised $ 175 million in funding, with plans to add staff and conduct research, according to the company's president, Oskar Schulz. Other founders have seen the perils of the old start-up playbook, which kept companies on a fund-raising treadmill where hiring more people created more costs that went beyond just their salaries. Bigger teams needed managers, more robust human resources and back office support. Those teams then needed specialized software, along with a bigger office with all the perks. And so on, which led start-ups to burn through cash and forced founders to constantly raise more money. Many start-ups from the funding boom of 2021 eventually downsized, shut down or scrambled to sell themselves. Turning a profit early on can change that outcome. At Gamma, employees use about 10 A.I. tools to help them be more efficient, including Intercom's customer service tool for handling problems, Midjourney's image generator for marketing, Anthropic's Claude chatbot for data analysis and Google's NotebookLM for analyzing customer research. Engineers also use Anysphere's Cursor to more efficiently write code. Gamma's product, which is built on top of tools from OpenAI and others, is also not as expensive to make as other A.I. products. (The New York Times has sued OpenAI and its partner, Microsoft, claiming copyright infringement of news content related to A.I. systems. The two companies have denied the suit's claims.) Other efficient start-ups are taking a similar strategy. Thoughtly, a 10-person provider of A.I. phone agents, turned a profit in 11 months, thanks to its use of A.I., its co-founder Torrey Leonard said. The payment processor Stripe created an A.I. tool that helps Mr. Leonard analyze Thoughtly's sales, something he would have previously hired an analyst to do. Without that and A.I. tools from others to streamline its operations, Thoughtly would need at least 25 people and be far from profitable, he said. Thoughtly will eventually raise more money, Mr. Leonard said, but only when it is ready. Not worrying about running out of cash is ""a huge relief,"" he said. At Gamma, Mr. Lee said he planned to roughly double the work force this year to 60, hiring for design, engineering and sales. He plans to recruit a different type of worker from before, seeking out generalists who do a range of tasks rather than specialists who do only one thing, he said. He also wants ""player-coaches"" instead of managers -- people who can mentor less experienced employees but can also pitch in on the day-to-day work. Mr. Lee said the A.I.-efficient model had freed up time he would have otherwise spent managing people and recruiting. Now he focuses on talking to customers and improving the product. In 2022, he created a Slack room for feedback from Gamma's top users, who are often shocked to discover that the chief executive was responding to their comments. ""That's actually every founder's dream,"" Mr. Lee said. Photograph PHOTO (PHOTOGRAPH BY KELSEY MCCLELLAN FOR THE NEW YORK TIMES); Sam Altman, the chief executive of OpenAI, has predicted there could someday be a one-person company worth $ 1 billion. (B3) This article appeared in print on page B1, B3.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-03-10,42,1508,2025-03-01,2025.1,Very Long,57,1,0.6631299734748011,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.6631299734748011,1,17,11.273209549071618,1,0.5464692123789824,Positive
"A test of the app Dia illustrates that the humble web browser may be the path to making artificial intelligence more natural to use. When was the last time you thought about your web browser? If you don't remember, no one will blame you. Web browsers have remained fundamentally unchanged for decades: You open an app, such as Chrome, Safari or Firefox, and type a website into the address bar. Many of us settled on one and fell into what I call ''browser inertia,'' never bothering to see if there's anything better. Yet a web browser is important because so much of what we do on computers takes place inside one, including word processing, chatting on Slack and managing calendars and email. That's why I felt excited when I recently tried Dia, a new kind of web browser from the Browser Company of New York, a start-up. The app is powered by generative artificial intelligence, the technology driving popular chatbots like ChatGPT and Google's Gemini, to answer our questions. Dia illuminates how a web browser can do much more than load websites -- and even help us learn and save time. I tested Dia for a week and found myself browsing the web in new ways. In seconds, the browser provided a written recap of a 20-minute video without my watching its entirety. While scanning a breaking news article, the browser generated a list of other relevant articles for a deeper understanding. I even wrote to the browser's built-in chatbot for help proofreading a paragraph of text. Dia is on the cusp of an emerging era of A.I.-powered internet navigators that could persuade people to try something new. This week, Perplexity, a start-up that makes a search engine, announced an A.I. web browser called Comet, and some news outlets have reported that OpenAI, the company behind ChatGPT, also plans to release a browser this year. OpenAI declined to comment. (The New York Times has sued OpenAI and its partner, Microsoft, claiming copyright infringement of news content related to A.I. systems. The two companies have denied the suit's claims.) Tech behemoths like Google and Apple have added lightweight A.I. features into their existing browsers, Chrome and Safari, including tools for proofreading text and automatically summarizing articles. Dia, which has not yet been publicly released, is available as a free app for Mac computers on an invitation-only basis. (Times readers can click here for an invite.) What does this all mean for the future of the web? Here's what you need to know. What is an A.I. browser, and what does it do? Like other web browsers, Dia is an app you open to load web pages. What's unique is the way the browser seamlessly integrates an A.I. chatbot to help -- without leaving the webpage. Hitting a shortcut (command+E) in Dia opens a small window that runs parallel to the webpage. Here, you can type questions related to the content you are reading or the video you are watching, and a chatbot will respond. For example: While writing this column on the Google Docs website, I asked the chatbot if I used ''on the cusp'' correctly, and it confirmed that I did. While reading a news article about the Texas floods, I asked the browser's chatbot to tell me more about how the crisis unfolded. The bot generated a summary about the history of Texas' public safety infrastructure and included a list of relevant articles. While watching a 22-minute YouTube video about car jump starters, I asked the chatbot to tell me which tools were best. Dia immediately pulled from the video's transcript to produce a summary of the top contenders, sparing me the need to watch the entire thing. In contrast, chatbots like ChatGPT, Gemini and Claude require opening a separate tab or app and pasting in content for the chatbot to evaluate and answer questions, a process that has always busted my workflow. How does it work? A.I. chatbots like ChatGPT, Gemini and Claude generate responses using large language models, systems that use complex statistics to guess which words belong together. Each chatbot's model has its strengths and weaknesses. The Browser Company of New York said it had teamed up with multiple companies to use their A.I. models, including the ones behind Gemini, ChatGPT and Claude. When users type a question, the Dia browser analyzes it and pulls answers from whichever A.I. model is best suited for answering. For instance, Anthropic's A.I. model, Claude Sonnet, specializes in computer programming. So if you have questions about something you are coding, the browser will pull an answer from that model. If you have questions about writing, the Dia browser may generate an answer with the model that OpenAI uses for ChatGPT, which is well known for handling language. What I appreciate about this design is that you, the user, don't need to know or think about which chatbot to use. That makes generative A.I. more accessible to the mainstream. ''You should just be able to say, 'Hey, I'm looking at this thing, I've got a question about it,''' said Josh Miller, the chief executive of the Browser Company, which was founded in 2020 and has raised over $100 million. ''We should be able to answer it for you and do work on your behalf.'' But aren't there imperfections? While Dia proved helpful in most of my tests, it was, like all generative A.I. tools, sometimes incorrect. While I was browsing Wirecutter, our sister publication that reviews products, I asked the chatbot if there were any deals on the site for water filters. The chatbot said no, even as I read about a water filtration system that was on sale. Mr. Miller said that because the browser drew answers from various A.I. models, its responses were subject to the same mistakes as their respective chatbots. Those occasionally get facts wrong and even make things up, a phenomenon known as ''hallucination.'' More often than not, however, I found Dia to be more accurate and helpful than a stand-alone chatbot. Still, I double-checked answers by clicking on any links Dia's bot was citing, like the articles about the recent floods in Texas. What about privacy? Asking A.I. to help with a webpage you're looking at means that data may be shared with whatever A.I. model is being used to answer the question, which raises privacy concerns. The Browser Company said that only the necessary data related to your requests was shared with its partners providing A.I. models, and that those partners were under contract to dispose of your data. Privacy experts have long warned not to share any sensitive information, like a document containing trade secrets, with an A.I. chatbot since a rogue employee could gain access to the data. So I recommend asking Dia's chatbot for help only with innocuous browsing activities like parsing a YouTube video. But when browsing something you wouldn't want others to know about, like a health condition, refrain from using the A.I. This exchange -- potentially giving up some privacy to get help from A.I. -- may be the new social contract going forward. How much will this cost? Dia is free, but A.I. models have generally been very expensive for companies to operate. Consumers who rely on Dia's A.I. browser will eventually have to pay. Mr. Miller said that in the coming weeks, Dia would introduce subscriptions costing $5 a month to hundreds of dollars a month, depending on how frequently a user prods its A.I. bot with questions. The browser will remain free for those who use the A.I. tool only a few times a week. So whether or not an A.I. browser will be your next web browser will depend largely on how much you want to use, and pay, for these services. So far, only 3 percent of the people who use A.I. every day are paid users, according to a survey by Menlo Ventures, a venture capital firm. That number could grow, of course, if generative A.I. becomes a more useful tool that we naturally use in everyday life. I suspect the humble web browser will open that path forward. Photograph Dia has its A.I. chatbot in the browser in a parallel window so you can stay on the webpage.; Users can ask Dia's chatbot a question about information on the webpage they are viewing. (B6) This article appeared in print on page B1, B6.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-07-12,40,1402,2025-07-01,2025.3,Long,58,1,0.7132667617689016,1,4,2.8530670470756063,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0.7132667617689016,1,0,0,0,0,0,0,0,0,0,-0.18252156248410742,Neutral
"I took three Waymo rides this month while in San Francisco for an economics conference. The smooth trips made for a haunting vision of the potential future of artificial intelligence. Inside the cabs, there was gentle New Age music and no one in the driver’s seat. Such could be the future of the economy in general if artificial intelligence substitutes for human labor in more and more occupations. The unemployed masses could come to depend on the charity of billionaires and trillionaires who own the means of intellectual production. But A.I. could also be designed to empower people rather than replace them, as I wrote a year ago in a newsletter about the M.I.T. Shaping the Future of Work Initiative. Which of those A.I. futures will be realized was a big topic at the San Francisco conference, which was the annual meeting of the American Economic Association, the American Finance Association and 65 smaller groups in the Allied Social Science Associations. Erik Brynjolfsson of Stanford was one of the busiest economists at the conference, dashing from one panel to another to talk about his hopes for a human-centric A.I. and his warnings about what he has called the “Turing Trap.” Alan Turing, the English mathematician and World War II code breaker, proposed in 1950 to evaluate the intelligence of computers by whether they could fool someone into thinking they were human. His “imitation game” led the field in an unfortunate direction, Brynjolfsson argues — toward creating machines that behaved as much like humans as possible, instead of like human helpers. Henry Ford didn’t set out to build a car that could mimic a person’s walk, so why should A.I. experts try to build systems that mimic a person’s mental abilities? Brynjolfsson asked at one session I attended. Other economists have made similar points: Daron Acemoglu of M.I.T. and Pascual Restrepo of Boston University use the term “so-so technologies” for systems that replace human beings without meaningfully increasing productivity, such as self-checkout kiosks in supermarkets. People will need a lot more education and training to take full advantage of A.I.’s immense power, so that they aren’t just elbowed aside by it. “In fact, for each dollar spent on machine learning technology, companies may need to spend nine dollars on intangible human capital,” Brynjolfsson wrote in 2022, citing research by him and others. A big question is who will pay for all that education. Employers fear that if they train their work force, the employees might take their in-demand skills to a competitor. And the workers may not be able to afford it on their own. This implies, Brynjolfsson wrote, that governments “should directly provide this training or provide incentives for corporate training.” Empowering workers might seem utopian but it’s the historical norm. Waymo aside, most technologies over the centuries have made people more powerful and effective (mechanized looms) or created new products (nylon). Interestingly, Waymo’s parent, Alphabet, is doing some of the coolest work on A.I. that extends human reach. Two researchers at Google DeepMind in London — a unit of Alphabet — shared the 2024 Nobel Prize in Chemistry with an American scholar for predicting the structure of proteins with the help of artificial intelligence. Various units of Google are also working on better weather forecasts, flood prediction and quantum computing, James Manyika, the senior vice president for research, technology and society at Google, who spoke at the San Francisco conference, said last year. The flood prediction system was first tested in Bangladesh and is being used in 100 countries with a combined 700 million people. Google is also using A.I. to reduce jet vapor trails and detect diabetic retinopathy, a preventable cause of blindness. Google is putting much of its energy into “agentic systems,” meaning intelligent agents working for people, Manyika stressed at the conference. A day may come when A.I. is so powerful that it’s better at every conceivable human activity, including child-rearing, according to Nick Bostrom, a Swedish-born philosopher whose book I covered in a newsletter last year. But there’s no reason for humanity to race toward that dystopian outcome. As Brynjolfsson wrote in his 2022 essay: “The future is not preordained.” Elsewhere: We’re Still Burning Coal “Coal is often considered a fuel of the past, but global consumption of it has doubled in the past three decades,” the International Energy Agency wrote in a report last month. Coal consumption fell during the pandemic, but has rebounded since, in part because natural gas, a rival energy source, has gotten more expensive. (Russia’s invasion of Ukraine is one reason for that.) Global coal demand “is set to plateau” in the next three years, as declining demand from advanced economies is offset by increases in developing economies such as India, Indonesia and Vietnam, the report said. Quote of the Day “Last year we celebrated the long-awaited arrival of Mickey and Minnie Mouse into the public domain. In 2025 we welcome a dozen new Mickey Mouse films from 1929. Mickey speaks his first words — ‘Hot dogs! Hot dogs!’ — and debuts his familiar white gloves. That version of Mickey is now officially in the public domain.” — Jennifer Jenkins and James Boyle, “January 1, 2025, is Public Domain Day: Works from 1929 are open to all, as are sound recordings from 1924!” (undated blog post) By Peter Coy",USA,"The New York Times, International edition; New York",2025-01-15,16,889,2025-01-01,2025.1,Long,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.124859392575928,1,1,1.124859392575928,1,-0.028424709525228926,Neutral
"The artificial intelligence start-up's funding shows investors remain enthusiastic about the A.I. boom. In October, OpenAI, the start-up behind ChatGPT, raised one of the largest rounds of venture capital, bringing in $ 6.5 billion. On Tuesday, another artificial intelligence start-up, Databricks, announced an even bigger haul: It is set to collect $ 10 billion in a new funding round, which would value the company at $ 62 billion. The record fundings show that two years into an A.I. boom, investor enthusiasm for the technology has not waned. In recent months, some A.I. start-ups have struggled to find their footing and been sold or folded into larger companies. Even the fastest-growing companies are burning enormous sums of cash. OpenAI recently told investors that it expected to lose $ 5 billion this year on $ 3.7 billion in sales. Investors remain bullish. Databricks, which was founded in 2013 and provides software tools for storing and analyzing large amounts of online data, said it expected in January to have more than $ 3 billion of ""revenue run rate,"" or monthly revenue extrapolated for a full year. It has morphed into an A.I. company in recent years, helping businesses build and operate the kind of software that drives chatbots and similar A.I. services. The San Francisco-based company also said it expected to have a ""positive free cash flow"" for the three months ending Jan. 31, a sign that its income was mostly outpacing its spending. The company sells its products to more than 10,000 customers, including Shell and Comcast. More than 500 customers are on a pace to pay Databricks over $ 1 million a year for its offerings, the company said. At a valuation of $ 62 billion, Databricks would surpass the market capitalization of its main competitor, Snowflake, which is publicly traded. In a statement, Databricks said it had so far secured $ 8.6 billion of the $ 10 billion that it was raising. Ali Ghodsi, the chief executive and a founder of Databricks, said the rest of the round was ""substantially oversubscribed."" ""These are still the early days of A.I.,"" he said. Databricks said it planned to use the money for new products, acquisitions and international expansion. It also plans to let its employees cash out some of their shares. Allowing start-up employees to sell shares before a traditional ""liquidity event,"" such as an initial public offering or acquisition, is a growing trend among older tech start-ups. The time between a start-up's raising funding and going public or selling has gotten longer over the past decade. Plentiful venture capital in the private markets means companies are under less pressure to tap the public markets for funding. That trend has been especially acute in recent years as I.P.O.s have dried up and antitrust scrutiny of big tech companies has squashed acquisitions. Some expect that I.P.O.s and deal-making will flourish next year under President-elect Donald J. Trump. But the hottest private tech companies, including Databricks, the payments firm Stripe and the rocket company SpaceX, have so much interest from private investors that they may choose to stay private and continue letting their employees and early investors cash out via private stock sales. Thrive Capital, the New York investment firm founded by Joshua Kushner, led the funding. Thrive also led OpenAI's round of funding in October. The firm, known for its early bets on Instagram, Stripe and Spotify, has been aggressively investing in the A.I. boom. In August, it raised $ 5 billion in new funds. Databricks' existing investors, which include Andreessen Horowitz, DST Global, GIC, Insight Partners and WCM Investment Management, also helped lead the round. (The New York Times has sued OpenAI, claiming copyright infringement of news content related to A.I. systems. OpenAI has denied the claims.) Cade Metz contributed reporting. Cade Metz contributed reporting. Photograph Ali Ghodsi, the chief executive and a founder of Databricks, at a 2019 event. (PHOTOGRAPH BY DATABRICKS, VIA REUTERS) This article appeared in print on page B4.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2024-12-19,20,661,2024-12-01,2024.4,Medium,60,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,6.051437216338881,1,-0.22437627605933722,Neutral
"EMPIRE OF AI: Dreams and Nightmares in Sam Altman’s OpenAI , by Karen Hao THE OPTIMIST: Sam Altman, OpenAI, and the Race to Invent the Future , by Keach Hagey The “paper clip problem” is a well‑known ethics thought experiment in the world of artificial intelligence. It imagines a superintelligent A.I. charged with the seemingly harmless goal of making as many paper clips as possible. Trouble is, as the philosopher Nick Bostrom put it in 2003, without common-sense limits it might transform “first all of earth and then increasing portions of space into paper clip manufacturing facilities.” The tale has long served as a warning about objectives pursued too literally. Two new books that orbit the entrepreneur Sam Altman and the firm he co-founded, OpenAI, suggest we may already be living with a version of the problem. In “Empire of AI,” the journalist Karen Hao, who has worked for The Wall Street Journal and contributes to The Atlantic, argues that the pursuit of an artificial superintelligence has become its own figurative paper clip factory, devouring too much energy, minerals and human labor. Meanwhile, “The Optimist,” by the Wall Street Journal reporter Keach Hagey, leaves readers suspecting that the earnest and seemingly innocuous paper clip maker who ends up running the world for his own ends could be Altman himself. “Empire of AI” is the broader and more critical of the two. Hao profiled OpenAI in 2020, two years before its most famous product, the intelligent chatbot called ChatGPT, debuted publicly. She portrays OpenAI and other companies that make up the fast‑growing A.I. sector as a “modern-day colonial world order.” Much like the European powers of the 18th and 19th centuries, they “seize and extract precious resources to feed their vision of artificial intelligence.” In a corrective to tech journalism that rarely leaves Silicon Valley, Hao ranges well beyond the Bay Area with extensive fieldwork in Kenya, Colombia and Chile. “The Optimist” is a more conventional biography, concentrated on Altman’s life and times. Born in Chicago to progressive parents named Connie and Jerry — in the 1980s, Jerry innovated a way to stir investment in affordable housing — Altman was heavily influenced by their do-gooder spirit. (“You can’t out‑nice Jerry,” his friends would say.) Altman’s relentlessly upbeat manner and genuine technical skill made him a perfect fit for Silicon Valley. Charming and smart, he tells people what they want to hear and has a knack for talking big in exactly the way 2010s Bay Area investors liked. The arc of Altman’s life also follows a classic script. He drops out of Stanford to launch a start‑up that fizzles, but the effort brings him to the attention of Paul Graham, the co-founder of Y Combinator, an influential tech incubator that launched companies like Airbnb and Dropbox. By age 28, Altman has risen to succeed Graham as the organization’s president, setting the stage for his leadership in the A.I. revolution. As Hagey makes clear, success in this context is all about the way you use the people you know. The author supplies a meticulous account of 21st‑century networking culture in Silicon Valley, where Altman’s technical talents end up being less important than some of the qualities usually associated with religious leaders — dare-to-dream boldness, self‑effacing geniality and, she writes, “a skill for convincing people that he can see into the future.” Not unlike ChatGPT, Altman molds himself into whatever people want him to be. As Graham once quipped, “You could parachute him into an island full of cannibals and come back in five years and he’d be the king.” During the 2010s Altman joined a group of Silicon Valley investors determined to recover the grand ambitions of earlier tech eras. Tired of start-ups based on incremental tweaks to social‑media platforms or gig-work apps, they sought to return to outer space, unlock nuclear fusion, achieve human-level A.I. and even defeat death itself. The investor Peter Thiel was a major influence, but Altman’s most important collaborator in the field of A.I. was Elon Musk. The early‑2010s Musk who appears in both books is almost unrecognizable to observers who now associate him with black MAGA hats and chain-saw antics. This Musk, the builder of Tesla and SpaceX, believes that creating superintelligent computer systems is “summoning the demon.” He becomes obsessed with the idea that Google will soon develop a true artificial intelligence and allow it to become a force for evil. Altman, dining regularly with Musk, mirrors his anxieties and persuades him to bankroll a more idealistic rival. “If it’s going to happen,” Altman emailed Musk in 2015, “it seems like it would be good for someone other than Google to do it first.” He pitched a “Manhattan Project for A.I.,” a nonprofit to develop a good A.I. in order to save humanity from its evil twin, just as the actual Manhattan Project sought to outrace the Nazis to the atomic bomb. Musk guaranteed $1 billion and even supplied the name OpenAI. Hagey’s book, written with Altman’s cooperation, is less critical, but no hagiography. “The Optimist” lets the reader see how thoroughly Altman outfoxed his patron, leveraging Musk’s paranoia into enormous sums of money while slowly making OpenAI his own. It’s striking that, despite providing much of the initial capital and credibility, Musk ends up with almost nothing to show for his investment. Hao’s 2020 profile of OpenAI, published in the M.I.T. Technology Review, was unflattering and the company declined to cooperate with her for her book. She believes that OpenAI was “begun as a sincere stroke of idealism,” but she wants to make its negative spillover effects evident. Hao does an admirable job of pulling the camera back, telling the stories of workers in Nairobi who earn “starvation wages to filter out violence and hate speech” from ChatGPT, and of visits to communities in Chile where data centers siphon prodigious amounts of water and electricity to run complex hardware. Both books climax with the weekend in November 2023 when Altman was abruptly fired by his company’s board, only to be reinstated days later after staff members and investors revolted. From the outside, many critics saw the coup as a last-ditch effort to stop OpenAI from becoming the very Eye of Sauron it was founded to restrain. Hagey renders this moment as a conventional board mutiny: Directors had tired of Altman’s “duplicity and calamitous aversion to conflict.” One of them, the OpenAI co-founder Ilya Sutskever, recalls that Altman “would tell him one thing, then say another, and act as if the difference was an accident.” Sutskever said he regretted his vote to oust Altman, but after Altman returned to OpenAI, Sutskever left the company. Hao’s version is darker. Relying on a lot of the same sources as Hagey, she presents Altman as a peddler of “many little lies and some big ones,” who helped create “a directionless, chaotic and back-stabbing environment.” In her book, Sutskever comes to see Altman as engaging in what some of his colleagues call “psychological abuse.” Together, these two excellent and deeply reported books form a diptych. On one panel stands Altman as the secular prophet preaching human progress and boundless optimism. Hagey calls Altman a “brilliant deal maker with a need for speed and a love of risk, who believes in technological progress with an almost religious conviction.” On the other panel is Altman the opportunist. He uses idealism as a tool, harnessing the concept of human progress to build an empire the way Europeans once used Christianity to justify conquest. Altman recently told the statistician Nate Silver that if we achieve human-level A.I., “poverty really does just end.” But motives matter. History suggests that some technologies aimed at growth have taken a bad situation and made it worse. The efficiencies of the cotton gin, for instance, saved on labor but made slavery even more lucrative. If the aim is not, in the first place, to help the world, but instead to get bigger — better chips, more data, smarter code — then our problems might just get bigger too. (The New York Times has sued OpenAI and its partner, Microsoft, accusing them of copyright infringement regarding news content related to A.I. systems. OpenAI and Microsoft have denied those claims.) EMPIRE OF AI : Dreams and Nightmares in Sam Altman’s OpenAI | By Karen Hao | Penguin Press | 482 pp. | $32 THE OPTIMIST : Sam Altman, OpenAI, and the Race to Invent the Future | By Keach Hagey | Norton | 367 pp. | $31.99 By Tim Wu",USA,"The New York Times, International edition; New York",2025-06-21,26,1419,2025-06-01,2025.2,Long,61,2,1.4094432699083863,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.7047216349541932,1,0,0,0,3,2.1141649048625792,1,2,1.4094432699083863,1,0.5649319188325923,Positive
"CoreWeave, which provides computing power for A.I., was founded by three Bitcoin enthusiasts. The company is now set to make the first prominent A.I. initial public offering. In 2016, three New York commodities traders -- Michael Intrator, Brian Venturo and Brannin McBee -- fell in love with cryptocurrencies. They were using Bitcoin to bet on pool games and fantasy football and were captivated by its wild swings in price. They soon decided to create more digital currencies through a process called ""mining,"" which requires a lot of computing power. So they logged on to Amazon and ordered two Nvidia graphics processing units, or GPUs, powerful chips that can run calculations and analyze enormous amounts of data. They then stockpiled the components, filling up a garage and a warehouse. In 2017, Mr. Intrator, Mr. Venturo and Mr. McBee started the company Atlantic Crypto to mine cryptocurrencies. When crypto prices cratered two years later, they renamed the business CoreWeave and raised money to buy as many GPUs from distressed crypto miners as possible. Their bet: that the chips would eventually feed the development of artificial intelligence, whose creation also requires substantial computing power. They were right. When OpenAI released the ChatGPT chatbot in 2022 and unleashed an A.I. frenzy, the demand for computing power exploded -- and CoreWeave was in the sweet spot. ""We assumed this was going to happen,"" Mr. Intrator, 55, CoreWeave's chief executive, said in a 2023 interview with The New York Times. ""We were well positioned for this transition."" CoreWeave is now set to find out whether it is in the sweet spot again -- this time with Wall Street. This month, the tech company filed paperwork for a public listing. If its initial public offering of stock goes as planned, it will be the first prominent A.I. start-up to reach the stock market. And it would test investor appetite at a time when the market has slipped into a correction, which could delay an offering. Perhaps most important, CoreWeave's I.P.O. would give investors a direct taste of the A.I. boom. Unlike tech giants such as Nvidia and Google, which have many businesses, CoreWeave is known in industry parlance as a ""pure A.I. play"" because it has focused solely on A.I. chips and sells processing capacity to clients that want to build A.I. tools. ""It would be the first really big tech I.P.O. to hit the markets this year and one that fits squarely into the A.I. narrative that everyone seems to want a piece of,"" said Brianne Lynch, the head of market insight at EquityZen, which helps private companies and their employees sell their stock. While CoreWeave is based far from Silicon Valley in Livingston, N.J., it has tech industry bona fides. Nvidia owns around 4 percent of the start-up and supplies most of its chips. Last week, CoreWeave also announced a deal to sell computing power to OpenAI worth up to $ 12 billion. As part of the agreement, OpenAI will take a $ 350 million stake in CoreWeave when it goes public. The young company, which has raised $ 2.3 billion in venture capital and was last valued in the private markets at $ 19 billion, is expected to go public this month at a valuation of about $ 35 billion, Ms. Lynch said. CoreWeave has been growing rapidly, with revenue reaching $ 1.9 billion last year, compared with $ 229 million a year earlier, according to its financial filings. But it lost $ 863 million last year after spending nearly $ 1 billion to finance the building of new data centers, large facilities that house its A.I. chips. If CoreWeave's public offering goes well, other tech companies could be motivated to follow, potentially leading to an ""I.P.O. parade,"" said Mark Klein, the chief executive of SuRo Capital, a venture capital firm that has invested in CoreWeave. The company aims to raise about $ 4 billion in its public offering, he added. CoreWeave declined to comment ahead of its I.P.O., and Nvidia declined to comment on its investment in the start-up. OpenAI also declined to comment. Mr. Intrator and Mr. Venturo, who is 40, met in 2006 at a New York hedge fund, Natsource, where they both worked. They later created a hedge fund together before founding CoreWeave in 2017 alongside Mr. McBee, now 39. Mr. Intrator became chief executive, Mr. McBee became chief strategy officer and Mr. Venturo chief technology officer. The three men soon amassed Nvidia chips to mine cryptocurrencies. For a time, Mr. Intrator, Mr. Venturo and Mr. McBee stockpiled the chips in a downtown Manhattan office, but they became worried that the intense heat from the components might burn down their building. So they moved the chips to Mr. Venturo's grandfather's garage in New Jersey, and then to a warehouse. By 2018, CoreWeave was pitching investors on a plan to diversify from crypto and into high-end graphics, which need GPUs to render images, said Nic Carter, who invested in the company that year and now runs the crypto investment firm Castle Island Ventures. ""They had this whole plan to go to visual-effects conferences and hand out free credits,"" he said. ""They were going to become the data center of choice for hobbyists and animation and movies."" Mr. Intrator, Mr. Venturo and Mr. McBee approached the business by ""trading GPUs like commodities"" and betting that they could repurpose the chips at the exact moment that crypto mining became unprofitable, Mr. Carter said. Until then, CoreWeave had to be scrappy. Unable to raise much venture capital, Mr. Intrator kept the start-up alive by gathering money from family offices and wealthy commodities traders he and his co-founders were friends with. ""This company could have easily gone out of business many times,"" Mr. Carter said. CoreWeave's big break came in 2021 when the hedge fund Magnetar invested $ 50 million. Then OpenAI released ChatGPT, shocking people with how the chatbot could answer questions, generate essays and compose love poems. Demand for CoreWeave's computing power skyrocketed. In the first five months of 2023, the company signed contracts with A.I. research labs and other customers totaling $ 7 billion, Mr. Intrator said in the interview that year. That April, Nvidia invested $ 100 million in CoreWeave at a $ 2 billion valuation, on top of another $ 200 million from Magnetar. CoreWeave needed to build more data centers, so that August, it secured $ 2.3 billion in debt financing, using its chips as collateral. By last year, its valuation had risen to $ 19 billion. The company now has 32 data centers in the United States and Europe, according to its prospectus, and about 800 employees. Last year, Mr. Venturo became the company's chief strategy officer and Mr. McBee the chief development officer. Along with Mr. Intrator, they have each sold more than $ 150 million of their CoreWeave stock, according to the company's prospectus. Together, Mr. Intrator, Mr. Venturo and Mr. McBee own 30 percent of the company, with a special class of shares giving them around 80 percent of the voting power. CoreWeave's single largest shareholder is Magnetar, which has about a 25 percent stake. CoreWeave faces stiff competition from Amazon, Microsoft and Google, which also provide computing power. And its business is heavily dependent on a single customer -- Microsoft, which generated 60 percent of CoreWeave's revenue last year. On a December podcast, Satya Nadella, Microsoft's chief executive, called the company's contract with CoreWeave a ""one-time thing,"" spurred by a scarcity of A.I. chips after ChatGPT's release. Most of those contracts end in 2029, and Microsoft has invested billions of dollars building its own data centers. (The Times has sued OpenAI and Microsoft, accusing them of copyright infringement of news content related to A.I. systems. OpenAI and Microsoft have denied those claims.) Last week, CoreWeave said it had agreed to buy Weights & Biases, an A.I. software start-up that helps companies manage A.I. tools, which could help diversify its customer base. Even with CoreWeave on the cusp of a Wall Street debut, Mr. Carter said his conversations with Mr. Venturo and Mr. McBee revolved around the same topics as before: sports betting and crypto. ""They're still traders at heart,"" Mr. Carter said. Photograph Michael Intrator is a founder and the chief executive of CoreWeave, an A.I. computing company that is among the first to file paperwork to go public. (PHOTOGRAPH BY CARLOS RODRIGUES/SPORTSFILE FOR WEB SUMMIT VIA GETTY IMAGES) (B5) This article appeared in print on page B1, B5.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-03-21,26,1415,2025-03-01,2025.1,Long,62,0,0,0,0,0,0,1,0.7067137809187278,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,1.4134275618374557,1,4,2.8268551236749113,1,-0.7779792915327534,Negative
"The rise of artificial intelligence has profoundly altered the technology world in recent years, upending how software is created, how people search for information, and how images and videos can be generated -- all with a few prompts to a chatbot. What the technology has yet to do, though, is find a preferred form in a physical, everyday gadget. A.I. largely remains the domain of an app on phones, despite efforts by start-ups and others to move it into devices. Now OpenAI, the world's leading A.I. lab, is taking a crack at that riddle. On Wednesday, Sam Altman, OpenAI's chief executive, said the company was paying $6.5 billion to buy IO, a one-year-old start-up created by Jony Ive, a former top Apple executive who designed the iPhone. The all-stock deal, which effectively unites Silicon Valley royalty, is intended to usher in what the two men call ''a new family of products'' for the age of artificial general intelligence, or A.G.I., which is shorthand for a future technology that achieves human-level intelligence. The deal, which is OpenAI's biggest acquisition, will bring in Mr. Ive and his team of roughly 55 engineers and researchers. LoveFrom will assume creative and design responsibilities across OpenAI and build hardware that helps people better interact with the technology. In a joint interview, Mr. Ive and Mr. Altman declined to say what such devices could look like and how they might work, but they said they hoped to share details next year. Mr. Ive, 58, framed the ambitions as galactic, with the aim of creating ''amazing products that elevate humanity.'' ''We've been waiting for the next big thing for 20 years,'' Mr. Altman, 40, added. ''We want to bring people something beyond the legacy products we've been using for so long.'' Mr. Altman and Mr. Ive are effectively looking beyond an era of smartphones, which have been people's signature personal device since the iPhone debuted in 2007. If the two men succeed -- and it is a very big if -- they could spur what is known as ''ambient computing.'' Rather than typing and taking photographs on smartphones, future devices like pendants or glasses that use A.I. could process the world in real time, fielding questions and analyzing images and sounds in seamless ways. Mr. Altman had invested in Humane, a company that pursued this kind of vision with the creation of an A.I. pin. But the start-up folded not long after its product flopped. In their interview, Mr. Ive expressed some misgivings with the iPhone and said that had motivated him to team up with Mr. Altman. ''I shoulder a lot of the responsibility for what these things have brought us,'' he said, referring to the anxiety and distractions that come with being constantly connected to the computer in your pocket. Mr. Altman echoed the sentiment. ''I don't feel good about my relationship with technology right now,'' he said. ''It feels a lot like being jostled on a crowded street in New York, or being bombarded with notifications and flashing lights in Las Vegas.'' He said the goal was to leverage A.I. to help people make some sense of the noise. As part of the deal, Mr. Ive and his design studio, LoveFrom, will remain independent and continue to work on projects separate from OpenAI. Scott Cannon, Evans Hankey and Tang Tan, who also founded IO with Mr. Ive, will become OpenAI's employees and report to Peter Welinder, a vice president of product, who will oversee the IO division. The acquisition is a significant windfall for Mr. Ive. OpenAI already owned a 23 percent stake in IO as part of an agreement between the two companies at the end of last year, two people with knowledge of the deal said, so it is now paying around $5 billion to fully acquire the start-up. OpenAI separately has a Start-Up Fund that invested in Mr. Ive's start-up last year, the people said. The deal is subject to regulatory approval. (The New York Times has sued OpenAI and its partner, Microsoft, for copyright infringement regarding news content related to A.I. systems. OpenAI and Microsoft have denied the claims.) OpenAI set off the A.I. boom in late 2022 when it released the ChatGPT chatbot. In March, the start-up completed a $40 billion funding that valued it at $300 billion, making it one of the world's most valuable private companies. The fund-raising round was led by the Japanese conglomerate SoftBank. As it has grown, OpenAI has struggled to adopt a new corporate structure. Founded in 2015 as a nonprofit organization, the A.I. lab has been trying to reinvent itself as a for-profit company so it can more easily raise money from investors. If it does not restructure by the end of the year, SoftBank could halve its investment in the company. That makes the billions that OpenAI is paying for Mr. Ive's start-up a steep outlay, especially as the start-up is also unprofitable. Building the technology that powers ChatGPT and other services is enormously expensive, and OpenAI is under pressure to raise revenues. OpenAI expects about $3.7 billion in sales this year and about $11.6 billion next year, according to financial documents reviewed by The Times. The company is also in talks to acquire Windsurf, an A.I.-powered programming tool, for about $3 billion. Asked how OpenAI would find the money to buy IO, Mr. Altman said the press worried about OpenAI's funding and revenues more than the company did. ''We'll be fine,'' he said. ''Thanks for the concern.'' The deal came together after Mr. Ive, a protégé of the Apple founder Steve Jobs who designed the iPod and many other products, became intrigued by A.I. He felt somewhat lost after leaving Apple in 2019, he said, and was eager to find his next act. Two years ago, Charlie Ive, one of his 21-year-old twin sons, told him about ChatGPT, Mr. Ive said. Curious about his son's excitement over the chatbot, Mr. Ive connected with Mr. Altman. They became friends. Mr. Ive said he was so enamored with the technology that he founded IO last year with several peers to conceptualize new hardware products suited to A.I. By early this year, it became clear that he and Mr. Altman wanted to form a partnership to work on a new generation of devices, he said. Mr. Ive said the partnership was being led not by a fiscal imperative but from a place of building products that ''benefit humanity.'' ''I believe everything I've done in my career was leading to this,'' he said. Tripp Mickle contributed reporting. Tripp Mickle contributed reporting. Photograph Jony Ive, a former Apple executive, wants the project to ''benefit humanity.'' (PHOTOGRAPH BY CAROLYN FONG FOR THE NEW YORK TIMES) (B6) This article appeared in print on page B1, B6.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-05-22,39,1129,2025-05-01,2025.2,Long,63,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.8857395925597874,1,0,0,0,1,0.8857395925597874,1,0.4480634980410428,Positive
"""A lot of compute is needed,"" Mr. Musk said in a post about the financing, in which BlackRock, Fidelity and Sequoia participated. The artificial intelligence company founded by Elon Musk, xAI, said on Monday that it had raised $ 6 billion, giving the start-up a major lift as it competes with rivals, including OpenAI. The company said on its website that it would use the money to continue building its infrastructure and accelerate research and development. BlackRock, Fidelity, Sequoia Capital and others participated in the funding round. ""A lot of compute is needed,"" Mr. Musk said of the investment round in a post on X. The fund-raising could value xAI at $ 35 billion to $ 40 billion, up from $ 24 billion earlier this year, The New York Times previously reported. Mr. Musk and xAI did not immediately respond to requests for comment. Mr. Musk is trying to play catch up on A.I. The billionaire, who also leads other companies, including Tesla, X and SpaceX, publicly debuted xAI last year, well after a boom in the A.I. industry spawned dozens of products that could create text, images and even videos. Mr. Musk quickly built what he said would be the world's largest supercomputer in Memphis, which powers Grok, xAI's chatbot. Grok is available to subscribers on X. Mr. Musk helped found OpenAI, but walked away from it in 2018 after disagreements with other co-founders, including Sam Altman, its chief executive. Mr. Musk has sued OpenAI to block it from transforming itself from a nonprofit into a for-profit company, saying that Mr. Altman and another co-founder, Greg Brockman, had breached the company's founding contract by putting commercial interests before the public good. OpenAI has argued that the lawsuit is an attempt to hamstring the company while Mr. Musk builds a rival. (The New York Times has sued OpenAI and Microsoft for copyright infringement of news content related to A.I. systems. OpenAI and Microsoft have denied those claims.) Mr. Musk has argued that A.I. could destroy humanity, but that he has the ability to build it more safely. To jump-start xAI, Mr. Musk has trained its A.I. on data from X. Several investors in Mr. Musk's bid to acquire X in 2022 have also provided funding for the A.I. start-up, including the venture capital firm Andreessen Horowitz and the Saudi Arabian prince Alwaleed bin Talal's investment company, Kingdom Holding. In total, xAI has raised more than $ 12 billion. Illustration This article appeared in print on page B4.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2024-12-25,17,417,2024-12-01,2024.4,Medium,64,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2.398081534772182,1,1,2.398081534772182,1,2,4.796163069544364,1,-0.8171725364901151,Negative
"After the president made A.I. dominance a top priority, tech companies changed course from a meeker approach under the Biden administration. For just over two years, technology leaders at the forefront of developing artificial intelligence had made an unusual request of lawmakers. They wanted Washington to regulate them. The tech executives warned lawmakers that generative A.I., which can produce text and images that mimic human creations, had the potential to disrupt national security and elections, and could eventually eliminate millions of jobs. A.I. could go ""quite wrong,"" Sam Altman, the chief executive of OpenAI, testified in Congress in May 2023. ""We want to work with the government to prevent that from happening."" But since President Trump's election, tech leaders and their companies have changed their tune, and in some cases reversed course, with bold requests of government to stay out of their way, in what has become the most forceful push to advance their products. In recent weeks, Meta, Google, OpenAI and others have asked the Trump administration to block state A.I. laws and to declare that it is legal for them to use copyrighted material to train their A.I. models. They are also lobbying to use federal data to develop the technology, as well as for easier access to energy sources for their computing demands. And they have asked for tax breaks, grants and other incentives. The shift has been enabled by Mr. Trump, who has declared that A.I. is the nation's most valuable weapon to outpace China in advanced technologies. On his first day in office, Mr. Trump signed an executive order to roll back safety testing rules for A.I. used by the government. Two days later, he signed another order, soliciting industry suggestions to create policy to ""sustain and enhance America's global A.I. dominance."" Tech companies ""are really emboldened by the Trump administration, and even issues like safety and responsible A.I. have disappeared completely from their concerns,"" said Laura Caroli, a senior fellow at the Wadhwani AI Center at the Center for Strategic and International Studies, a nonprofit think tank. ""The only thing that counts is establishing U.S. leadership in A.I."" Many A.I. policy experts worry that such unbridled growth could be accompanied by, among other potential problems, the rapid spread of political and health disinformation; discrimination by automated financial, job and housing application screeners; and cyberattacks. The reversal by the tech leaders is stark. In September 2023, more than a dozen of them endorsed A.I. regulation at a summit on Capitol Hill organized by Senator Chuck Schumer, Democrat of New York and the majority leader at the time. At the meeting, Elon Musk warned of ""civilizational risks"" posed by A.I. In the aftermath, the Biden administration started working with the biggest A.I. companies to voluntarily test their systems for safety and security weaknesses and mandated safety standards for the government. States like California introduced legislation to regulate the technology with safety standards. And publishers, authors and actors sued tech companies over their use of copyrighted material to train their A.I. models. (The New York Times has sued OpenAI and its partner, Microsoft, accusing them of copyright infringement regarding news content related to A.I. systems. OpenAI and Microsoft have denied those claims.) But after Mr. Trump won the election in November, tech companies and their leaders immediately ramped up their lobbying. Google, Meta and Microsoft each donated $ 1 million to Mr. Trump's inauguration, as did Mr. Altman and Apple's Tim Cook. Meta's Mark Zuckerberg threw an inauguration party and has met with Mr. Trump numerous times. Mr. Musk, who has his own A.I. company, xAI, has spent nearly every day at the president's side. In turn, Mr. Trump has hailed A.I. announcements, including a plan by OpenAI, Oracle and SoftBank to invest $ 100 billion in A.I. data centers, which are huge buildings full of servers that provide computing power. ""We have to be leaning into the A.I. future with optimism and hope,"" Vice President JD Vance told government officials and tech leaders last week. At an A.I. summit in Paris last month, Mr. Vance also called for ""pro-growth"" A.I. policies, and warned world leaders against ""excessive regulation"" that could ""kill a transformative industry just as it's taking off."" Now tech companies and others affected by A.I. are offering responses to the president's second A.I. executive order, ""Removing Barriers to American Leadership in Artificial Intelligence,"" which mandated development of a pro-growth A.I policy within 180 days. Hundreds of them have filed comments with the National Science Foundation and the Office of Science and Technology Policy to influence that policy. OpenAI filed 15-pages of comments, asking for the federal government to pre-empt states from creating A.I. laws. The San Francisco-based company also invoked DeepSeek, a Chinese chatbot created for a small fraction of the cost of U.S.-developed chatbots, saying it was an important ""gauge of the state of this competition"" with China. If the Chinese developers ""have unfettered access to data and American companies are left without fair use access, the race for A.I. is effectively over,"" OpenAI said, requesting that the U.S. government turn over data to feed into its systems. Many tech companies also argued that their use of copyrighted works for training A.I. models was legal and that the administration should take their side. OpenAI, Google and Meta said they believed they had legal access to copyrighted works like books, films and art for training. Meta, which has its own A.I. model, called Llama, pushed the White House to issue an executive order or other action to ""clarify that the use of publicly available data to train models is unequivocally fair use."" Google, Meta, OpenAI and Microsoft said their use of copyrighted data was legal because the information was transformed in the process of training their models and was not being used to replicate the intellectual property of rights holders. Actors, authors, musicians and publishers have argued that the tech companies should compensate them for obtaining and using their works. Some tech companies have also lobbied the Trump administration to endorse ""open source"" A.I., which essentially makes computer code freely available to be copied, modified and reused. Meta, which owns Facebook, Instagram and WhatsApp, has pushed hardest for a policy recommendation on open sourcing, which other A.I. companies, like Anthropic, have described as increasing the vulnerability to security risks. Meta has said open source technology speeds up A.I. development and can help start-ups catch up with more established companies. Andreessen Horowitz, a Silicon Valley venture capital firm with stakes in dozens of A.I. start-ups, also called for support of open source models, which many of its companies rely on to create A.I. products. And Andreessen Horowitz gave the starkest arguments against new regulations for A.I. Existing laws on safety, consumer protection and civil rights are sufficient, the firm said. ""Do prohibit the harms and punish the bad actors, but do not require developers to jump through onerous regulatory hoops based on speculative fear,"" Andreessen Horowitz said in its comments. Others continued to warn that A.I. needed to be regulated. Civil rights groups called for audits of systems to ensure they do not discriminate against vulnerable populations in housing and employment decisions. Artists and publishers said A.I. companies needed to disclose their use of copyright material and asked the White House to reject the tech industry's arguments that their unauthorized use of intellectual property to train their models was within the bounds of copyright law. The Center for AI Policy, a think tank and lobbying group, called for third-party audits of systems for national security vulnerabilities. ""In any other industry, if a product harms or negatively hurts consumers, that project is defective and the same standards should be applied for A.I.,"" said K.J. Bagchi, vice president of the Center for Civil Rights and Technology, which submitted one of the requests. Photograph In testifying before Congress in May 2023, Sam Altman, the chief executive of OpenAI, warned lawmakers that artificial intelligence could go ""quite wrong."" (PHOTOGRAPH BY HAIYUN JIANG/THE NEW YORK TIMES); Lawmakers, left, took part in an A.I. forum with technology chief executives in September 2023. At the time, the executives wanted Washington to regulate A.I. But since President Trump's election, technology leaders, including Meta's Mark Zuckerberg, right, and their companies have changed their tune. (PHOTOGRAPHS BY HAIYUN JIANG FOR THE NEW YORK TIMES; KENNY HOLSTON/THE NEW YORK TIMES) (B3) This article appeared in print on page B1, B3.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-03-25,38,1404,2025-03-01,2025.1,Long,65,13,9.25925925925926,1,0,0,0,1,0.7122507122507122,1,0,0,0,0,0,0,0,0,0,0,0,0,24,17.094017094017097,1,5,3.561253561253561,1,0,0,0,-1.2451213520013997,Negative
"Companies like OpenAI and Google are running out of the data used to train artificial intelligence systems. Can new methods continue years of rapid progress? Demis Hassabis, one of the most influential artificial intelligence experts in the world, has a warning for the rest of the tech industry: Don't expect chatbots to continue to improve as quickly as they have over the last few years. A.I. researchers have for some time been relying on a fairly simple concept to improve their systems: The more data culled from the internet that they pumped into large language models -- the technology behind chatbots -- the better those systems performed. But Dr. Hassabis, who oversees Google DeepMind, the company's primary A.I. lab, now says that method is running out of steam simply because tech companies are running out of data. ""Everyone in the industry is seeing diminishing returns,"" Dr. Hassabis said this month in an interview with The New York Times as he prepared to accept a Nobel Prize for his work on artificial intelligence. Dr. Hassabis is not the only A.I. expert warning of a slowdown. Interviews with 20 executives and researchers showed a widespread belief that the tech industry is running into a problem that to many was unthinkable just a few years ago: They have used up most of the digital text available on the internet. That problem is starting to surface even as billions of dollars continue to be poured into A.I. development. On Tuesday, Databricks, an A.I. data company, said it was closing in on $ 10 billion in funding -- the largest-ever private funding round for a start-up. And the biggest companies in tech are signaling that they have no plans to slow down their spending on the giant data centers that run A.I. systems. Not everyone in the A.I. world is concerned. Some, like OpenAI's chief executive, Sam Altman, say that progress will continue at the same pace, albeit with some twists on old techniques. Dario Amodei, the chief executive of the A.I. start-up Anthropic, and Jensen Huang, Nvidia's chief executive, are also bullish. (The New York Times has sued OpenAI, claiming copyright infringement of news content related to A.I. systems. OpenAI has denied the claims.) The roots of the debate trace back to 2020 when Jared Kaplan, a theoretical physicist at Johns Hopkins University, published a research paper showing that large language models steadily grew more powerful and lifelike as they analyzed more data. Researchers called Dr. Kaplan's findings ""the Scaling Laws."" Just as students learn more by reading more books, A.I. systems improved as they ingested increasingly large amounts of digital text culled from the internet, including news articles, chat logs and computer programs. Seeing the raw power of this phenomenon, companies like OpenAI, Google and Meta raced to get their hands on as much internet data as possible, cutting corners, ignoring corporate policies and even debating whether they should skirt the law, according to an examination this year by The Times. It was the modern equivalent of Moore's Law, the oft-quoted maxim coined in the 1960s by the Intel co-founder Gordon Moore. He showed that the number of transistors on a silicon chip doubled every two years or so, steadily increasing the power of the world's computers. Moore's Law held up for 40 years. But eventually, it started to slow. The problem is, neither the Scaling Laws nor Moore's Law is an immutable law of nature. They're simply smart observations. One held up for decades. The others may have a much shorter shelf life. Google and Dr. Kaplan's new employer, Anthropic, cannot just throw more text at their A.I. systems, because there is little text left to throw. ""There were extraordinary returns over the last three or four years as the Scaling Laws were getting going,"" Dr. Hassabis said. ""But we are no longer getting the same progress."" Dr. Hassabis said that existing techniques would continue to improve A.I. in some ways. But he said he believed that entirely new ideas were needed to reach the goal that Google and many others were chasing: a machine that can match the power of the human brain. Ilya Sutskever, who was instrumental in pushing the industry to think big as a researcher at both Google and OpenAI before leaving OpenAI to create a start-up this spring, made the same point during a speech last week. ""We've achieved peak data, and there'll be no more,"" he said. ""We have to deal with the data that we have. There's only one internet."" Dr. Hassabis and others are exploring a different approach. They are developing ways for large language models to learn from their own trial and error. By working through various math problems, for instance, language models can learn which methods lead to the right answer and which do not. In essence, the models train on data that they themselves generate. Researchers call this ""synthetic data."" OpenAI recently released a new system, called OpenAI o1, that was built this way. But the method works only in areas like math and computing programming, where there is a firm distinction between right and wrong. Even in these areas, A.I. systems have a way of making mistakes and making things up. That can hamper efforts to build A.I. ""agents"" that can write their own computer programs and take actions on behalf of internet users, which experts see as one of A.I.'s most important skills. Sorting through the wider expanses of human knowledge is even more difficult. ""These methods only work in areas where things are empirically true, like math and science,"" said Dylan Patel, chief analyst for the research firm SemiAnalysis, who closely follows the rise of A.I. technologies. ""The humanities and the arts, moral and philosophical problems are much more difficult."" People like Mr. Altman of OpenAI say that these new techniques will continue to push the technology ahead. But if progress reaches a plateau, the implications could be far-reaching, even for Nvidia, which has become one of the most valuable companies in the world thanks to the A.I. boom. During a call with analysts last month, Mr. Huang, Nvidia's chief executive, was asked how the company was helping customers work through a potential slowdown and what the repercussions might be for its business. He said that evidence showed there were still gains being made, but that businesses were also testing new processes and techniques on A.I. chips. ""As a result of that, the demand for our infrastructure is really great,"" Mr. Huang said. Though he is confident about Nvidia's prospects, some of the company's biggest customers acknowledge that they must prepare for the possibility that A.I. will not advance as quickly as expected. ""We have had to grapple with this. Is this thing real or not?"" said Rachel Peterson, vice president of data centers at Meta. ""It is a great question because of all the dollars that are being thrown into this across the board."" Photograph (PHOTOGRAPH BY ALASTAIR GRANT/ASSOCIATED PRESS) (B3) This article appeared in print on page B1, B3.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2024-12-26,23,1173,2024-12-01,2024.4,Long,66,0,0,0,0,0,0,1,0.8525149190110827,1,0,0,0,0,0,0,0,0,0,0,0,0,8,6.820119352088661,1,6,5.115089514066496,1,0,0,0,0.15659727352850397,Neutral
"The spy agency is trying to give its teams better tools and make it easier for the private sector to develop technology for their secretive work. Understanding leaders around the world is one of the C.I.A.’s most important jobs. Teams of analysts comb through intelligence collected by spies and publicly available information to create profiles of leaders that can predict behaviors. A chatbot powered by artificial intelligence now helps do that work. Over the last two years, the Central Intelligence Agency has developed a tool that allows analysts to talk to virtual versions of foreign presidents and prime ministers, who answer back. “It is a fantastic example of an app that we were able to rapidly deploy and get out to production in a cheaper, faster fashion,” said Nand Mulchandani, the C.I.A.’s chief technology officer. The chatbot is part of the spy agency’s drive to improve the tools available to C.I.A. analysts and its officers in the field, and to better understand adversaries’ technical advances. Core to the effort is to make it easier for companies to work with the most secretive agency. William J. Burns, the C.I.A. director for the past four years, prioritized improving the agency’s technology and understanding of how it is used. Incoming Trump administration officials say they plan to build on those initiatives, not tear them down. In his confirmation hearing, John Ratcliffe, President-elect Donald J. Trump’s choice to lead the C.I.A., said the agency had “struggled to keep pace” as technological innovation had shifted from the public to private sectors. But Mr. Ratcliffe spoke in positive terms about Mr. Burns’s efforts and said he would expand them because “the nation who wins the race in the emerging technologies of today will dominate the world of tomorrow.” The C.I.A. has long used digital tools, spy gadgets and even artificial intelligence. But with the development of new forms of A.I., including the large language models that power chat bots, the agency has stepped up its investments. Making better use of A.I., Mr. Burns said, is crucial to the U.S. competition with China. And better A.I. models have helped the agency’s analysts “digest the avalanche of open-source information out there,” he said. The new tools have also helped analysts process clandestinely acquired information, Mr. Burns said. New technologies developed by the agency are helping spies navigate cities in authoritarian countries where governments use A.I.-powered cameras to conduct constant surveillance on their population and foreign spies. “We’re making decent strides,” Mr. Burns said. “But I’d be the first to argue we’ve got to go faster and further.” Shortly after Mr. Burns took up his job, he picked Dawn Meyerriecks, who led the agency’s directorate of science and technology from 2014 to 2021, to review the C.I.A.’s efforts. The review pushed for something of a culture change. Ms. Meyerriecks said the C.I.A. had long believed that it could do everything itself. The agency had to make an adjustment and embrace the idea that some of the technology it needed had been developed by the commercial sector and was designed to keep information secure. “There was really no reason that the C.I.A. couldn’t adopt and adapt commercial technology,” Ms. Meyerriecks said. Under Mr. Burns, the agency created a technology-focused mission center to better understand the technology being used by China and other adversaries. And it hired Mr. Mulchandani, who helped found a series of successful start-ups before joining the Pentagon’s artificial intelligence center, as the agency’s first chief technology officer. His mandate over the last two and a half years was to make it easier for private companies that had developed new technologies to be able to sell those applications and tools to the C.I.A. The conundrums facing anyone wanting to do business with the agency are twofold. First, its needs are classified. How can you sell something to America’s spies if you do not know what they are doing or what they need? Second, there is the bureaucracy. In his work space, Mr. Mulchandani unfurled a six-foot-long chart detailing the layers of approvals and other steps to get a contract with the agency. Each of the rules was put in place for a reason — for example, to address a problem with a contract, or something else going wrong on a project. But the cumulative result is a set of regulations that has made it difficult for companies to work with government. The C.I.A. is reviewing, and trying to prune, those rules. But it is also trying to be more open with technology companies about what it needs. “The more we share about how we employ technology, how we procure technology, what we’re going to do with it, will make companies want to work with us and want to team with us more,” said Juliane Gallina, who leads the directorate of digital innovation for the C.I.A. Ms. Gallina says the agency has taken the step to declassify some material to “expose a little bit” of the problem it is trying to overcome, so tech firms can compete for agency contracts. The C.I.A. has long recognized the technology problem. A quarter century ago the agency helped found In-Q-Tel, a nonprofit venture capital fund, to help foster companies that could offer new technologies to the intelligence community. Its successes include helping expand firms like Palantir, a secretive data analytics company, and the company that became Google Earth. But the C.I.A. also wants more established firms, or firms with other venture capital backing, to offer their ideas to the agency. That is where the bureaucratic clutter cutting comes in, along with efforts to change at least parts of the spy agency’s culture. Many offices in the C.I.A. are warrens of cubicles or have clusters of desks for assistants. When Mr. Mulchandani started, he was given a space on the same floor as the C.I.A.’s top leadership, but he was not pleased. Mr. Mulchandani recalled that the agency officer giving him the tour asked, “What is wrong?” He answered, “Everything.” He was turned off by the small offices, the lack of natural light and the closetlike rooms for viewing the most classified of material. He ordered a renovation. The old offices were replaced by different spaces with movable desks for meetings and exchanging ideas. The goal was to make a space that echoed the workplaces of Silicon Valley — and signal to visiting entrepreneurs that the agency was ready to change. “The space is going to drive the culture, a culture of talking,” Mr. Mulchandani said. “A slice of Silicon Valley on the seventh floor.” Whether the cultural changes will stick is an open question. And adjusting the rules and cutting red tape is the work of years not months. But Mr. Mulchandani and the agency’s departing leadership are hopeful. “Nobody will deny the fact that like tech is literally the single most disruptive force in the world today,” Mr. Mulchandani said. “And government and our own work is going to be completely dependent on tech and disrupted by tech. I can’t speak for the leadership coming in, but I don’t have any doubt in my mind that this is super top on their list.” By Julian E. Barnes",USA,"The New York Times, International edition; New York",2025-01-28,18,1196,2025-01-01,2025.1,Long,67,0,0,0,1,0.8361204013377926,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.8361204013377926,1,4,3.3444816053511706,1,0,0,0,0.7120054205233988,Positive
"Enlarge this image. Inside China, it was called the tipping point for the global technological rivalry with the United States and the “darkest hour” in Silicon Valley, evoking Winston Churchill. It was possibly a breakthrough that could change the country’s destiny. The news that the Chinese startup DeepSeek can build artificial intelligence models that are as good as OpenAI’s, and at a fraction of the cost, tanked the stock market Monday and sent Silicon Valley into a panic. The claim about DeepSeek’s success was viewed in China as a shot in the arm for a discouraged tech industry and a public that’s suffering through a stagnating economy. On social media posts and state news outlets, DeepSeek was nothing less than a testament to the country’s ability to innovate, especially when facing efforts by the United States to limit China’s access to the most advanced technologies. “A nation like China, which is equipped with substantial technological resources, cannot truly be suppressed,” wrote Hu Xijin, a retired editor-in-chief of the Communist Party tabloid Global Times. “U.S. sanctions in one area will only spur more comprehensive and resilient progress in China, potentially leading to breakthroughs that outpace the U.S.” U.S. semiconductor policies toward China “may ultimately backfire on the U.S,” he wrote. On Monday evening, 4 out of the 10 most popular topics on the social media platform Weibo were related to DeepSeek. “DeepSeek, keep up the momentum!” a Weibo user in Beijing wrote. “The nation must protect the founder of DeepSeek at all costs! Seriously!” wrote another user in Shanghai who usually posts about entertainment news. Even a hashtag about the DeepSeek CEO, Liang Wenfeng, visiting his hometown in southern Guangdong province for the Lunar New Year, which falls on Wednesday, was a hot topic on Weibo. It had more than 50 million views. Much of the outpouring of attention emphasized the U.S.-China tech rivalry. The assumption that the United States would lead the next wave of the technological revolution was now open to challenge, Li Chengdong, an e-commerce investor, wrote on his WeChat timeline. Fancaiju, a business blog on WeChat, had a post saying that DeepSeek had burst the U.S. stock bubble in one fell swoop — a more significant strike than when George Soros bet against the British pound in 1992. The DeepSeek breakthrough had turned the $100 billion AI initiative known as Stargate that President Donald Trump announced last week into “Interstellar Graveyard,” said a post on Fancaiju. The commentariat took immense pride that DeepSeek was stocked with talented Chinese technologists educated in China. DeepSeek dispelled the myth of the dominance of American AI talent and demystified companies like OpenAI, said Tom Zhang, a human resources expert who has worked at several big tech companies in Silicon Valley. “A group of ‘homegrown Ph.D. graduates’ from Tsinghua and Peking University outshines their counterparts from Stanford and MIT,” he wrote on his WeChat timeline. Entrepreneurs and investors said that DeepSeek demonstrated that China’s AI sector had an edge in innovation. They also said they believed that the U.S. government’s export restrictions on specialized chips from Silicon Valley tech giant Nvidia forced Chinese companies to be more efficient. That was not what Washington had intended, people in China said, but that was what happened. DeepSeek trained its AI chatbot with 2,000 specialized Nvidia chips, compared with as many as the 16,000 chips used by leading U.S. counterparts. It’s also not the only Chinese company to prove the efficiency of its engineering: 01.ai, a startup founded by Kaifu Lee, a Beijing investor and entrepreneur, trained its AI models with computing power that cost about $3 million, the company said, compared with the $80 to $100 million OpenAI has tapped. “In my book AI Superpowers, I predicted that US will lead breakthroughs, but China will be better and faster in engineering,” Lee, who studied artificial intelligence at Carnegie Mellon in the 1980s, wrote on the social media platform X on Sunday. “With the recent DeepSeek releases, I feel vindicated.” Among the most popular articles on the Chinese internet were two interviews of Liang, the reclusive CEO, with a tech blog. In the interviews, Liang, who founded a quantitative stock trading firm called High-Flyer after graduating with a master’s degree in artificial intelligence, came across as a geeky billionaire full of idealism and optimism. He started DeepSeek as a side project in 2023 because he wanted to explore the limits of AI, he said. Liang said he believed that innovation was, first and foremost, a matter of belief. “Why is Silicon Valley so innovative? It starts with daring to try,” he said. Liang noted that when OpenAI’s ChatGPT came out, China was suffering from a lack of confidence to pursue such innovation. “From investors to major tech companies, many felt the gap was too wide,” he said. As China’s economy develops, he said, China should gradually become a contributor to tech innovation, rather than a follower. He said he believed that China’s economic slowdown wasn’t necessarily a bad thing because it could force company founders to be driven less by financial success. “When many people realize that making quick money in the past was likely due to the luck of the times,” Liang said, “they will become more willing to focus on genuine innovation.” Liang told the blog that he had hired mostly young graduates or even graduate students with little work experience. Every team member who worked on an AI model that was released last spring graduated from a Chinese university, he said. “The top 50 talents might not currently be in China, but perhaps we can cultivate such talent ourselves,” he said, a quote that has been reposted many times. People on social media portrayed DeepSeek employees as geniuses, posting their names and citing their educational background and academic papers. Liang, born in 1985, didn’t attract much public attention until last week when he joined a group of businesspeople and academics for a meeting with Li Qiang, China’s premier. The meeting was a sign that Liang had risen to the top, but it could also put him in an awkward position, tech executives said. The relationship between Chinese entrepreneurs and the government has been tricky after Beijing’s crackdown on the tech sector in recent years. The government wants the companies to help make China a tech power less reliant on the United States. But it’s also wary of the companies’ influence. The crackdown crushed the promise of the Chinese internet. A Hong Kong investor told me that he would not invest in Chinese internet stocks as long as Xi Jinping, the Chinese leader, was in office — even with the DeepSeek breakthrough. Xi is known for his dislike of the internet sector. Business executives hope the government can refrain from interfering with DeepSeek. “As is often the case with this government,” Zhang Fuyu, an entrepreneur, wrote on WeChat, “any promising company gets incorporated into a national strategy — receiving funding and resources but being subjected to state directives — or is directly placed under control, then DeepSeek’s future prospects may be entirely stifled.” Under government regulations, AI models that serve consumers are subject to censorship rules. Many DeepSeek users posted photos and videos of its chatbot censoring topics including Xi, the war in Ukraine, the Cultural Revolution and the Tiananmen Square massacre. “If DeepSeek is truly as remarkable as claimed, so impactful that it shakes the U.S. stock market, yet it remains confined to being just an AI model with Chinese socialist characteristics,” a journalist using the handle Xiaoming wrote on her Threads account, “then that would be truly tragic.” This article originally appeared in The New York Times.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-01-29,30,1276,2025-01-01,2025.1,Long,68,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,1.567398119122257,1,11,8.620689655172413,1,1,0.7836990595611285,1,0.7795366009883646,Positive
"Enlarge this image. SAN FRANCISCO — Silicon Valley’s artificial intelligence frenzy has found a new gear. Two and a half years after OpenAI set off the artificial intelligence race with the release of the chatbot ChatGPT, tech companies are accelerating their AI spending, pumping hundreds of billions of dollars into their frantic effort to create systems that can mimic or even exceed the abilities of the human brain. The tech industry’s giants are building data centers that can cost more than $100 billion and will consume more electricity than 1 million American homes. Salaries for AI experts are jumping as Meta offers signing bonuses to AI researchers that top $100 million. And venture capitalists are dialing up their spending. U.S. investment in AI companies rose to $65 billion in the first quarter, up 33% from the previous quarter and up 550% from the quarter before ChatGPT came out in 2022, according to data from PitchBook, which tracks the industry. “Everyone is deeply afraid of being left behind,” said Chris V. Nicholson, an investor with the venture capital firm Page One Ventures who focuses on AI technologies. This astonishing spending, critics argue, comes with a huge risk. AI is arguably more expensive than anything the tech industry has tried to build, and there is no guarantee it will live up to its potential. But the bigger risk, many executives believe, is not spending enough to keep pace with rivals. “The thinking from the big CEOs is that they can’t afford to be wrong by doing too little, but they can afford to be wrong by doing too much,” said Jordan Jacobs, a partner with the venture capital firm Radical Ventures. The biggest spending is for the data centers. Meta, Microsoft, Amazon and Google have told investors that they expect to spend a combined $320 billion on infrastructure costs this year. Much of that will go toward building new data centers — more than twice what they spent two years ago. As OpenAI and its partners build a roughly $60 billion data center complex for AI in Texas and another in the Middle East, Meta is erecting a facility in Louisiana that will be twice as large. Amazon is going even bigger with a new campus in Indiana. Amazon’s partner, the AI startup Anthropic, says it could eventually use all 30 of the data centers on this 1,200-acre campus to train a single AI system. Some experts question whether companies like Anthropic will continue to improve their AI systems at the rapid rate they have maintained over the last few years. But Amazon says that even if the progress stops, it will use those 30 data centers to deliver AI systems to customers. These companies are spending so much on data centers, they see no problem with dropping several billions more to buy a startup or millions on a world-class AI researcher. In 2013, Google shocked Silicon Valley when it paid $44 million for just three researchers. Today, that seems like table stakes. Meta just invested $14.3 billion in Scale AI, a startup that helps collect and organize the enormous amounts of digital data needed to train AI systems. In return, Meta landed Scale AI’s young chief executive, Alexandr Wang, who is considered an up-and-coming deal maker in the AI world. Meta was not the first big technology company to make such an unusual deal. Google, Microsoft and Amazon have also been investing hundreds of millions — or even billions — in startups just for the right to hire their employees and use their technology. In essence, they bought everything but the startups. “Companies are acquiring other companies not necessarily for their products or their services or their revenues but just for their talent,” said Dimitri Zabelin, an emerging-technology analyst at PitchBook. The Scale AI investment was part of an effort by Mark Zuckerberg, Meta’s chief executive, to start an AI research lab dedicated to the creation of superintelligence, a hypothetical technology that would be more powerful than the brain. Zuckerberg has been offering compensation packages worth as much as $100 million a person. He and his company made more than 45 offers to researchers at OpenAI alone, according to a person familiar with these approaches. One Silicon Valley giant, Apple, has been more cautious about chatbots. But as the AI race escalates, Apple is also scrambling for talent. The company has had internal discussions about buying the AI startup Perplexity, according to a person familiar with those conversations. Perplexity is valued at $14 billion. “Apple seems to be sitting on its hands. But I am sure they will surprise us before too long,” said Matt Murphy, a partner at the venture firm Menlo Ventures. An Apple spokesperson did not respond to a request for comment. But as venture firms double down on their deal making, there is less appetite for investing in general AI systems designed to do everything, because that work is dominated by established companies like OpenAI and Google. Instead, they are starting to focus on AI that does specific tasks, like Ribbon, a company that does AI for job interviews, and Eleos Health, which creates AI to record and summarize doctor visits. Tech companies acknowledge that they may be overestimating AI’s potential. But even if the technology falls short, many executives and investors believe, the investments they’re making now will be worth it. “Christopher Columbus thought he was headed to the Orient, and he ended up in the Caribbean,” said Nicholson of Page One Ventures. “He did not get to where he thought he was going, but he still got to a place that was highly valuable.” This article originally appeared in The New York Times.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2025-06-29,27,948,2025-06-01,2025.2,Long,69,2,2.109704641350211,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.0548523206751055,1,0,0,0,1,1.0548523206751055,1,2,2.109704641350211,1,-0.3134943321244443,Neutral
"Mustafa Suleyman remembers the epochal moment he grasped artificial intelligence's potential. It was 2016 -- Paleolithic times by A.I. standards -- and DeepMind, the company he had co-founded that was acquired by Google in 2014, had pitted its A.I. machine, AlphaGo, against a world champion of Go, the confoundingly difficult strategy game. AlphaGo zipped through thousands of permutations, making fast work of the hapless human. Stunned, Suleyman realized the machine had ""seemingly superhuman insights,"" he says in his book on A.I., ""The Coming Wave."" The result is no longer stunning -- but the implications are. Little more than a year after OpenAI's ChatGPT software helped bring generative A.I. into the public consciousness, companies, investors and regulators are grappling with how to shape the very technology designed to outsmart them. The exact risks of the technology are still being debated, and the companies that will lead it are yet to be determined. But one point of agreement: A.I. is transformative. ""The level of innovation is very hard for people to imagine,"" said Vinod Khosla, founder of the Silicon Valley venture capital firm Khosla Ventures, which was one of the first investors in OpenAI. ""Pick an area: books, movies, music, products, oncology. It just doesn't stop."" If 2023 was the year the world woke up to A.I., 2024 might be the year in which its legal and technical limits will be tested, and perhaps breached. DealBook spoke with A.I. experts about the real-world effects of this shift and what to expect next year. Judges and lawmakers will increasingly weigh in. The flood of A.I. regulations in recent months is likely to come under scrutiny. That includes President Biden's executive order in October, which, if Congress ratifies, could compel companies to ensure that their A.I. systems cannot be used to make biological or nuclear weapons; embed watermarks on A.I.-generated content; and to disclose foreign clients to the government. At the A.I. Safety Summit in Britain in November, 28 countries, including China -- though not Russia -- agreed to collaborate to prevent ""catastrophic risks."" And in marathon negotiations in December, the E.U. drafted one of the world's first comprehensive attempts to limit the use of artificial intelligence, which, among other provisions, restricts facial recognition and deep fakes and defines how businesses can use A.I. The final text is due out in early 2024, and the bloc's 27 member countries hope to approve it before European Parliament elections in June. With that, Europe might effectively create global A.I. rules, requiring any company that does business in its market, of 450 million people, to cooperate. ""It makes life tough for innovators,"" said Matt Clifford, who helped organize the A.I. summit in Britain. ""They have to think about complying with a very long list of things people in Brussels are worried about."" There are plenty of concerns, including about A.I.'s potential to replace large numbers of jobs and to reinforce existing racial biases. Some fear overloading A.I. businesses with regulations. Clifford believes existing fraud and consumer-protection laws make some portions of Europe's legislation, the A.I. Act, redundant. But the E.U.'s lead architect, Dragos Tudorache, said that Europe ""wasn't aiming to be global regulators,"" and that he maintained close dialogue with members of the U.S. Congress during the negotiations. ""I am convinced we have to stay in sync as much as possible,"" he said. Governments have good reason to address A.I.: Even simple tools can serve dark purposes. ""The microphone enabled both the Nuremberg rallies and the Beatles,"" wrote Suleyman, who is now the chief executive of Inflection AI, a start-up he co-founded last year with Reid Hoffman, a co-founder of LinkedIn. He fears that A.I. could become ""uncontained and uncontainable"" once it outsmarts humans. ""Homo technologicus could end up being threatened by its own creation."" A.I. capabilities will soar. It's hard to know when that tipping point might arrive. Jensen Huang, the co-founder and chief executive of Nvidia, whose dominance of A.I. chips has seen its share price more than triple since Jan. 1, told the DealBook Summit in late November that ""there's a whole bunch of things that we can't do yet."" Khosla believes the key A.I. breakthrough in 2024 will be ""reasoning,"" allowing machines to produce far more accurate results, and that in 2025, ""A.I. will win in reasoning against intelligent members of the community."" A.I. machines will be steadily more capable of working through several logical steps, and performing probabilistic thinking, such as identifying a disease based on specific data, Khosla said. Exponential growth in computational power, which hugely increases the capability of A.I. machines, factors into those predictions. ""In 2024, it will be between 10 and 100 times more than current-day models,"" Clifford said. ""We don't actually know what kind of innovations that's going to result in."" One new tool could be generative audio that allows users to deliver speeches in, say, Biden's voice or to generate rap songs, opera or Beethoven's nonexistent 10th symphony. DeepMind and YouTube have partnered with musicians to create A.I. tools allowing artists to insert instruments, transform musical styles or compose a melody from scratch. Billions in investments will be needed. None of this will come cheap, and the question now is which companies will be able to build truly sustainable A.I. businesses. Of 175,072 A.I. patents filed between 2012 and 2022, more than half were filed in the last three years, according to Deutsche Bank. In 2024 and 2025, the bank predicts sharp increases in companies using A.I. for human resources, marketing, sales and product development. That is already happening: Legal firms, for example, have begun using A.I.-generated contracts, cutting out hours of work for lawyers. ""The time is ripe for an explosion of A.I. innovation,"" it predicted last May. As those innovations roll out, fund-raising has ramped up. The French A.I. start-up Mistral AI-- considered a European contender to OpenAI -- raised more than half a billion dollars in 2023. More than $ 200 million came from the Silicon Valley venture capital giant Andreessen Horowitz in a funding round that valued Mistral, just seven months old, at $ 2 billion. But that might not be enough to create a general-purpose A.I. system of the kind that powers ChatGPT and that Mistral has in mind. ""It's becoming clear the vast sums of money you need to be competitive,"" Clifford said. ""If you want to build a general-purpose model, it may be that the amount of capital needed is so great, it makes it very tricky for traditional venture capital."" The story could be different for A.I. tools that serve a specific purpose, a category that spawned hundreds of start-ups in 2023. After a sharp downturn last year, A.I. venture funding is rising fast, with most invested in U.S. companies. Khosla said that this year he had backed 30 A.I. start-ups, including in India, Japan, Britain and Spain, companies which he said ""are not afraid of the Big Tech guy."" He expects A.I. funding to continue rising through at least 2024. ""Every country wants to be in the game,"" he said. ""That will accelerate the money flow, and the number of start-ups will keep accelerating."" Illustration This article appeared in print on page B4.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2023-12-30,23,1196,2023-12-01,2023.4,Long,70,5,4.180602006688963,1,0,0,0,1,0.8361204013377926,1,0,0,0,1,0.8361204013377926,1,1,0.8361204013377926,1,0,0,0,8,6.688963210702341,1,5,4.180602006688963,1,0,0,0,0.09858333907801169,Neutral
"Inside a curved glass building next to the Golden State Warriors' arena in San Francisco, eight cans of Spam with tiny arms whirred to life, tapping out artificial-intelligence-generated word slop on miniature keyboards. They were part of the Misalignment A.I. Museum, a gallery dedicated to A.I.-inspired art. Across town, in the basement of a Lower Haight boutique, a group of tech workers delivered stand-up comedy sets about programming languages, ChatGPT and Nvidia's stock price for Artificially Unintelligent, a tech-themed comedy show. And a month earlier, on a foggy summer night in San Francisco's Glen Park neighborhood, a group of tech workers gathered at a midcentury house being used as a start-up office for a reading of ""Doomers,"" a new, ripped-from-the-headlines play about the weekend that Sam Altman, the chief executive of the start-up OpenAI, was briefly fired. A.I. is providing the art and entertainment worlds with plenty to fear, from potential copyright violations on a global scale to the loss of jobs taken by a soulless machine. But A.I. is also quickly becoming fodder for the creative community. The technology has long been a staple of science fiction, but now, two years into the boom kicked off by OpenAI's ChatGPT, the issues raised by those movies and books all feel a little more real. More artists, playwrights and comedians are finding inspiration in the A.I. technology that's currently available: its ethical quandaries, its impact, its risks, its absurdities and even its executives. Audiences are eating it up. ""Doomers"" is set to have an official run early next year in New York and San Francisco, after raising funding via Kickstarter. The Misalignment A.I. Museum, which started in 2023 as an eight-week pop-up, is set to move into a larger, permanent space in San Francisco's Mission Bay neighborhood and reopen in February. And Artificially Unintelligent has become a monthly event and is seeking to expand into other cities. Some of the creative work about A.I. is coming from tech industry insiders laughing at the hype or raising alarms about the dangers. Some of it is coming from outsiders fascinated by the industry's intrigue. A.I. has become a meaty topic for artists to dissect, invoke and mock even as they fight its use as a tool in their fields. In October, more than 10,000 actors, authors and musicians signed a letter opposing the use of their works to train A.I. systems. At least 10 groups, including voice actors, the Recording Industry Association of America and The New York Times, have sued A.I. companies, claiming the technology unlawfully used copyrighted work without permission. And many artists, from graphic designers to movie stars, have expressed concern that the A.I. could soon replace them. ""It's definitely a big topic that's affecting us all, and we should have everyone who is affected by it be part of the conversation,"" said Audrey Kim, the museum's founder and curator. From Misalignment to Marie Antoinette Ms. Kim has been thinking about A.I. for at least a decade, starting when she worked in operations at Cruise, the autonomous vehicle company now owned by General Motors. Her colleagues would have lunchtime debates about the role of A.I. in society. In recent years, as that debate became more mainstream, Ms. Kim decided that art could be a way to bridge the gap between the people building A.I. and the rest of us. Initially her museum, which is a nonprofit supported by donations, focused entirely on addressing the risks of A.I. But through discussions with people working in tech, Ms. Kim, realized there was no consensus among techies on what A.I.'s risks to humanity were, or even a universal understanding about how it worked. So she shifted to education and asking the big questions. One exhibit is a phone booth made for conversations with an A.I. version of the television legend Fred Rogers. The exhibit breaks down all the different pieces of technology that were used to create it, including the speech-to-text converter that interprets the visitor's voice, the A.I. system -- called a large language model -- that creates the Fred Rogers-like personality and the text-to-speech model that simulates his voice, ""so it's not just like this nebulous, vague A.I.,"" Ms. Kim said. A sculpture in the shape of two humans embracing is made of paper clips, evoking a popular theory of A.I.-induced doom. (Told to make paper clips, an A.I. model could decide that humans get in the way of making more paper clips and kill them.) ""I don't think it's going to happen,"" Ms. Kim said of the paper clip apocalypse while giving a tour of the museum, ""but one of the main dangers of A.I. is not having a stop valve."" A piece called ""Sonosynthesis"" features a grand piano that plays A.I.-generated music in response to close-up images of bacteria. It's designed to provoke discussions about who should get the rights to the music: the researchers who built the A.I., the artists who created the music used to train the A.I. or the scientists who grew the bacteria that inspired the music. Many of the pieces, like an A.I. palm reader or a Broomba (a broom on a Roomba vacuum), are playful. A clown puppet that cries for help in the visitor's voice is terrifying. Other exhibits are purely aesthetic. Ms. Kim unfurled a pair of seven-foot neon pink and blue tapestries called ""Marie Antoinette After the Singularity,"" designed using A.I. tools by the musician Grimes, who has encouraged fans to make A.I. music using her voice. To create the pieces and advise on the placement of the crests, Ms. Kim contracted Factum Arte, a group in Madrid whose nonprofit arm worked on one of Henry VIII's tapestries. Ms. Kim, who embodies the museum's winking, whimsical spirit on tours, considers herself a tech optimist. ""The future is not set,"" she said. ""It's what we're doing today that's going to make that or break that."" 'Millennials with a ton of power' ""Doomers"" takes a different view. Matthew Gasda said he had been inspired to write the play after following Mr. Altman's firing on social media last year. The abrupt ouster set off a public debate over potential dangers of OpenAI's technology and whether Mr. Altman was ignoring them. While researching those conversations, Mr. Gasda, 35, decided that the people building this new technology were not any more qualified to have debates about its role in society than anyone else. ""It's just millennials with a ton of power,"" he said. Mr. Gasda's previous plays were similarly of the moment. ""Zoomers"" was about young people in New York. ""Dimes Square"" was about the city's postpandemic downtown scene. He often stages casual readings of drafts of his scripts, then collects audience feedback and adapts. He told the audience in San Francisco that this process might be familiar. ""You all know about beta testing and releasing products that don't work,"" he said to laughter. That version of ""Doomers"" hit on tropes familiar to anyone who has followed tech culture in recent years: polycules, Waymo, p(doom), ketamine. The characters, based on Mr. Altman and his peers at OpenAI, had heated ethical debates about benevolence and abundance. At one point, Mr. Altman's character (called Seth in the play) grabs a knife and demands that someone stab him because doing so would lower the probability of A.I.-induced doom. Some of the early viewers worked at Anthropic, a rival to OpenAI, and they agreed with the play's message that A.I. poses a threat to the world. Mr. Gasda said the wave of resignations at OpenAI in recent months further validated his thesis. ""I told the actors, 'I think we're onto something here,""' he said. Court jesters At Artificially Unintelligent, the threat from A.I. was not quite as dire. Onstage, the host, Neal Patel, surveyed the audience of 120 about their roles in the industry, cheering on the ""value-add engineers"" and poking fun at the people with ""nontechnical fake email jobs."" To the founders, he asked, ""Why are you here and not building?"" He joked that A.I. would soon take everyone's jobs. Mr. Patel, 24, started his comedy group, Not-So-Daily Stand Up, in part because he and his fellow techie comedians kept hearing other comedians make jokes about technology that misunderstood how it worked. The group's roster of around 55 comedians all have day jobs in tech. (Mr. Patel is an engineer.) ""The credentials back up the jokes,"" he said. Even as A.I. threatens to replace workers, including engineers, Mr. Patel said, a common theme among his peers is how limited the technology's abilities actually are. ""We're all dealing with the same crap where a nontechnical manager says, 'Can we use A.I. for that?' And we say, 'No.' And they say, 'Try it anyway,""' Mr. Patel said. Lately, tech companies have invited Mr. Patel's comedy group to come roast them. The setup tests whether Silicon Valley's famous ""radical candor"" extends to delicate egos of those at the top. But comedians can get away with saying things others can't, Mr. Patel said. ""It's almost like us being a bunch of court jesters."" Photograph A detail of the piece titled ""Spambots"" by Neil Mendoza at the Misalignment A.I. Museum in San Francisco. (B1); Audrey Kim, founder of the Misalignment A.I. Museum, with tapestries designed by the musician Grimes using A.I.; An interactive exhibit at the museum, and, right, Raymond Yao, who works at the crypto company Coinbase.; The Misalignment A.I. Museum in San Francisco is set to move to a bigger, permanent space next year. (PHOTOGRAPHS BY LOREN ELLIOTT FOR THE NEW YORK TIMES) (B4) This article appeared in print on page B1, B4.",USA,"New York Times, Late Edition (East Coast); New York, N.Y.",2024-11-30,31,1602,2024-11-01,2024.4,Very Long,71,6,3.745318352059925,1,0,0,0,2,1.2484394506866416,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.6537453216394359,Negative
"Artificial intelligence has been a fast-moving, high-drama industry since OpenAI launched the viral chatbot ChatGPT a little more than two years ago and awakened the world to the technology's potential. But nobody has seen a week like the past one: DeepSeek, a disruptive new Chinese AI company, emerged seemingly out of nowhere; the world's most-valuable company lost nearly $600 billion of market value in a day; and it emerged that the developer of ChatGPT was in talks for a fundraising round that would value it at an eye-popping $300 billion. Here are the most important things we learned from last week's events: DeepSeek is a coup for China and a new approach to AI. Most people heard of DeepSeek for the first time in the past week, but tech geeks had it on their radar for months. The Chinese startup -- launched by an engineer who ran a hedge fund -- released a series of AI models nearly as good as the best that Silicon Valley has to offer. DeepSeek isn't just good. Thanks to some widely praised engineering breakthroughs, it was made at a fraction of the cost and in a fraction of the time of those made by Silicon Valley tech giants. Those breakthroughs allowed it to be made in China, a country that many experts thought was behind the U.S. because of export restrictions on powerful AI chips. And DeepSeek gave its models away, as open-source code, which helped make them immediately popular among consumers, businesses and developers. DeepSeek's rise made investors skeptical of Nvidia. As buzz about DeepSeek's latest model grew and its app shot to the top of the download and performance charts, Wall Street panicked. If artificial intelligence can be made more simply, investors figured, maybe the world won't need as many pricey chips. Shares of Nvidia, which was the world's most valuable company just a week earlier, plunged 17% last Monday, as part of a market wipeout that erased some $1 trillion of stock value. Nvidia, like other tech companies, said that DeepSeek's ascendance would increase overall demand for AI and boost demand for the company's products. But Nvidia stock recovered only slightly by the end of the week, signaling that investors are still wondering whether Nvidia's power as the computational arms dealer atop the AI boom has been diminished. Chip limits on China aren't stopping its AI ascendance. The U.S. in recent years has tightened restrictions on shipments of chips to China, aiming to limit development of the most advanced AI tools. Nvidia responded to the controls by producing less-powerful chips for the Chinese market. DeepSeek has said it used some of those chips to develop previous AI models. Many in Washington, D.C., are pressuring President Trump to take action to keep the U.S. ahead in an AI arms race. And members of Congress are advocating for a ban on exporting H20 chips to China. DeepSeek may be succeeding by free-riding on OpenAI's work. As American technologists began investigating how DeepSeek got so good, the word ""distillation"" started popping up. It is a technical term for an AI model that becomes intelligent by asking another model questions and learning from its answers. DeepSeek said it distilled from an open-source model released by Meta Platforms and from one of its own. But some technologists think it also may have distilled from other proprietary models whose terms of service forbid distillation. OpenAI said Wednesday it is reviewing indications that DeepSeek did just that to its systems. If that is the case, it would mean that American investment is benefiting China's new AI star. DeepSeek didn't respond to requests for comment. DeepSeek's strategy puts Silicon Valley's at risk. A growing chorus in Silicon Valley is questioning whether it still makes sense for the biggest AI developers, which are collectively losing billions of dollars a year, to keep spending gobs of cash developing advanced technologies. Now that DeepSeek has proven how successful distillation can be, investors expect many startups that could never afford to develop complex artificial-intelligence models to ape the Chinese company's approach. That could be great for consumers who may get more AI at low or even no cost. But it could dissuade leading AI companies from building the next cutting-edge model, out of fear that their work will quickly be distilled and benefit competitors. SoftBank is making a huge bet on OpenAI. OpenAI Chief Executive Sam Altman praised DeepSeek for ""impressive work"" in a post on X but said that his company is ""excited to continue to execute on our research road map."" On Thursday, The Wall Street Journal reported that a powerful partner is standing behind Altman. SoftBank, the Japanese tech and investment conglomerate led by Masayoshi Son, is in talks to invest $15 billion to $25 billion in OpenAI. That money would be part of an investment round of up to $40 billion from numerous backers that would value OpenAI as high as $300 billion. In October, OpenAI was valued at $157 billion. The increase is a loud endorsement by Son of Altman's spend-big-to-dominate strategy. As the wild week for AI ended, Altman threw one last bomb: In an ""ask me anything"" session on Reddit on Friday, the CEO said his company should consider giving away its AI models, just as DeepSeek does. By Ben Fritz and Thomas Gryta",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-02-03,27,888,2025-02-01,2025.1,Long,72,1,1.1261261261261262,1,0,0,0,0,0,0,0,0,0,1,1.1261261261261262,1,0,0,0,1,1.1261261261261262,1,1,1.1261261261261262,1,4,4.504504504504505,1,3,3.3783783783783785,1,0.18815267802348845,Neutral
"They have names like Grace, Max and Tom and some entrepreneurs say they are the future of healthcare. They are artificial-intelligence agents: bots that execute tasks end to end. Already, AI agents can automate the ordering of groceries and filing of expense reports, and now venture-backed companies are designing them for healthcare tasks such as enrolling participants in clinical trials, ensuring proper care after hospitalization and helping doctors quickly learn medical histories when seeing patients for the first time. AI companies aim to reduce physician burnout through agents that complete administrative tasks and help them provide better care as their caseload grows. As more startups enter the field, some observers say hype is outpacing published data supporting the efficacy of AI agents in healthcare, and that some startups use the term agentic AI as a marketing gimmick. ""It's going to take some time to filter out the fakes and frauds before the cream rises to the top,"" said Steve Mika, commercial lead, data and AI solutions, for consulting firm OmniData, recently acquired by Fresche Solutions. The more AI agents do, the more administrators, physicians and patients must trust them. Several entrepreneurs said they build in guardrails to stop agents from straying beyond their intended function, keep humans in the loop and plan to publish research in peer-reviewed journals. Agentic AI has gained prominence over the past two years as large language models have grown increasingly powerful. More-conventional automation required humans to set up highly specific rules, said Kulveer Taggar, an entrepreneur and investor in technology startups. ""What's different now is these LLMs can understand context a lot better,"" he added. ""They can do the decision-making themselves, and they can be a lot more flexible."" As a result, agents can handle jobs such as conversing with patients in human-sounding voices as they schedule appointments, answer questions and book transportation to the doctor's office -- capabilities some companies are using to help healthcare providers reduce clinicians' workload and contend with shortages of doctors and nurses. San Francisco-based Grove AI, which recently raised seed financing, has created Grace, an AI agent to help researchers conduct clinical trials. The agent -- so named because Grove wants trials to be operated with grace -- can do things like calling patients to prescreen them for studies and arrange transportation to clinical sites. Grace identifies itself as a digital assistant when contacting patients, said co-founder and Chief Executive Tran Le. Clinical researchers field many patient inquiries, but finding people who qualify for studies is often difficult because trials have strict inclusion criteria, said co-founder and Chief Technology Officer Sohit Gatiganti, adding that Grove customizes agents for clients. For those working with older patients, for example, it can make Grace speak slower and repeat things more often, he said, adding that Grove tests agents by intentionally trying to throw them off to ensure Grace does only what clients want. ""She is there to fit the exact needs of customers,"" Gatiganti said. Similarly, San Francisco-based Infinitus Systems uses agents to automate calls for healthcare providers, health insurers and drugmakers and says it builds in safeguards, working with clients to restrict what agents can say and enabling them to escalate calls to humans. ""As agentic AI continues to be developed and adopted, we believe the customer should have control over what these agents can and cannot do,"" co-founder and CEO Ankit Jain said. AI software can be subject to Food and Drug Administration regulation depending on what it does, said Erin Whaley, a partner with law firm Troutman Pepper Locke. The more autonomous it is intended to be and the higher the risk it poses, the more likely it is to be regulated, she added. Venture-backed companies applying agentic AI to patient care said their products don't require FDA authorization because they are support tools that inform physicians but leave the final judgment to them. Eli Ben-Joseph, co-founder and CEO of New York startup Regard, had his sights set on medical school as an undergraduate at the Massachusetts Institute of Technology, but changed his mind after shadowing doctors and seeing that many were unhappy. Administrative work and the need to do more in less time sapped much of their enjoyment, he said. Regard uses AI to scan electronic-medical records and recommend diagnoses, including those that might have been missed because a doctor was pressed for time, he said. When suggesting a diagnosis, the system displays data used to reach that conclusion, Ben-Joseph said. Regard tracks diagnoses it recommends, compares them with the decisions physicians make and plans to publish research about the accuracy of the system later this year, he said. Regard also has developed Max, an AI agent that would be assigned to patients in a medical system and stay with them to serve as their advocate -- quickly updating new physicians on their medical history to enable more-efficient care, Ben-Joseph said. Regard expects to fully roll out Max in June. ""Max is like the med student that never gets tired and knows everything about the patient,"" Ben-Joseph said. St. Louis-based Lumeris, founded in 2012, has built a $3 billion business as a provider of Medicare Advantage plans and using technology to help health systems operate under value-based care arrangements. Through agentic AI, it seeks to expand by serving primary-care practices nationwide. Tom -- an agent named for the late Lumeris co-founder Dr. Tom Doerr -- is an extension of the care team and can perform about 100 actions, such as automating calls to patients after a hospital discharge, according to the company. Patients typically should visit their primary-care doctor within seven days of discharge to ensure they are adhering to their treatment plan and reduce the risk of readmission, said Dr. David Carmouche, executive vice president and chief clinical transformation officer of Lumeris. Today, with the shortage of primary-care doctors, that often doesn't happen and patients fall through the cracks, he said. ""There aren't enough primary-care hours to deliver the services that are needed for all of the patients,"" said Jean-Claude Saghbini, chief technology officer and president of technology services for Lumeris. By Brian Gormley",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-03-03,26,1017,2025-03-01,2025.1,Long,73,2,1.9665683382497543,1,0,0,0,0,0,0,0,0,0,1,0.9832841691248771,1,0,0,0,0,0,0,4,3.9331366764995086,1,0,0,0,0,0,0,-0.2793280552334724,Neutral
"[Financial Analysis and Commentary] When the excitement around artificial intelligence started spreading to power stocks, the rally was concentrated on those with a big portfolio of nuclear power plants, such as Constellation Energy and Vistra. This could now be changing. Thanks to regulatory scrutiny in key markets of deals between nuclear plants and data centers, most of which effectively draw power away from the rest of the grid, investor favor may shift to others: companies that can quickly build new gas-fired power plants and vertically integrated utilities. Shares of NRG Energy, which doesn't own any nuclear capacity, had lagged behind the surge seen by nuclear-owning peers Vistra, Constellation Energy and Talen Energy over the past two years. But last Wednesday, its shares jumped 11% after it made two big announcements. One was an agreement with gas-turbine manufacturer GE Vernova and contractor Kiewit to construct more than 5 gigawatts worth of new gas-fired power plants, which would be enough to power millions of homes. Separately, the company said it is in talks with two data-center developers to supply power, primarily from new natural-gas-fired plants. Even after a broad selloff among power stocks on Thursday, its shares are outperforming Vistra and Constellation so far this year. Vistra shares fell 12% on Thursday after it failed to disclose any new contracts in its earnings call. Chief Executive Jim Burke said on the call that ""there are a number of questions to be answered"" from regulators before Vistra can finalize certain contracts with data center customers. The company expects more regulatory clarity by midyear. Constellation disclosed quarterly results last month without announcing new contracts. Constellation and Vistra operate the largest and second-largest nuclear-power fleets, respectively, that operate in competitive power markets. There are many reasons why funneling energy from existing nuclear power makes sense for data centers. It is quicker than building a power plant from scratch and provides round-the-clock clean power. Because these sites have plenty of land, it is possible to co-locate data centers and possibly dodge transmission fees. The sites also have ready access to cooling water. These are also the most lucrative contracts for power plant owners, who get to charge higher power prices for an existing asset. But these deals are facing skepticism from regulators in the largest competitive power markets, which are already seeing surging power prices. Late last year, the Federal Energy Regulatory Commission blocked part of Talen Energy's plan to sell power from its existing nuclear power plant directly to Amazon.com's data center in Pennsylvania. Trump-nominated FERC Chair Mark Christie said an agreement of that type could have ""huge ramifications for both grid reliability and consumer costs."" The regulators last month voted to launch a review of issues associated with such arrangements in PJM Interconnection, a market that includes Pennsylvania. Meanwhile, Texas' state Senate introduced a bill a few weeks ago that, if passed, would add transmission costs for large power customers and create guardrails to make sure they don't threaten grid reliability. None of this spells complete doom for nuclear power plants looking to sign deals. Depending on how regulation shakes out, it could just mean such customers have to pay more transmission fees. The bigger risk is that the regulatory process drags out. FERC's Christie has said the commission would act quickly, but given the sensitivity around grid stability and power prices, there could be legal challenges to their decision. Time is of the essence for data center customers; they may prefer to ink contracts that involve less regulatory uncertainty. ""It's not just the money, it's really the time,"" notes Stephen Byrd, equity analyst at Morgan Stanley. His team's analysis shows that there is going to be some 42 gigawatts worth of shortfall between data-center demand for power and actual grid capacity through 2028 in the U.S. Easier contracts to strike could include ones for new natural gas-fired power plants, such as those NRG announced. Next to existing nuclear power, new natural-gas-fired power is the best bet for AI because it runs around the clock and can be built much faster than nuclear power. Technology companies might find it easier to put net-zero ambitions on the back burner under the current administration. NRG said long-term contracts on those new gas-fired power projects could range from $70 to $90 per megawatt-hour, similar to industry analysts' estimate of what Talen Energy's nuclear power plant would get from its lucrative contract with Amazon. There is enough potential profit here that even Exxon Mobil and Chevron, oil majors with higher return hurdles, plan to build new natural-gas-fired power plants for data centers. Because there is a limited supply of gas turbines, investors will want to keep an eye on companies that have secured slot agreements. Vistra, for its part, said it has gas turbines booked for delivery in 2026 and 2027. More data-center customers could also look to work with vertically integrated utilities rather than power producers in competitive markets. Going this route may not come with the same level of speed or long-term fixed price certainty, but it could be simpler because it only involves dealing with one entity, according to Rodney Rebello, co-portfolio manager of Virtus Reaves Utilities ETF. Shares of utilities announcing data-center deals have rallied: Entergy, which operates across several Southern states, is up about 74% over the past 12 months, while Alliant Energy, which serves Iowa and Wisconsin, has gained 36%. As tech companies broaden the pool of prospective contracts, the AI power rally should spread across more stocks, not just a few highfliers. Enlarge this image. By Jinjoo Lee",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-03-03,14,927,2025-03-01,2025.1,Long,74,2,2.157497303128371,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1.0787486515641855,1,0,0,0,9,9.70873786407767,1,0,0,0,1,1.0787486515641855,1,-0.13809868013185844,Neutral
"In his newly built palace near Tokyo, lined by stone statues of Roman emperors and surrounded by an 18-hole golf course, Masayoshi Son was stewing. After declaring for years the imminent arrival of the artificial-intelligence revolution, the chief executive officer of SoftBank Group had missed out on it. ""I haven't been able to do anything,"" he thought, according to a speech he gave to SoftBank investors last year. ""Can I just get old like this and die?"" As it turned out, all he needed was a new golden boy. And now Son, who has a history of latching onto charismatic startup founders, has found one in Sam Altman. In what would be the largest-yet investment in a startup, Son is preparing to put as much as $43 billion toward Altman's OpenAI in a pair of transactions. SoftBank is in talks to invest between $15 billion and $25 billion in the ChatGPT maker as part of a blockbuster $40 billion funding round, The Wall Street Journal reported Thursday. The round would value OpenAI at up to $300 billion -- nearly double its valuation in October and an undeniable sign of Son's confidence in its prospects. In addition, the Japanese tech and investment conglomerate has committed $18 billion toward Stargate, a venture to build cloud computing centers for OpenAI to use, according to people familiar with the matter. For Altman, who was traveling to Tokyo on Thursday, the tie-up provides him with a deep-pocketed backer at a critical moment. His company's relationship with Microsoft, its biggest investor to date and longtime exclusive technology partner, has been fraying over OpenAI's contention that it wasn't getting enough cloud computing power. And the broader AI world has been thrown on its heels by the splashy arrival of DeepSeek, a Chinese developer of cheaply made and free-to-use AI models, which has sparked skepticism about OpenAI's strategy of spending big on proprietary technology. Son's commitment to Open-AI and Stargate ensures that Altman's company will have ample cloud-computing firepower in the coming years just as it has ended the exclusivity portion of its Microsoft deal. And by leading a funding round that would be the biggest in Silicon Valley history, Son is enthusiastically endorsing Altman's plan to keep spending gobs of cash on leading-edge AI systems. ""For all of us, the AI era represents a once-in-a-generation opportunity to help build a better, safer, healthier, and more prosperous future,"" Son wrote in a widely distributed email message Thursday. At an event hosted by his company in Washington, D.C., the same day, Altman praised DeepSeek's technology as ""great work"" and framed its emergence in terms of U.S. competition with China. ""This is a reminder of the level of competition and the need for democratic AI to win,"" he said. Son's plans for OpenAI eclipse his biggest-ever bets. The fortunes of the debt-friendly, risk-addicted CEO have been on a roller coaster since he emerged as a force in the 1990s. An early wager on Alibaba and Jack Ma was a highlight, while a failed $16 billion investment in WeWork and its co-founder Adam Neumann was a black eye. Son's past few years have been turbulent. The $130 billion he spent through SoftBank's Vision Fund unit -- theoretically aimed at AI companies -- resulted in middling overall returns. Meanwhile, SoftBank didn't invest early in any of the generative AI companies that have defined the frenzy of the past two years. Instead, it was the $32 billion purchase in 2016 of Arm Holdings, a chip-design company, that became SoftBank's golden goose. The company surged in the halo of the AI rush after SoftBank relisted it on the Nasdaq in 2023. Arm's stock has tripled since then, and SoftBank's stake is worth more than $140 billion -- an asset that gives Son plenty of wiggle room financially. Borrowing against its Arm stake would be one way for SoftBank, which had around $30 billion of cash as of September and has vowed to keep a large buffer on hand, to fund OpenAI. It could also sell some assets such as its stakes in T-Mobile and Deutsche Telekom, valued at a combined $27 billion, according to FactSet. SoftBank has held early talks with potential lenders to help fund its investments in OpenAI and Stargate, people familiar with the discussions said. Altman, one of the most charismatic and accomplished fundraisers in contemporary Silicon Valley, has talked to Son about investments dating back to at least 2017, when he ran the startup incubator Y Combinator. Altman met with Son in Tokyo, after which the SoftBank chief proposed investing in some of Y Combinator's businesses. The talks fell apart because Son wanted to invest at larger amounts than Y Combinator's leaders felt their business could handle, according to people familiar with the matter. More recently, the two have discussed various projects SoftBank and OpenAI could work on together, including a potential effort to build AI chips across the world. Over the course of 2024, Son became a convert to Altman's vision. The SoftBank CEO gave a speech in October in which he gushed about OpenAI's newest technology and said he had recently asked ChatGPT how to turn 10 million yen into 100 million yen -- though he didn't share its answer with the crowd. That month, SoftBank put $500 million into a $6.6 billion fundraising round for OpenAI, though Son had wanted a larger stake, people familiar with the matter said. The next month, SoftBank launched a $1.5 billion tender offer to purchase existing shares from OpenAI employees. Meanwhile, the SoftBank chief had been looking for just how he could make a giant AI bet. Months ago, he settled on the data-center push, according to a person familiar with the deliberations, reasoning that SoftBank could bring large chunks of cash and connections with top tech companies. After Donald Trump's election victory in November, the plans for Stargate began to coalesce as a way to marry SoftBank and OpenAI's needs. Altman brought in Oracle Chairman Larry Ellison, whose company was already helping launch a data center in Texas for OpenAI to use with Microsoft's blessing. At a January press conference with President Trump at the White House, Stargate's founders said they were committing $100 billion to the venture and aiming to spend up to $500 billion over the next four years. OpenAI and SoftBank initially committed about $18 billion each to Stargate, with much of OpenAI's money expected to come from the SoftBank-led round, according to people familiar with the matter. Oracle and the United Arab Emirates fund MGX are expected to contribute money as well. The value of the Texas data center will also be counted in the initial $100 billion. They still have billions more they need to secure. --- Deepa Seetharaman, Tom Dotan, Miho Inada, Amrith Ramkumar and Peter Landers contributed to this article. By Eliot Brown, Berber Jin and Keach Hagey",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-02-04,28,1145,2025-02-01,2025.1,Long,75,1,0.8733624454148472,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.8733624454148472,1,3,2.6200873362445414,1,0.8030376931566581,Positive
"Silicon Valley's hottest investment isn't a new app or hardware product. It's one man. AI researcher Ilya Sutskever is the primary reason venture capitalists are putting some $2 billion into his secretive company, Safe Superintelligence, according to people familiar with the matter. The new funding round values SSI at $30 billion, making it one of the most valuable AI startups in the world. Sutskever became one of the industry's most revered AI researchers as chief scientist at OpenAI, where he helped develop the technology behind ChatGPT. He left OpenAI last year following a painful rupture with the company's chief executive, Sam Altman. SSI said it doesn't plan to release any products until it develops super intelligence -- an industry term for an AI that can outsmart experts in nearly every field. Competitors like Google, OpenAI and Anthropic are trying to develop similarly advanced systems but are releasing consumer chatbots and business applications in the interim to generate revenue and demonstrate their progress. Sutskever has told associates he isn't developing advanced AI using the same methods he and colleagues used at OpenAI. He has said he has instead identified a ""different mountain to climb"" that is showing early signs of promise, according to people close to the company. ""Everyone is curious about exactly what he's pushing and exactly what the insight is,"" said James Cham, a partner at venture firm Bloomberg Beta, which hasn't invested in SSI. ""It's super-high risk, and if it works out, maybe you have the potential to be part of someone who is changing the world."" Most AI startups work hard to get attention, hoping it will help attract employees and investors in a highly competitive space. SSI operates as secretly as it can out of offices in Silicon Valley and Tel Aviv. Its bare-bones website contains little more than a 223-word mission statement. Its approximately 20 employees -- a fraction of the 1,000-plus at OpenAI and Anthropic -- are discouraged from mentioning SSI on their LinkedIn profiles, according to knowledgeable people. Candidates who secure an in-person interview are instructed to leave their phone in a Faraday cage, a container that blocks cellular and Wi-Fi signals, before entering SSI's offices, one of the knowledgeable people said. Most of its staffers aren't well known in Silicon Valley because the company is looking for promising technologists whom Sutskever can mentor, rather than experienced people likely to jump between employers and take what they have learned with them. Still, Silicon Valley's top investors, including Sequoia Capital and Andreessen Horowitz, have poured money into the company. The latest financing, which marks an increase from its $5 billion valuation in September, is being led by Greenoaks Capital. SSI's talks to raise funding at a $30 billion valuation, which doesn't include the new cash it is collecting, were previously reported by Bloomberg. Sutskever was born in the former Soviet Union, grew up in Israel and made his name as a graduate student in Canada, after co-authoring a paper about deep-learning AI algorithms, which use a process called scaling to become smarter by processing massive amounts of data. He later joined Alphabet's Google but left in 2015 to become one of the first employees at OpenAI. He was attracted to Altman and fellow co-founder Elon Musk's vision of a nonprofit dedicated to developing artificial general intelligence -- AI as capable as most people at most things -- for the public good. Colleagues jokingly described Sutskever as a prophet prone to musing about what a world with AGI might look like and how to prevent it from causing catastrophic disasters. ""Our goal is to make a mankind-loving AGI,"" he said at the company's 2022 holiday party. After ChatGPT was released as a test in late 2022 and became a worldwide sensation, OpenAI became less of a pure research lab and more of a traditional company focused on products and revenue. Sutskever and members of his team felt that left them with fewer resources for studying advanced AI and curbing its potential risks. His relationship with Altman, who became OpenAI's chief executive, deteriorated. In November 2023, Sutskever delivered a message to his boss that would change both their lives: OpenAI's board was firing Altman after determining he hadn't been consistently candid with them. The move backfired when hundreds of employees threatened to quit and Microsoft offered to hire them along with the deposed CEO. Sutskever said he regretted ""my participation in the board's actions."" Altman was rehired in under a week. Sutskever remained officially employed but stopped working. He resigned last May and started SSI with former OpenAI researcher Daniel Levy and investor Daniel Gross. By focusing entirely on creating a safe super intelligence, the new company hopes to avoid the tension between products and research at OpenAI as it works toward a goal even more ambitious than AGI. The startup assembled some small seed funding to get started before raising $1 billion in September. Rumors have since swirled about SSI's business and technological strategy. In a rare public appearance at the NeurIPS AI conference in December, Sutskever discussed the kind of super intelligence he is trying to develop He told thousands of fellow researchers that when such systems emerge, they could be unpredictable, self-aware and may even want rights for themselves. ""It's not a bad end result if you have AIs and all they want is to coexist with us,"" he said. By Berber Jin and Deepa Seetharaman",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-03-05,29,903,2025-03-01,2025.1,Long,76,4,4.4296788482834994,1,0,0,0,0,0,0,0,0,0,4,4.4296788482834994,1,0,0,0,1,1.1074197120708749,1,0,0,0,4,4.4296788482834994,1,3,3.3222591362126246,1,0.0935484784087095,Neutral
"It was a seductive pitch to city governments and police departments: Use predictive software to deter crime before it's committed. Artificial intelligence-powered algorithms, the software companies said, could chew up data on incident reports, weather, time and other variables, learn historical patterns, and spit out forecasts faster, cheaper and more accurately than human analysts. Using big data to put cops in the right place at the right time would help discourage crime. Federal funding helped push such tools to police departments in Los Angeles, New York and elsewhere in the 2010s. More recently, however, those tools have faced pushback. Criminal-justice advocates warn that a disproportionate number of reported incidents involving low-income people or people of color could lead to outsize police footprints in their communities and unequal enforcement relative to total crime. Some academics question how effective the tools really are. Santa Cruz, Calif.'s police department stopped using software developed by PredPol, now known as Geolitica, in 2017. Weeks after the police killing of George Floyd ignited racial justice demonstrations around the U.S. last year, the city became the first in the nation to effectively ban such technology, with local officials warning it could contribute to racial profiling and strain police ties with the community. More recently, lawmakers in Oakland, Calif., and New Orleans voted to prohibit the tools. The Los Angeles Police Department, where the prototype of Geolitica's software was initially tested, says it stopped using it last year because of budget cuts, a move that came after an internal watchdog called for more oversight of predictive analytics. The LAPD and Chicago Police Department halted programs to predict potential repeat offenders. Now, predictive-policing companies are starting to rebrand and rethink their products, focusing less on ""forecasting"" crime and more on tracking cops, both to provide oversight and to learn what behaviors correlate to reduced crime. The evidence that predictive software reduces crime more effectively than human analysts is mixed, says Sarah Brayne, a University of Texas at Austin professor who researched the LAPD's use of such tools for five years for a 2020 book, ""Predict and Surveil: Data, Discretion, and the Future of Policing."" ""Part of that is that nobody has access to the data required to engage in independent evaluation,"" says Ms. Brayne. Most predictive tools are privately made, though the New York Police Department created an in-house program. Proving causation between the tools and reductions in crime is also difficult, Ms. Brayne says. Santa Cruz-based Geolitica, which helped popularize the term ""predictive policing,"" changed its name in March to better reflect how police departments use its software, Chief Executive Brian Macdonald says. The company also wants to differentiate itself from programs that try to predict potential criminals. PredPol ""was a terrible name,"" Mr. MacDonald says, adding that he wanted to make the change for years. ""We weren't really doing predictions. We were just identifying high-risk locations that officers could patrol."" Mr. MacDonald believes Geolitica's core data -- incident reports about shootings, burglaries and other crimes -- is the least biased information that data analytics tools can use. ""It's not the police who are deciding there's a crime,"" he says. ""It's you, the victim, who's saying crime happened here."" Racial biases in policing, he says, are largely symptoms of deeper socioeconomic inequalities. Police departments that use Geolitica often install global positioning systems in cop cars or radios to track where individual officers are patrolling, comparing deployments to crime forecasts. The private company's platform, maintained by 10 employees and used by about 50 departments, projects such data onto souped-up heat maps of geographic areas broken down into 500-square-foot boxes. ""We now actually have a couple of customers who don't even use our predictions,"" Mr. McDonald says. ""They actually set their own patrol areas, and they use us for monitoring and management of the officers out in the field."" Ms. Brayne and some executives at software companies expect departments could increasingly employ these bird's eye views to monitor officers' movements and improve accountability, in addition to forecasting crime hot spots. Newark, Calif.-based ShotSpotter is following a similar path. Its core product attempts to detect gunshots in neighborhoods using audio sensors. In 2018 it acquired crime-forecasting software HunchLab and launched a product called ""ShotSpotter Missions."" Last year, the company renamed the offering to ""ShotSpotter Connect"" and says it beefed up the platform's patrol-management features. AI-powered algorithms spot patterns and trends across data sets to help police supervisors put cops in the right places, says Sam Klepper, ShotSpotter's senior vice president of marketing and product strategy. The goal is to track granular details of their work, from making traffic stops to visiting local businesses, he says. ""It's having reporting that the command staff can go back and look at and say, 'These tactics correlate to the most reductions in crime,' and they can double down on those types of activities,"" Mr. Klepper says. Mr. Klepper says ShotSpotter, which has about 120 customers for various products, including police departments in Washington, D.C., and Wilmington, Del., sees an opportunity to use its predictive software to help target social work and mental-health services for high-risk areas. The firm hopes to test such a program but can't share details, he says. William Bratton helped start the wave of crime-forecasting software when he called for predictive analytics as LAPD commissioner in 2008. He was also commissioner of the NYPD in 1994 when it launched CompStat, short for computerized statistics, which analyzed data about recent crimes to help cops identify hot spots. He says younger cops are more willing to embrace such technologies, but departments need to clarify guidelines that address community concerns. ""Let's face it, policing has not been a perfect institution,"" Mr. Bratton says. ""We have, in some instances, abused the tools we're given."" Crime-forecasting firms are quick to distinguish their products from data-mining or facial-recognition technologies that use personal information to attempt to identify suspects. Geolitica and ShotSpotter say they rely on reported incidents and don't use data points such as arrests -- a metric for police activity rather than total crime -- for fear of perpetuating a cycle of over-policing in communities of color. Critics such as Rashida Richardson, a Northeastern University professor who studies big data and racial justice, warn that firms' attempts to reposition their products could distract from fundamental questions about predictive analytics. The potential for biased data goes beyond historical arrest records to the level of 911 calls, she says, pointing to the incident when a white dog walker in Central Park falsely accused a Black bird watcher of threatening her life. ""That's going to come in as a data point,"" Ms. Richardson says. ""How does one deal with a data set when you don't know the context of every call for service?"" The tools also don't collect analogous information about nonphysical spaces where cyber- or white-collar crime occurs, which could make enforcement for such incidents less likely, she says. The complexities of such data sets pose questions that many governments aren't currently equipped to answer, says Robert Cheetham, CEO of Azavea, the firm that sold HunchLab to ShotSpotter. ""We have zoning boards,"" Mr. Cheetham says. ""Why not have civic-algorithm review boards, populated by a mix of people who understand cities, and academics and practitioners?"" A handful of jurisdictions have created such bodies in recent years to research government-used algorithms. In Pittsburgh, which paused a predictive-policing program last year, a public-private partnership is preparing to issue recommendations for how to use such technology in policing, criminal sentencing and child welfare services. In Santa Cruz, lawmakers left open the possibility of using crime-forecasting and facial-recognition tools if the city council approves them based on a review of peer-reviewed research into bias. Mr. MacDonald of Geolitica says he supports the idea. Justin Cummings, the Santa Cruz councilman and former mayor who spearheaded the city's restriction, says law-enforcement officials and lawmakers should root out existing bias before adopting new tools with unclear impacts. ""Ensuring everyone is treated equally under the law is something that we have to address first,"" Mr. Cummings says. ""It's really tough."" Credit: By David Uberti",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2021-07-09,7,1346,2021-07-01,2021.3,Long,77,1,0.7429420505200593,1,2,1.4858841010401187,1,8,5.943536404160475,1,0,0,0,3,2.2288261515601784,1,0,0,0,1,0.7429420505200593,1,7,5.200594353640416,1,1,0.7429420505200593,1,0,0,0,-1.8248284394361847,Negative
"At the annual convention of the International Longshoremen's Association last year, two large screens played a TikTok video from a crane operator over the docks of Los Angeles. ""Aaaall of this, man, is all automated,"" he said, pointing toward acres of stacked containers stretching to the Pacific. He marveled at the automated vehicles driving amid the containers: ""No drivers in any of those machines. . . . They're no joke, man."" From the podium, Harold Daggett, the union's pugnacious leader, was having none of it. ""They say that's the future,"" he bellowed to the thousands of gathered workers. ""Over my dead body."" Tens of thousands of dockworkers last week returned to their jobs on East Coast ports after a three-day strike that threatened to snarl trade and hobble the economy. Workers won a 62% pay increase. But a much larger, thornier issue remains -- one that's playing out in other businesses as well, from factories to grocery stores to Hollywood: How much, and how quickly, are humans willing to concede to machines? The new dockworker agreement extends only until mid-January. Daggett's opposition to automation of any kind threatens to derail the continuing negotiations over the next three months and push the dockworkers back to the picket lines. The long march of technological advancement has transformed manufacturing and countless industries over the decades -- and brought attempts to defy change. Workers in a range of industries today fear being displaced by ever-more sophisticated machines, particularly with the rapid rise of artificial intelligence. ""Someone has to get into Congress and say, 'Whoa, time out,'"" Daggett said in a recent video interview. ""What good is it if you're going to put people out of work?"" Shipping executives note with frustration that U.S. ports lag behind facilities in Europe and Asia in automation. Major Asian and European ports consistently rank higher than their U.S. counterparts in an annual ranking by the World Bank that measures factors such as port productivity and the amount of time a ship spends in port. One issue of contention is the effect of adapting automation on jobs. Employers say it creates new jobs as automation requires new roles and drives more cargo through ports. Unions point to roles lost to machines -- a dockworker, for instance, might not be equipped to slot into a new job that requires specialized or high-tech skills. It's challenging to quantify the impact. Many factors impact the amount of work at any given terminal, including the size and frequency of ships arriving and departing. Across all industries, Daron Acemoglu, an economist at the Massachusetts Institute of Technology, estimates that around six jobs are lost for every robot implemented. About 700,000 American jobs -- many of them in manufacturing and other blue-collar trades -- have been lost over the past 30 years due to automation, he said. For dockworkers and others, ""the real issue is what does this look like five years, 10 years into the future?"" said Timothy Bartl, CEO of the HR Policy Association, a trade group representing chief human-resources executives. ""How do we start anticipating that?"" Shipping industry officials say they need machines, like automated stacking cranes powered by artificial intelligence, to squeeze ever-growing volumes of cargo through ports hemmed in by sprawling metropolitan areas. Some of the world's most efficient ports, such as Khalifa Port in Abu Dhabi, are built on greenfield or offshore sites where space is abundant. The U.S. already boasts some advanced cargo-handling operations. In sections of ports around the country, automated cranes lift massive containers from ships, autonomous vehicles carry boxes across the docks, and yet more autonomous cranes organize giant container stacks. Cameras and computer systems read codes on containers and truck trailers to automatically allow trucks into and out of ports and to track and speed the flow of cargo. Automation drives predictability and consistency. Machines don't call in sick, stop to chat in the parking lot with friends or text friends and family members during work hours. But they can't solve everything. The supply-chain snarls during the Covid pandemic that led more than 100 containerships to back up off the coast of Long Beach and Los Angeles were caused as much by shortages of warehouse space, trucking equipment and railcars as they were by inefficiencies on the docks. Still, the sole fully automated terminal at Long Beach worked ships and managed container stacks better than all of its peers in the pandemic. Now shipping companies are looking to add autonomous cranes and vehicles to terminals large enough to accommodate them. In terminals too small or awkwardly shaped to deploy robots, they're aiming to make better use of cameras, artificial intelligence and other technologies. The ILA and its West Coast counterpart, the International Longshore and Warehouse Union, are opposing, stalling and drawing out expensive concessions from employers in return for allowing the technology. Total Terminals International at the Port of Long Beach recently negotiated a deal with the West Coast union to install automated technologies. The details are still being worked out but are likely to include taller cranes that will be operated from a remote room and automated stacking cranes on the docks, said Ammar Kanaan, the chief executive of the terminal's owner. Kanaan said the total number of jobs at the terminal won't be cut, but the mix of jobs will change. He said the improvements will allow the terminal to move twice as much cargo without expanding the facility. Some in the shipping industry are concerned the company made costly commitments to the union on staffing requirements, according to one industry official. Dockworkers, meanwhile, are worried that the West Coast union is allowing automation at too many terminals, the official said: ""The East Coast has seen what has happened on the West Coast and they want to prevent that from happening."" The East Coast union says the Los Angeles automation project beamed into the ILA convention cost hundreds of dockworker jobs. The company that owns the terminal, Denmark-based shipping giant A.P. Moller-Maersk, declined to comment. But the West Coast employer group that represents companies like Maersk said the number of longshore workers at the Los Angeles and Long Beach ports where automation is increasingly used employed 13% more dockworkers last year than five years earlier. On the East Coast, the ILA's labor contracts usually include language requiring employers to seek union approval for use of automation at existing port facilities. According to one shipping industry official, the tentative salary deal that would raise dockworker pay to $63 an hour from $39 an hour was reached on the condition that dockworkers agree to efficiency gains that include more technology. Marc Santoro remembers the anxiety he felt when his terminal at Bayonne, one of the smallest at the Port of New York and New Jersey, installed automated dockyard cranes 10 years ago. Santoro, 44 years old, said the employer gave the impression the cranes would handle larger volumes of cargo and create more work. What happened, he said, was 30 people who used to operate in shifts working 20 cranes became four people working in a room. Santoro, a father of three who was on the picket lines outside Port Liberty last week, said people may not have been laid off as a direct result, but the restructuring was felt by crane operators like him immediately. He rushed to call other terminals to pick up new shifts, but ""it definitely didn't make up for the lack of work,"" he said. ""Automation impacts humans and Americans,"" he said. ""If you have kids, you should be concerned about automation."" At the Port of New York and New Jersey, the largest port on the East Coast, the number of licensed dockworkers in 2020 was virtually unchanged from a decade earlier at just over 5,800 workers, according to the port's regulator. Shipping industry officials say that more efficient cargo operations lead to more cargo, which requires more dockworkers to load and unload ships. Machinery requires more mechanics and highly skilled employees who can oversee, upgrade and fix computers and robots. An executive at CMA CGM, the French ocean shipping company that bought the Port Liberty terminal two years ago, has said beefed-up automation will help the facility double its capacity so that it can move more than four million boxes annually over the coming years. Daggett said he wants to take his fight worldwide. He has talked in recent months about joining with unions around the world to punish ocean shipping companies that push automation by temporarily refusing to work their ships. ""I don't care about lawyers and governments,"" he said. ""It's time to stop machines from taking our jobs, destroying our lives and crippling the American economy. It's time we put companies out of business that push automation."" Over the past century, machines have upended many industries. In the U.S., robots have been used on automotive assembly lines since at least the early 1960s, with carmakers turning to automation to increase productivity, cut labor costs and reduce injuries, particularly on repetitive tasks. Robots took on the difficult work of lifting, welding and painting cars. Today, the auto industry is a top consumer of robots around the world. Breakthroughs in AI now threaten the jobs of highly skilled professionals, such as software developers. To lessen the pain for workers today, the U.S. may be able to draw lessons from how German firms incorporated automation and technology, MIT's Acemoglu said. There, unions and worker councils represent the workforce and advocate for technological changes and training programs that can allow employees to take on new jobs within the same organization. ""The ILA saying no to robots is never going to work,"" he said. The challenge for workers is how to adapt, said Michael Chui, a partner at the McKinsey Global Institute, the research arm of the consulting firm. ""It's very rare that a machine will come in and do everything that's in someone's occupation,"" Chui said. ""Typically, what we see is automation doing pieces of people's jobs. It's less about, 'In comes the robot and out goes the person.'"" Proponents point to technology's safety benefits. Dockworkers work long hours, sometimes around the clock, in heat and cold and rain, and around large containers, vehicles and cranes. Automation allows them to work in the comfort and relative safety of a climate-controlled office. Many see efforts to hold off advances in technology as ultimately doomed to fail. ""Ever since the Luddites, it's been pretty hard for workers to really slow down automation over the longer term,"" said Harry Holzer, an economist at Georgetown University. ""Unions can slow it down a bit, but really, not very much, and not for very long."" --- Costas Paris contributed to this article. (See related letters: ""Letters to the Editor: Is the Robot Economy to Be Feared or Feted?"" -- WSJ Oct. 16, 2024) Credit: By Paul Berger, Chip Cutter and Chao Deng",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-10-09,58,1816,2024-10-01,2024.4,Very Long,78,5,2.7533039647577096,1,0,0,0,0,0,0,0,0,0,20,11.013215859030838,1,0,0,0,0,0,0,3,1.6519823788546255,1,3,1.6519823788546255,1,3,1.6519823788546255,1,-1.1599759095763014,Negative
"Parents have been steering their children into science and technology fields for so long that some of those kids are grown with little ones of their own. Their advice? Careers in the humanities, arts or skilled trades might be safer bets for the next generation. Bots that write software and perform surgical tasks inspire fear that today's glut of STEM majors are in a bubble, kind of like their predecessors who flooded finance programs in the roaring '80s. From 2009 to 2022, the number of bachelor's degrees awarded in computer science nearly tripled, according to the National Center for Education Statistics. If the people who work on tech's cutting edge think their children should reverse course, then maybe the rest of us ought to reconsider our parental guidance. Dan Dumont recently did what any responsible engineering director would do: He asked his favorite artificial-intelligence assistant whether his children, ages 2 and 1, should follow in his footsteps. Maybe not, the bot warned. It recommended fostering creativity and people skills, while stopping short of prescribing specific jobs for the toddlers. The advice jolted Dumont, 38, who works at a software startup in Greater Boston. He thought back to his time at a vocational school in Massachusetts two decades ago, when he felt confident that enrolling in college and launching a tech career was more promising than the blue-collar paths of most classmates. His professional life has generally reinforced that belief. Now, as he and his wife mull a home renovation with their third child due this summer, he suspects the odds could be different for his kids. ""Maybe they should be contractors or electricians,"" he says. ""Maybe we shouldn't push them into technology."" More parents are coming to the same conclusion, says David Ferreira, spokesman for the Massachusetts Association of Vocational Administrators. He says vocational school was long looked-down-on in a state known as a science and tech hub. ""It was where other people's kids went -- kids who learned with their hands and who were not college material,"" he says of many white-collar parents' thinking. Not anymore. Ferreira says 1 in 5 Massachusetts high-schoolers is in a vocational program, about 30% more than a decade ago. And trade schools that used to accept virtually everyone now have hundreds of teens on wait lists. Demand is so high that several mainstream public high schools in the state are reviving or expanding shop classes, part of a nationwide trend. Parents who pictured their children in desk jobs are drawn to vocational education because the schools have upped their academic standards and provide a hands-on hedge against AI, according to Ferreira. ""They see a dual pathway where their kids can go to this kind of school and have options at the end,"" he says. ""They're qualified to go on to college or go directly into the workforce."" Jeannie Chung deals with constant anxiety about children's career prospects -- and she doesn't even have kids yet. She works in Washington state as an applied AI lead at a large tech company and has become an unofficial counselor to the many parents in her social circle who want inside advice. ""Jobs that require just logical thinking are on the chopping block, to put it bluntly,"" she says. ""I think the pendulum is swinging back to the creative side of things."" Chung, 32, initially wanted to major in English and become a fiction writer. Her parents worried there was little money in the liberal arts, so she studied biomedical engineering and electrical computer engineering instead. There were 30 students in her introductory computer science class at Duke. The same course had more than 300 students by the time she graduated. She developed a genuine interest in technology. But Chung also says she and many peers ""rode the wave"" to careers that appeared lucrative and safe, instead of following their passions. How ironic, 10 years later, to see the recent spate of tech-sector layoffs. Not that novel writing is a sure ticket to stability, but rapid labor-market changes show the potential folly of trying to predict hot jobs for your kids. If someday Chung has a daughter who wants to be a book author? ""I'll be like, 'Work on your style and work on your voice,'"" she says. "" 'Your creativity is your value.'"" Technology training has seemed like a golden ticket to Rajeev Madumba since he came to the U.S. from India in the early aughts. ""Coding was not my cup of tea, but it was evident that this was the next frontier, so I did an M.B.A. in information systems and e-commerce,"" he says. It served him well. Madumba, 52, leads the global healthcare practice at 22nd Century Technologies in McLean, Va. Like a lot of parents who watched tech companies hit trillion-dollar market values, he urged his two children to learn Python and other computer programming languages when they were young. He figured it would be a valuable skill in their back pockets. Now he's not sure whether an entry-level coding job is a reliable fallback plan -- or if such a role will even exist in the future. These days he's encouraging his teenage daughter's musical interests while she looks at colleges and considers majoring in biology. He's noticed the woman who runs their local dance studio appears to earn a nice living -- and her job looks relatively bot-proof. ""I keep telling my daughter, if nothing else works out, you could still help others learn to sing and dance and you should be OK,"" he says. (See related letter: ""Letters to the Editor: How to Live in the Age of AI"" -- WSJ March 19, 2025) By Callum Borchers ERIC",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-03-10,6,948,2025-03-01,2025.1,Long,79,0,0,0,0,0,0,0,0,0,1,1.0548523206751055,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0085728871329451,Positive
"[Financial Analysis and Commentary] The last time independent power producers were this excited about an electricity-boom cycle was in the late 1990s and early 2000s, when demand-growth expectations were fueled in part by the growth of Silicon Valley. It didn't end well. In a piece published on Forbes.com in 1999, Peter Huber and Mark Mills wrote: ""Southern California Edison, meet Amazon.com. Somewhere in America, a lump of coal is burned every time a book is ordered online."" The two authors, who co-wrote books about energy, including ""The Bottomless Well,"" estimated that one billion PCs on the Web would represent electrical demand equal to the total power capacity of the U.S. at that time. The piece drew much attention and pushback, including from scientists at the Lawrence Berkeley National Laboratory, who said the authors were overstating the impact. In a 2001 PBS interview, Peter Cartwright, who was then chief executive of Calpine, said, ""Silicon Valley, as everybody knows, is well aware -- is one of the fastest-growing demand centers in the state, and we have very -- hardly any power generated in this area."" Calpine was one of the most-aggressive independent power producers of that era. Its installed base grew at a compound annual growth rate of 63% between 1998 and 2002 through both new construction and acquisitions. In addition to demand growth, Calpine and other developers believed their new gas-fired power plants would come to replace older, less-efficient generators that were built by monopolistic utilities rather than competitive developers. Similar to today, stocks of independent power producers at the time fetched much higher multiples of expected earnings than regulated utility peers, according to Chris Seiple, vice chairman at Wood Mackenzie. Seiple was a power-industry consultant at the time and wrote a report warning that the power-plant construction frenzy could result in a bust. He turned out to be right: Major developers went bankrupt in the aftermath, including Calpine and NRG Energy. There were other factors driving the power-plant boom at the time. For one, that was an era when many states -- including California -- started deregulating power generation, allowing independent developers to build large-scale power projects in a competitive market. Hugh Wynne, co-head of utilities and renewable-energy research at SSR, worked at a power-project development company from the mid-1990s to 2001. He said developers at the time assumed that the long-term power price would match the long-run marginal cost of new power-plant capacity, covering both variable operating costs as well as capital costs. In fact, Wynne said, after the aggressive build-out resulted in a surplus of capacity in many markets, power prices fell sharply, covering only the operating costs of these power plants -- not enough for the companies to pay off the debt they raised to build them. Could the current market face similar problems? Likely not in quite the same way. For one thing, the drivers of electricity-demand growth are more tangible this time around. Tech companies are spending real money on building out AI infrastructure, and the Chips Act contains clear incentives for nearshoring chip manufacturing. At the same time, energy demand from transportation and industrial applications -- including fracking equipment -- is steadily shifting to electricity. Secondly, the industry today is more familiar with how competitive power markets work. Independent power producers and the financiers backing their projects were burned enough by the last gold rush that they aren't likely to invest in new power plants without some kind of long-term power-purchase agreement, Wynne said. Always-available power is important for data centers, and tech companies have been willing to sign long-term contracts at premium prices. While power demand looks set to inevitably grow, the magnitude and timing are an open question. The utility Arizona Public Service in its latest planning document forecast that its demand from high-power users -- such as data centers -- could range anywhere from less than 100 megawatts to more than 1,000 megawatts in the coming years. Notably, utilities' long-term demand forecasts changed a lot over the past few years and are likely to keep being revised. In early 2021, utilities in aggregate expected load to grow 8.2% by 2035 compared with 2021 levels, according to analysis by the Rocky Mountain Institute. As of June 2024, the expected growth was 23.9%. So while an outright bust looks unlikely, there is the risk that independent power producers' stocks get ahead of themselves. Vistra and Constellation Energy are the top and the fourth-best performers of the S&P 500 in the year to date, alongside Nvidia, the second-best performer. Constellation Energy is trading at about 30 times forward earnings, 47% higher than its historical average. Vistra is about 30% more expensive than its historical average based on that metric. Independent power producers might not be the only beneficiaries of rising electricity demand. Regulated utilities have been teaming up with tech companies to develop new generation, including Berkshire Hathaway-backed NV Energy and Duke Energy. Utilities in certain states aren't allowed to own their generation, but some have said they would push for legislation to change that. The power industry has matured a lot in the past two decades. But the same fundamental lesson holds: High-voltage expectations can lead to painful burns. Enlarge this image. Credit: By Jinjoo Lee",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-10-10,11,873,2024-10-01,2024.4,Long,80,1,1.1454753722794961,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4.581901489117985,1,1,1.1454753722794961,1,0,0,0,-0.7346379664610042,Negative
"California's AI safety act is dead following Gov. Gavin Newsom's veto last week, but its specter lingered over technology leaders some 3,000 miles away who remained divided about what future regulation could look like. The technology sector remains divided over whether Newsom made a mistake by using his veto, but SB 1047's introduction and ultimate demise raise bigger questions, including whether regulation should target AI model makers or AI applications and whether it should be directed more by states or federal agencies. ""I think we should be very narrowly focused on the application of AI, not the training of AI models and the operation of AI models,"" Aaron Levie, co-founder and chief executive of Box, said at The Wall Street Journal's CIO Network Summit in New York on Tuesday. Levie added that he was against SB 1047 and the government shouldn't risk slowing down the pace of AI innovation. ""Right now, we need as much progress as humanly possible,"" he said. Levie added that is especially the case since he believes we are still far away from any kind of existential risk from AI -- although that is debated. While regulation should definitely come, Levie said it should come more from individual agencies designed to regulate specific markets, like the Federal Aviation Administration and the Food and Drug Administration. The California bill would have required developers of large AI models to take ""reasonable care"" to ensure that their technology didn't pose an ""unreasonable risk of causing or materially enabling a critical harm."" ""I would say that both politically and substantively, Newsom was wrong"" to veto the bill, Bradley Tusk, co-founder and chief executive of Tusk Venture Partners, said at the summit. For AI to grow into a truly multibillion-dollar industry, ""you've got to give the public some level of confidence that somebody is keeping an eye on this thing,"" Tusk said, calling the bill a reasonable first step to setting up intellectual frameworks for mitigating risks and harms. While it may not have been perfect, Tusk said, a better move would have been passing the bill and then a trailer bill next year. He also said he tends to agree with some opponents of the bill who felt regulation should focus more on the application level than the model level. Arvind Narayanan, professor at Princeton University, director of the Center for Information Technology Policy at the school and author of ""AI Snake Oil,"" agreed. There is a lot of concern that the misuse of AI in the financial system will cause panic in the market -- but that market is already regulated by federal agencies and has been for decades, he said at the summit. ""I look at the [Federal Trade Commission] a lot, in particular consumer protection, and they have been doing this work, and that's the kind of thing I want to see at a number of other federal agencies,"" Narayanan said. He added that it would make sense to regulate the model makers to some extent, but at the moment that should be focused more around transparency. ""We don't know a lot about what is going on inside these companies,"" he said. Tusk said he saw 800 some bills generally related to AI introduced in state legislatures last year and that he has been in touch with legislators from New York state looking to draft their own AI bill. He said the New York bill likely will be stricter than California's SB 1047. Meanwhile, CIOs remain torn: certain that some kind of regulation is good but uncertain how it could impact their work and technology innovation overall. Still, there is a need to do something, Andrea Malagodi, CIO of software company Sonar, told The Wall Street Journal at the event. ""Of course, there's the view that innovation is going to be encumbered,"" he said. ""I wonder if that kind of thing is based on fear."" ""There is a reasonable friction that should be expected between innovation and safety,"" he added. --- Bellle Lin contributed to this article. Credit: By Isabelle Bousquette",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-10-10,21,674,2024-10-01,2024.4,Medium,81,7,10.385756676557863,1,0,0,0,1,1.483679525222552,1,0,0,0,0,0,0,0,0,0,1,1.483679525222552,1,9,13.353115727002967,1,5,7.4183976261127595,1,0,0,0,-1.1013395933285912,Negative
"New York state banned the use of DeepSeek on government devices, citing concerns over data privacy and censorship in the generative artificial-intelligence app from China. DeepSeek, owned by Chinese hedge fund High-Flyer, shot to fame in January, with its low-cost AI models rivaling American ones using less-advanced Nvidia chips. The app quickly dethroned OpenAI's ChatGPT as the most downloaded consumer AI app in the U.S. and made its models open source, attracting attention from U.S. developers eager to latch on to the generative AI craze. DeepSeek also is causing security concerns among Western governments. Analysis from security experts suggests that it has hidden code that could enable information to be sent to sanctioned companies in China. State agencies began removing the app over the weekend at Gov. Kathy Hochul's direction, said Colin Ahern, the state's chief cyber officer. ""We do have high confidence that the reporting that we're seeing is true and should be taken very seriously,"" he said. DeepSeek didn't respond to a request for comment. Tests performed by The Wall Street Journal and others also raised concerns about censorship in the app, after it gave answers to politically sensitive questions over Taiwan, Tibet and the deadly 1989 military crackdown in Beijing's Tiananmen Square in line with China's official government positions. In some cases, the app refused to answer, unlike Western-developed apps such as ChatGPT. News Corp, owner of The Wall Street Journal, has a content-licensing partnership with OpenAI. Under the New York ban, state employees will be prohibited from downloading the app on all devices and networks managed by the state's Office of Information Technology Services, which manages tech for executive branch agencies in New York. The ban doesn't affect state workers' personal devices, Ahern said. He declined to comment on whether the state had been in communication with DeepSeek over the ban. Hochul's administration has taken several legislative and executive actions related to cybersecurity in recent years, including a statewide cyber strategy, AI limits, tighter regulatory requirements and closer integration between state and federal agencies on cyber issues. Much of the focus on cybersecurity was prompted by a ransomware attack on Suffolk County in 2022 that caused key state services to grind to a halt for months. The attack has cost more than $25 million to fix so far, officials said. The Australian government also has banned DeepSeek's use on official devices, while Italy's privacy watchdog blocked it there. Although the U.S. hasn't formally banned DeepSeek, some federal agencies like the U.S. Navy and NASA have blocked it due to security and privacy concerns. Texas was the first state to ban DeepSeek on government devices, citing national security concerns. By James Rundle",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-02-11,11,445,2025-02-01,2025.1,Medium,82,6,13.483146067415731,1,3,6.741573033707866,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2.247191011235955,1,0,0,0,0,0,0,-1.1684115983102752,Negative
"Generative artificial intelligence has sparked one of the biggest spending booms in modern American history, as companies and investors bet hundreds of billions of dollars that the technology will revolutionize the global economy and one day lead to massive profits. The question is when, and even whether, all those investments will pay off. Apps like OpenAI's ChatGPT have attracted hundreds of millions of users, but relatively few people are paying for premium versions and businesses are still experimenting with how generative AI can increase their productivity. Nonetheless, the biggest tech companies are putting record amounts of money into capital spending, primarily for the hardware needed to develop and run AI models. ""The risk of underinvesting is dramatically greater than the risk of overinvesting,"" Sundar Pichai, chief executive of Google parent Alphabet, said on an earnings call in July. Venture capitalists are similarly betting that at least a few AI startups could one day be worth billions or even trillions, even though most currently aren't profitable. Venture-capital investments in AI startups are at $64.1 billion so far this year, putting them on track to approach a peak set during a broader investing upsurge in 2021. And the total share of VC investments going toward AI this year is the highest on record. The fruits of all that spending can be seen around the U.S., as new data centers are popping up with increased frequency. In the past, data centers were used primarily to remotely store data and run non-AI software. AI-optimized data centers house specialized chips needed to develop and run generative AI applications. From early 2020 through this year, Microsoft has more than doubled its number of data centers. Google's total is up 80% over the same period. Oracle is heavily focused on the business and plans to build 100 new data centers. AI data centers are more power-hungry than those built in the past, because AI chips require a constant and reliable source of energy to operate. Even brief dips in power could damage the ""training runs"" in which AI models improve by analyzing reams of data. For large models, each training run costs tens or hundreds of millions of dollars. Since 2015, the amount of power that data centers in the U.S. and Canada have ordered from energy companies has increased nearly ninefold. The chips used to train and operate AI models primarily come from one company: Nvidia. Its graphics processing units, originally designed for videogames, cost tens of thousands of dollars each on the high end. Tech companies building and hosting AI models vie to get the biggest possible allocation from Nvidia. Meta Platforms CEO Mark Zuckerberg has said his company plans to have 600,000 GPUs by the end of 2024. Tesla chief Elon Musk, who is building his own AI startup xAi, said he hopes to have 300,000 GPUs by next summer. Highly skilled talent is another scarce commodity. Despite widespread layoffs that have hit Silicon Valley recently, tech giants are spending many millions of dollars to lock down research scientists who they believe can push AI into new frontiers. Many of these experts until recently would have worked in academia. Now they are among the most richly compensated technologists in the world. Even people with a more basic understanding of the machine learning that underpins AI have their pick of six-figure jobs. New listings for AI-related positions in July were up nearly 50% compared with last year, while postings for tech jobs overall were down slightly. Investors' patience with Silicon Valley's outsize AI spending is likely to wane. They have already penalized the stock of companies such as Meta and Microsoft for increases in AI spending without fast-enough revenue growth. A partner at the venture-capital firm Sequoia Capital recently calculated that to justify this year's investment in data centers and chips alone, AI businesses will ultimately need to generate $600 billion in revenue. Though most companies don't disclose their revenue from AI, analysts have estimated the annual total is at most in the tens of billions. Worries about whether AI backers have gotten ahead of their skis hark back to the dot-com era a quarter-century ago, when companies poured cash into fiber-optic networks to support bullish estimates of internet use that took longer to develop than expected. Executives at the top tech companies are preaching patience. In the recent earnings calls, Zuckerberg said it would be years until AI apps are monetized, and Google's Pichai said ""there is a time curve in terms of taking the underlying technology and turning it into meaningful solutions."" Enlarge this image. Credit: By Nate Rattner and Tom Dotan",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-09-13,25,769,2024-09-01,2024.3,Medium,83,2,2.6007802340702213,1,0,0,0,0,0,0,1,1.3003901170351106,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,9.102730819245773,1,0.1674762607869987,Neutral
"The nerd brigade is reporting for duty. They probably won't win any push-up contests and might not be sharpshooters. Yet for part of the year, a set of brainy Silicon Valley executives will trade their corporate-branded vests for U.S. Army Reserve uniforms because they know a lot about artificial intelligence. The chief technology officers from Palantir and Meta Platforms -- Shyam Sankar and Andrew ""Boz"" Bosworth, respectively -- will join Kevin Weil and Bob McGrew of OpenAI pedigree to make up the inaugural cohort of a new Army innovation corps. Their mission: Swap C-suites for bases and bring some badly needed tech upgrades to the Army. ""It's possible I watched too much 'Top Gun,'"" said Bosworth, 43 years old. Standing more than 6-feet-2, he was told he was too tall to realize his youthful ambitions of flying an F-16 jet fighter. Bosworth said Meta Chief Executive Mark Zuckerberg supported his decision to join the Reserve. ""There's a lot of patriotism that has been under the covers that I think is coming to light in the Valley,"" he said. Less than a decade ago, even working on technology that might be used in the military -- never mind suiting up for service -- was anathema in Silicon Valley. The new Reserve program reflects how the relationship between the Pentagon and the tech industry has deepened. Meta and OpenAI adjusted their policies to work more with the military last year. Recently they each joined with the weapons maker Anduril Industries to develop products for the Pentagon. Palantir has been involved in national-security work for two decades. It has an AI and data project with the Army valued at potentially more than $1 billion. Many in Silicon Valley assert that their cutting-edge know-how can equip the military for a conflict with a tech-powerful adversary like China -- while profiting the tech sector. For the Army, the deepening ties can help it prepare for the wars of the future. They are expected to be waged, in part, with ground robots and drones, and rely on networks of sensors and artificial intelligence to coordinate it all. ""We need to go faster, and that's exactly what we are doing here,"" said Gen. Randy George, the Army's chief of staff. The Army, in a show of its own geekiness, has dubbed the tech-reservist program Detachment 201. The moniker refers to the hypertext transfer protocol status code 201, which indicates the creation of a new resource on a server. Detachment 201 is the first deployment of tech elites. Brynt Parmeter, the Pentagon's chief talent management officer who has been leading the creation of a tech-reservist program since last year, is pushing for other services in the armed forces to follow the Army. The tech reservists will serve for around 120 hours a year. Because of their private-sector status, each will carry the rank of lieutenant colonel. Reservists are deployed based on their skills, so the tech unit's members more than likely won't find themselves in a firefight. Instead, they will work on projects that, for instance, teach soldiers how to use AI-powered systems or use health data to improve fitness. By Heather Somerville",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-06-16,11,525,2025-06-01,2025.2,Medium,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,3.8095238095238093,1,1,1.9047619047619047,1,-0.4486578854981372,Negative
"Unit X By Raj M. Shah and Christopher Kirchhoff Scribner, 336 pages, $30 When Raj Shah was an F-16 pilot in 2006 flying missions along the Iraq-Iran border, he discovered that the multimillion-dollar fighter's navigation system was so out of date that it couldn't tell him which side of the border he was flying over. He decided to load his hand-held, $300 Compaq iPAQ with civilian GPS software and digital maps. It worked perfectly. Ten years later, when Mr. Shah visited the U.S. Air Force command center in Qatar as a civilian, the same tech gap was evident: To his dismay, he saw computers armed with software programs that were older than the military officers using them. Fortunately, help was on the way. Mr. Shah had just been named director of the Defense Innovation Unit Experimental, an organization set up by the Defense Department the year before to bridge the gap between the Pentagon and Silicon Valley. In ""Unit X,"" Mr. Shah and Christopher Kirchhoff -- another founder of what came to be called DIUx -- chronicle their experiences at the unit in its first years of existence. Along the way, they suggest how the tech gap got so wide and describe the attempts to close it. Driving the DIUx mission, Messrs. Shah and Kirchhoff write, was a vision of what warfare might look like if the Pentagon and Silicon Valley joined forces: ""Imagine a stealthy electric flying car that lands like a helicopter [and] flies in near silence . . . or tiny AI-powered quadcopters that can map the inside of a building and recognize faces of terrorists before Navy SEALs break down the door. Or a constellation of microsatellites . . . or a fleet of seagoing drones that can scan for threats for a fraction of the price of a single navy destroyer."" These weren't science fiction-scenarios but real possibilities. The missing component was the idea of adapting for national-security use technologies the private sector was already developing and deploying. A major mindset shift would be required. During World War II and the Cold War, as the authors remind us, the Pentagon had largely funded major technological advances that spilled over to the commercial world, from nuclear power and computers to lasers, advanced sensors and the pioneering versions of the internet. At a certain point, though, the paths of the Pentagon and America's tech companies diverged. While China learned how to master advanced tech to manufacture and deploy lethal weaponry, the U.S. fell behind -- until by 2015 it was a problem that no defense secretary could afford to ignore. It was the late Ash Carter, defense secretary in the Obama administration, who stood up the Defense Innovation Unit Experimental. It put its headquarters in Mountain View, Calif. -- right in the belly of the high-tech beast. At the time, Messrs. Shah and Kirchhoff write, the U.S. military ""barely knew anything about"" tech advances in the commercial sector. Meanwhile, allies like Israel and potential foes like China were moving to take advantage of what high tech could provide. China's best high-tech companies -- Huawei in 5G, SenseTime in facial recognition, DJI in drones, Bytedance and TikTok in AI-driven social media -- were working for the government whether they wanted to or not. The Defense Department would need to bring U.S. tech companies on board in a different way. At the start, the Pentagon bureaucracy lived up to its stereotype. It was slow, self-protective, and resistant to any change that might threaten its relations with the defense industry and Congress. The DIUx team saw this outlook at work when it tried to enlist Capella, a company headed by a 20-something Iranian-born immigrant and engineer named Payam Banazadeh. Capella was signed up to develop and launch low-cost satellites equipped with synthetic-aperture radar sensors, so-called SAR satellites, which can see through clouds and see at night. The aim was to use them to keep more precise track of North Korea's weapons programs. The Pentagon blew a gasket. The fear was that Capella's $15 million program would upset negotiations with Congress over the Defense Department's billion-dollar program to build much bigger SAR satellites -- even though Capella's were not only cheaper but employed more up-to-date technology. Bob Work, the deputy defense secretary, finally intervened, endorsing the DIUx proposal over the objections of his subordinates. But ""it turned out the battle was just beginning,"" Messrs. Shah and Kirchhoff write. The national-intelligence establishment now objected -- fearing that Capella would show Congress that it could get better results faster and for less money, upsetting its own plans and budget negotiations -- and it managed to get the Capella program halted. Finally, in October 2018, DIUx used its own ""walking around"" money, earmarked for other projects, to get Capella's promising new technologies off the ground (literally). ""Toward the end of 2018, the company launched its first test satellite into space. In 2020, Capella launched its first commercial satellite, and landed its first customer -- which was, ironically, the intelligence service of an allied country."" As for the billion-dollar satellite program the Pentagon had fought to defend, ""by 2023 the firm that had won the contract still hadn't delivered a working satellite."" It wasn't just government bureaucrats who proved recalcitrant, however. Some companies in Silicon Valley treated the whole idea of working with the Pentagon as a form of moral betrayal. One of those was Google. A DIUx-recruited team enlisted Google in late 2017 to help develop wide-area motion imagery for a program dubbed Project Maven. Here the aim was to identify objects and also actors, who would then be analyzed by AI software. When the news broke that Google might be helping the Pentagon with image-recognition software, it triggered a revolt among employees. Three thousand of them signed a letter demanding that Google back out of the project, claiming that the company ""should not be in the business of war."" ""To us, this was ridiculous,"" the authors write. ""For one thing, Google was not making weapons."" But Google's shamefaced withdrawal from Project Maven (which went ahead anyway) demonstrated the other problem that the Pentagon faced, in addition to its own inertia, if it wanted to get up to speed technologically: Silicon Valley was populated by ""well-paid people who were reaping the benefits of Western democracy but didn't want to support the freedoms they enjoyed."" Fortunately, there were (and are) other high-tech companies with a different outlook, companies like Palantir, which has worked to develop the kind of safe and glitch-free software systems that the military should be depending on, and Anduril, which has improved drone technology to fit the needs of a 21st-century military. Project Maven's own master architect, MIT-trained Brendan McCord, gave up a seven-figure private-sector salary to move to San Francisco to work with DIUx. Slowly the successes began to grow. ""By the end of 2017, DIUx had awarded contracts for forty-eight projects leveraging $84 million in funding from thirty military entities,"" write Messrs. Shah and Kirchhoff. By using what is called Other Transactions Authority to get around conventional bureaucratic rules, DIUx managed to close more and more deals ever more rapidly. By 2017, DIU had shed its ""x,"" or experimental, label: It had become a permanent feature in the Pentagon landscape. The heroes in ""Unit X"" include not only Silicon Valley patriots and DIU godfathers like Ash Carter and Bob Work but also, perhaps surprisingly, former Secretary of State Madeleine Albright. She was one of the first visitors to DIUx headquarters, and she brought 20 former foreign ministers with her. ""The U.S. is f-----"" was her first reaction when briefed on how far China had advanced the use of artificial intelligence to bolster its military power. She saw DIUx as a way for the U.S. to reclaim a tech advantage. For the authors, getting the Pentagon to take the AI revolution seriously is one of the successes they are most proud of. They point to the creation of the Joint Artificial Intelligence Center in 2018 and to the Pentagon's recently announced Replicator initiative, aimed at integrating AI into thousands of mass-produced aerial, seaborne and undersea drones -- a visionary program that, though now under way, is still trying to catch fire within certain sectors of the Defense Department. Despite such progress, as the authors note, Silicon Valley and the Pentagon are still culturally divergent, not least in the way they approach innovation. One culture is shaped by the need to make things happen. The other is shaped by the need to make things not happen -- i.e., to prevent mistakes that could end in disaster. Of course, being risk averse means missing big opportunities -- in AI or drones or space or something new that we can't yet quite imagine. Until the proper balance is struck between the two cultures, dangers will persist. This is one reason why ""Unit X,"" within its steady and revealing narrative, has a tone of urgency. The U.S. military, the authors observe with emphasis, is still ""dangerously behind its rivals when it comes to technology."" It's a position we can't afford to be in much longer. --- Mr. Herman is a senior fellow at the Hudson Institute and the author of ""Freedom's Forge: How American Business Produced Victory in World War II."" Credit: By Arthur Herman",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-08-17,15,1550,2024-08-01,2024.3,Very Long,85,3,1.935483870967742,1,0,0,0,1,0.6451612903225806,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,6.451612903225806,1,0,0,0,-0.26669805772076133,Neutral
"New York's financial regulator said firms need to address the specific cybersecurity risks arising from the use of artificial intelligence, as more regulators aim to ensure the safe use of this rapidly evolving technology. The New York State Department of Financial Services on Wednesday issued a new guidance document that advises the entities it regulates to monitor and assess risks from AI-enabled tools, as part of the agency's existing cybersecurity regulation. The department said financial-services firms need to better understand AI-related risks, including from social engineering, cyberattacks and the theft of nonpublic information. The state regulator said the 11-page guidance document didn't impose new requirements but was just the latest installment in the department's efforts to rein in the risks from AI tools. The department recently adopted new guidance targeting discrimination by insurers through the use of AI. The NYDFS said it issued the guidance in response to inquiries about how AI is changing cyber risk and how it can be mitigated. NYDFS Superintendent Adrienne Harris said the guidance reinforces the need for firms to factor AI into their risk frameworks and the requirements under the amended cybersecurity regulations. ""I think it's really about making sure there's expertise in the institution, making sure they're engaging with lots of stakeholders, so they understand the development of the technology,"" she said in an interview, adding that firms need to be engaged in conversations around the use of AI even if they don't have a huge team of experts in the technology. ""It's about making sure that you've got the right expertise in-house -- or that you're otherwise seeking it through external parties -- to make sure your institution is equipped to deal with the risk presented,"" she added. The NYDFS said financial firms should have several layers of security controls with overlapping protections to ensure that if one control fails to mitigate the impact of a cyberattack, another control is in place to help, according to the guidance. These controls should include a risk assessment and risk-based programs and procedures, the ability to conduct due diligence on third parties and vendors, cybersecurity training and data management. The AI guidance from New York's regulator came after California Gov. Gavin Newsom vetoed an AI safety bill last month that pitted some of the biggest tech companies against scientists who developed the technology. The Democrat governor said he was rejecting the measure because it would apply only to the biggest and most-expensive AI models and didn't take into account whether they are deployed in high-risk situations. Despite the veto, many tech leaders still expect rules to regulate the technology will take shape in some form, as the bill raised bigger questions, including whether regulation should target AI model makers or AI applications and whether it should be directed more by states or federal agencies. Credit: By Mengqi Sun",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-10-17,14,473,2024-10-01,2024.4,Medium,86,11,23.25581395348837,1,0,0,0,1,2.1141649048625792,1,0,0,0,0,0,0,0,0,0,0,0,0,13,27.48414376321353,1,1,2.1141649048625792,1,0,0,0,-0.8702022064931859,Negative
"Crunchbase, the firm best known for its startup financing data, is using artificial intelligence to predict when those startups will raise funding, get acquired or hit the public markets. But in an era where chatbots are taking over traditional web and data searches, its biggest challenge is predicting its own future. The startup was founded in 2007 as a crowdsourced database. In 2010, AOL acquired Crunchbase through its purchase of tech blog TechCrunch, but in 2015 it spun it out. Yet since launching its premium database services roughly a decade ago, Crunchbase hasn't dramatically innovated on its core product: a comprehensive data set of startup financing that has served as a go-to for startup founders, venture investors and Silicon Valley watchers. That's changing with the launch of its AI-based prediction engine on Wednesday, which uses Crunchbase's 17 years of startup data to predict where Silicon Valley's hottest startups are heading, the firm's chief executive, Jager McConnell, said. Crunchbase's new direction was one of necessity -- when OpenAI's ChatGPT launched in late 2022, the moment sparked a crisis: What is the role of a database company like Crunchbase when chatbots can trawl the web for similar data and provide similar answers? ""If you deal with historical facts, once the AI absorbs it, it doesn't need you anymore,"" McConnell said. ""Historical data companies are already dead."" Like other startups that have been challenged by the emergence of generative AI, Crunchbase needed to pivot. At a leadership meeting convened to discuss its future, McConnell said the team decided it could make predictions using its most valuable asset: the proprietary data generated by the startup's 80 million users. That data isn't the public information on a company's profile. It's all of the data Crunchbase doesn't expose to its users, such as when company profiles have been edited, who edits them and what they're editing. If a startup's employee makes edits to a company's profile and is searching for investor profiles, plus there's an increase in investor interest in that startup's profile, those are the kinds of signals Crunchbase's AI uses to indicate the startup is about to raise funding, McConnell said. Each startup has thousands of potential signals like those pointing to whether they might be about to fundraise, be acquired or make a play for an initial public offering, McConnell said. Public AI platforms don't have access to that customer-usage data, he added, making it the ""crown jewels"" of Crunchbase's assets. Based on Crunchbase's own testing, its fundraising predictions are up to 95% accurate. Last year, the company predicted that AI startup Anthropic had a 74% probability of raising cash. Anthropic raised $2 billion in January. Last October, Crunchbase predicted the startup Coda, maker of a productivity platform, would be acquired with a 93% probability. The company was bought by Grammarly, maker of an AI-based writing assistant, two months later. What's much harder to predict is when startups shut down, McConnell said. The company's accuracy rate for startups that fold is below 50%, he said, because companies can survive for a long time, even with slowed growth. The company uses open-source machine-learning tools to train its AI prediction models, as well as OpenAI's AI assistant and tools to help create content and process data, McConnell said. News Corp, owner of The Wall Street Journal, has a content-licensing partnership with OpenAI. Crunchbase's own pivot, which began about two years ago, included some bloodshed. It laid off one-third of its 200-some employees, mostly in sales and marketing, in order to reinvest in data science and engineering, McConnell said. Of its roughly $50 million in revenue, 60% currently comes from premium subscriptions for the company's data, and the other 40% from access to its behind-the-scenes user data, he said. Now, with a stronger technical team in place, the company is also aiming to harness AI to make it easier for customers to use its products. Crunchbase's AI agent, named Scout, is like a chatbot that users can prompt with questions, and helps them search and analyze startup data. It isn't clear how useful Crunchbase's AI-based insights and agent will be. Large venture-capital firms tend to build out their own data science teams, with prediction engines of their own. The goal is for Crunchbase to be a source of data and prediction for them, McConnell said, rather than replace their own work. There are also other sources of data on startups and financing, including firms like PitchBook and CB Insights, which compete directly with Crunchbase. Like Crunchbase, PitchBook has a tool called VC Exit Predictor, which aims to use AI to forecast which companies will be acquired, go public or shutter. The challenge for startups like Crunchbase is staying ahead of AI-based tools, while the challenge for AI tools is getting access to proprietary data, said Mike Gualtieri, a vice president and principal analyst at Forrester Research. But new reasoning models like those from OpenAI and its Chinese competitor DeepSeek might be capable of making the kind of predictions that Crunchbase and PitchBook are making, because they can work through more complex problems, Gualtieri said. By Belle Lin",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-02-20,26,851,2025-02-01,2025.1,Long,87,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1.1750881316098707,1,0,0,0,0,0,0,2,2.3501762632197414,1,-0.05720603531029028,Neutral
"BERLIN -- The world's technology revolution is leaving Europe behind. Europe lacks any homegrown alternatives to the likes of Google, Amazon or Meta. Apple's market value is bigger than the entire German stock market. The continent's inability to create more big technology firms is seen as one of its biggest challenges and is a major reason why its economies are stagnating. The issue is even more urgent with the prospect of higher tariffs threatening to further curb economic growth. Investors and entrepreneurs say obstacles to tech growth are deeply entrenched: a timid and risk-averse business culture, strict labor laws, suffocating regulations, a smaller pool of venture capital and lackluster economic and demographic growth. Thomas Odenwald, a German tech entrepreneur, left Silicon Valley in January of last year to join Aleph Alpha, a Heidelberg, Germany-based startup that aimed to go head-to-head with artificial intelligence leader OpenAI. Odenwald had spent nearly three decades working in California but hoped he could help build a European tech giant to compete with the Americans. He was shocked by what he saw. Colleagues lacked engineering skills. None of his team had stock options, reducing their incentive to succeed. Everything moved slowly. After two months, Odenwald quit and returned to California. ""If I look at how quickly things change in Silicon Valley . . . it's happening so fast that I don't think Europe can keep up with that speed,"" he said. Aleph Alpha has since said it would move away from building a large-scale AI model and focus instead on contract work for government and businesses. The company said more than 90% of employees participate in its stock option program. Having largely missed out on the first digital revolution, Europe seems poised to miss out on the next wave, too. The U.S. and China, flush with venture capital and government funding, are spending heavily on AI and other technologies that hold the promise of boosting productivity and living standards. In Europe, venture capital tech investment is a fifth of U.S. levels. Marc Andreessen, the U.S. tech investor, posted a meme on his X account that showed an image of big AI players like OpenAI and Chinese rival DeepSeek fighting for dominance. At a nearby table, a figure labeled with the European Union flag sat apart, staring at an image of a plastic cap tethered to a drinks bottle -- a new legal requirement in Europe aimed at encouraging recycling. The message: Europe is focusing on the wrong battles. ""This is an existential challenge,"" wrote Mario Draghi, the former European Central Bank president who was tasked by the European Union's top official to help diagnose why Europe's economy is stagnating. In a report published last September, Draghi pinpointed the lack of a thriving tech sector as a key factor. ""The EU is weak in the emerging technologies that will drive future growth,"" he wrote. Only four of the world's top 50 tech companies are European, despite Europe having a larger population and similar education levels to the U.S. and accounting for 21% of global economic output. The problems go deeper than tech and reflect a broader truth about Europe: It isn't creating its share of new, disruptive companies that shake up markets and spur innovation. Over the past 50 years, the U.S. has created, from scratch, 241 companies with a market capitalization of more than $10 billion, while Europe has created just 14, according to calculations from Andrew McAfee, a principal research scientist at the MIT Sloan School of Management and co-founder of AI startup Workhelix. New companies and industries -- think autos replacing horse and buggies -- allows a country to produce more goods with the same amount of workers, a key driver of prosperity. Europe is dominated by old-school industries like autos and banks that extracted productivity gains long ago. The typical company in the top 10 publicly traded U.S. firms was founded in 1985, while in Europe, it was in 1911, according to the International Monetary Fund. By the late 1990s, when the digital revolution got under way, the average EU worker produced 95% of what their American counterparts made per hour. Now, the Europeans produce less than 80%. The EU economy is now one-third smaller than the U.S.'s and is stuck in low gear, growing at a third of the U.S. pace over the past two years. Europe has world-class research universities and a deep pool of engineering and scientific talent, much of which populates top U.S. firms. Spotify and fintech firms Revolut and Klarna are success stories. Venture capital arrived relatively late, but big U.S. venture-capital firms have set up shop in Europe in the past decade, including Sequoia Capital, Lightspeed, Iconiq and NEA. ""Europe is a much smaller market, but that doesn't mean it doesn't have great opportunities,"" said Luciana Lixandru, a partner at Sequoia Capital based in London. Europe had a promising start. At the start of the digital revolution in the 1990s, the region boasted several leading semiconductor companies (Netherlands-based ASML, Britain's ARM), software giants (Germany's SAP) and the dominant player in mobile phones (Finland's Nokia). The World Wide Web was invented by a Brit, Tim Berners-Lee, working at a European research facility. A big reason why Europe is now behind can be summed up as a lack of speed. Entrepreneurs complain that everything takes longer in Europe: raising money, complying with local regulations, and hiring and firing workers. ""In Germany a lot of people are just too cautious,"" said Karlheinz Brandenburg, the German engineer who helped invent the MP3 digital audio compression format. German consumer-electronics companies didn't think the invention was important and didn't invest enough in it, he said, and then Apple seized on the invention in the early 2000s to sell nearly half a billion iPod players. Brandenburg is now seeking 5 million euros ($5.6 million) in financing for a next-generation headphones startup. ""What is different in America is the speed of almost everything,"" said Fabrizio Capobianco, an early tech entrepreneur from Italy who lived for decades in Silicon Valley. ""Americans make decisions very fast. Europeans need to talk to everybody -- it takes months."" Capobianco, who returned to Italy three years ago, is now building a startup factory in the Italian Alps to scout out European tech companies. The prize for the winners: a one-way ticket to Silicon Valley. ""I don't think you can replicate Silicon Valley"" in Europe, said Capobianco. He wants other European entrepreneurs to follow his example: implant themselves in America's tech hub and manage teams of engineers located in Europe, where wages and living costs are lower. That inevitably means that the highest-value jobs will be in the U.S., Capobianco said. Most European startups find it so difficult to expand at the same pace as their U.S. counterparts that they typically move to the U.S., are bought by U.S. companies, or partner with them. One of the U.K.'s largest startups, delivery company Deliveroo, recently agreed to sell its business to U.S.-based DoorDash for $3.9 billion. Even Europe's hottest AI firms are linking up with American firms rather than competing against them. London-based DeepMind was bought by Google parent Alphabet in 2014. Paris-based Mistral AI, which has raised over $1 billion in the race to build large AI models, has signed distribution deals with Microsoft, Google and Amazon. In Europe, most business financing still comes from banks, which generally require physical collateral -- a building, perhaps -- in the event of losses. Other forms of financing include risk-averse public-pension funds. Early venture capital investors also demanded terms that left founders hamstrung, say entrepreneurs. ""There are a lot of scattered, small amounts of capital, and then you have these very large, slow-moving, bureaucratic, quasi-government agencies. And you don't have very much in the middle -- the more dynamic endowment capital that is in the U.S.,"" said Hussein Kanji, an American tech investor who founded Hoxton Ventures, a London-based venture-capital firm. Scaling up quickly in Europe is hard. The U.S. is a large integrated market, while Europe has dozens of countries with their own language, laws and taxes. Labor laws slow down worker mobility by making it harder to hire and fire workers. (There is often a three-month notice period in Europe for leaving a firm, and in some cases a six-month noncompete clause, jokingly known in Britain as ""gardening leave."") Until the past year or two, stock options in most European nations were little used because they were taxed as income before they vested. Taxes are higher, and regulations designed to corral big business become a costly and time-consuming headache for startups. It is easier for large AI companies in the U.S. or China to move to Europe than ""growing out of Europe and to have to invest from the start to satisfy a much more complex regulatory framework,"" said Sebastian Steinhauser, chief strategy and operating officer at German software giant SAP. Europe's love of regulation is one reason why Han Xiao started to think about moving his Berlin-based AI startup to the U.S. He and two friends founded their company, Jina AI, five years ago after studying in Germany, aiming to apply machine learning to search information in unstructured data for companies. ""When Germans talk about AI, the first topic is ethics and regulation,"" whereas investors in the U.S. and China focus on innovation, Xiao said. Engineers in Berlin are also hard to find, he said. Xiao's attempts to fire underperforming workers have landed in court. His 17 employees tried to form a union. Xiao initially raised about $7 million from American and Chinese venture-capital firms and SAP's U.S. arm. His latest $30 million funding round was led by Silicon Valley investment firm Canaan Partners. The European market for AI technology is very small, Xiao said, with local clients adopting the technology slowly. After spending November and December in Palo Alto, Xiao decided to make the move to the U.S. European businesses spend 40% of their IT budgets on complying with regulations, according to a recent survey by Amazon. Two-thirds of European businesses don't understand their obligations under the EU's AI Act, which came into force last summer, the survey found. Meta delayed the launch of its latest AI model in Europe by nearly a year because of EU regulations. It began rolling out a limited version in March that doesn't include features like image generation or editing. Apple also postponed its new AI features for iPhones in Europe until recent weeks. Software company Bird, one of the Netherlands' most successful startups, said recently it plans to move its main operations out of Europe to the U.S., Dubai and other locations due to AI regulation. ""Stop regulating, Europe. We might be the first, but we won't be the last (to leave),"" Robert Vis, the company's founder, wrote on his LinkedIn page. European cities crowd the top spots on quality of life rankings, far ahead of their American counterparts. That lifestyle might contribute to less appetite for risk, along with a culture of equality that frowns on naked ambition. ""I get a lot of pitch decks that say, 'This could be a $50 to $100 million company,' and that doesn't really interest me,"" said Chris Hill, a Santa Monica, Calif., native who lives in London and manages a fund for EdenBase. The rise of venture capital in London could eventually create an entrepreneurial ecosystem where money, talent and ideas are circulating quickly, said Sebastian Mallaby, a fellow at the Council on Foreign Relations whose book ""The Power Law"" details how Silicon Valley built an entrepreneurial culture. In some cases, though, old habits might die hard. The Draghi report, said McAfee at MIT, did a great job diagnosing Europe's lagging tech sector, but then urged governments to spend more public money spurring the sector, missing the point that it was private money that was absent -- most likely due to regulation and other problems. Said McAfee: ""That's when I went from nodding my head in agreement to banging it on the table."" By Tom Fairless and David Luhnow",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-05-21,40,2004,2025-05-01,2025.2,Very Long,88,4,1.996007984031936,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.499001996007984,1,15,7.485029940119761,1,2,0.998003992015968,1,4,1.996007984031936,1,-0.5911478094000111,Negative
"Artificial-intelligence search company Perplexity has begun fundraising talks in which it is looking to more than double its valuation to $8 billion or more, as startups try to ride the coattails of OpenAI's recent big investment. Perplexity has raised three rounds of funding in the past year -- an unusually fast pace even by Silicon Valley standards. In January, it was valued at $520 million. By this summer, its valuation grew to $3 billion. Its efforts to raise more money so soon at a substantially increased valuation will be a test of how eager investors are to own a piece of buzzy AI startups showing signs of market traction. OpenAI, whose ChatGPT dominates the market, recently closed one of the largest funding rounds in Silicon Valley history, raising $6.6 billion at a valuation of $157 billion. Perplexity has told investors it is looking to raise around $500 million in the new funding round, according to people familiar with the matter. The terms could change and the funding may not come together. The startup's annualized revenue -- an extrapolation of the next 12 months' revenue based on recent sales -- is currently about $50 million, the people familiar with the matter said. In March, its annualized revenue was a little over $10 million. Founded two years ago and backed by Jeff Bezos, Perplexity is a combination search engine and AI chatbot. It scours the web for current information like Google and provides answers to questions like ChatGPT. Perplexity currently earns money by selling premium subscriptions to consumers. It recently launched an enterprise version for corporate customers that searches their internal files and will soon start selling advertisements, broadening its sources of revenue. The consumer search engine is getting around 15 million queries a day, according to one of the people familiar with the matter. Perplexity has been criticized by a host of web publishers that have accused it of using their material without permission to generate AI search results. The New York Times recently sent Perplexity a ""cease and desist"" notice demanding the startup stop accessing its content. Perplexity Chief Executive Aravind Srinivas said his company isn't ignoring the Times' attempts to block access to its website and doesn't want to have an antagonistic relationship with news publishers. Credit: By Berber Jin and Tom Dotan",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-10-21,10,384,2024-10-01,2024.4,Medium,89,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,13.020833333333334,1,-1.082355697521525,Negative
"Data centers' squat, industrial aesthetic is getting a vertical and visual upgrade, driven by artificial intelligence-fueled demands for computing power, as well as geographic necessity. A movement of data centers from the boonies to the burgs has led operators to reconsider the windowless, prisonlike look that has defined data-center design for decades, resulting in projects more pleasing to the eye from street level. Buildings of two stories or more are becoming more common, as urban and suburban builders don't have the land to spread out, or don't want to pay the higher costs of doing so. ""Data-center footprints are continuing to expand, and if you can't go outwards, sometimes you have to go upwards,"" said Stephen Donohoe, vice president of global data-center design at Equinix. The company has properties that rise eight, nine and 10 stories high in cities across the globe -- plus its tallest, a 12-story building in Amsterdam. Some of its facilities have slick facades, exterior ""green walls"" of plants, or rooftop greenhouses powered by excess heat. ""They can't just be big boxes anymore,"" he said. Meeting the AI boom's power demand is among the factors that fed record high data-center construction in the first half of this year, according to real-estate firm CBRE. And overall construction of data centers is up more than sevenfold in just two years, says commercial property giant JLL. The traditional data center evokes images of sprawl -- vast, one-story server farms set on thousands of acres of rural land. That's the design developers have carefully refined over the past few decades to optimize their ""cost per megawatt,"" said Raul Martynek, chief executive of data-center operator DataBank. ""Ideally, the best scenario is an industrial warehouse building, single-story,"" he said. ""When you go up, you introduce certain cost elements."" Those additional costs include things like more copper piping, to extend from the data-center floor to the generator yard, he said, and less roof space for equipment like chillers. Plus, urban land and power are just more expensive. Yet building taller in cities like New York and San Francisco has allowed data-center operators to keep their services close to the areas they serve, said Dan Drennan, a principal and data-center sector leader at design firm Corgan. Closer data centers mean faster connections and lower lag time for city dwellers and local companies and it's easier to find workers to staff the facilities. Compared with the U.S., international data-center hubs in denser regions have more, and taller multistory buildings, which the industry generally defines as more than two stories. Singapore, for instance, is home to an 11-story Meta Platforms data-center campus, while tightly-packed Hong Kong boasts the 30-story iAdvantage data center. Land constraints have forced data-center designers there to ""be more creative,"" Drennan said. As long as there are still large tracts of land to build on elsewhere, data centers over 10 stories aren't likely coming to the U.S. anytime soon, according to Brett Rogers, chief development officer of data-center provider EdgeCore. In New York and Chicago, however, some tall data centers -- typically retrofitted older buildings -- predate their sprawling rural cousins. Sabey Data Centers put several floors of servers in a 1970s-era 32-story high-rise, formerly the Verizon Building, in Manhattan's financial district in 2011. In Chicago's South Loop, an eight-story Digital Realty data center was originally built as a printing press around 1912. Over the past two years, suburban areas have started to see growth in multistory data centers, too. Suburbs near Chicago like Elk Grove and Franklin Park are running out of land for data centers, said Andy Cvengros, a managing director and co-lead of JLL's U.S. data-center markets group. Data centers in Dallas and Atlanta are also pushing out into the burbs. In those areas near Chicago, land prices have nearly tripled over the past 36 months, and ""you're butting up against residential, golf courses, highways, airports,"" Cvengros said. Add on the cost of power -- especially if a utility provider needs to build a substation on-site -- and ""you've got to go vertical to make sense of those numbers,"" he said. But just as the need and appetite for more compute continues to push data centers skyward, new demands for AI are driving them to their next inflection point. Rapid innovation in AI chip design has forced designers to rethink things like structural capacity and how much equipment is needed, EdgeCore's Rogers said. Credit: By Belle Lin",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-09-21,15,737,2024-09-01,2024.3,Medium,90,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2.7137042062415193,1,0,0,0,-0.13356569856480657,Neutral
"President Biden's energy policies are so detached from reality that companies are scrambling to find more power on their own. Witness Constellation Energy's deal on Friday to restart its Three Mile Island nuclear reactor to power Microsoft AI data centers. Artificial intelligence is increasing demand for power while baseload plants that provide power around the clock shut down. New data centers are on hold because the grid can't support AI systems with intermittent wind and solar. Hence, the Microsoft deal. One of Three Mile Island's two nuclear reactors in Pennsylvania shut down permanently after the 1979 nuclear accident, and the other closed in 2019 because it became uneconomic. But the AI boom and forced retirements of coal plants are reviving nuclear power. Microsoft's deal follows Amazon's purchase of a Pennsylvania data center powered by an on-site 2.5 gigawatt nuclear plant. The Energy Department has approved a $1.5 billion government loan guarantee to restart a closed nuclear reactor in Michigan amid a Midwest power crunch. A growing problem is that coal and natural gas plants are closing prematurely amid an onslaught of regulation and heavily subsidized renewables. Wind and solar power can turn a profit running only some of the time. Baseload power plants can't -- especially as they're burdened by costly new Environmental Protection Agency rules. The EPA finalized its Clean Power Plan 2.0 that will require coal and new or refurbished gas plants to implement carbon-capture technology by 2032. Such technology isn't economic or feasible now, so coal plants will have to shut down. This threatens grid reliability, four regional grid operators warn in a friend-of-court brief supporting a challenge to the rule by 27 states. The grid operators that provide power to some 156 million customers -- MISO, PJM, ERCOT and SPP -- tell the D.C. Circuit Court of Appeals that EPA compliance timelines ""are not workable and are destined to trigger an acceleration in the pace of premature retirements"" of coal and gas generators. This will ""substantially strain"" their ability to ""maintain the reliability of the electric power grid."" The lights are flickering. Is anyone at the White House home? (See related letter: ""Letters to the Editor: Why Don't We Build Nuclear Plants Anymore?"" -- WSJ Sept. 27, 2024)",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-09-23,5,372,2024-09-01,2024.3,Medium,91,1,2.688172043010753,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,5.376344086021506,1,0,0,0,1,2.688172043010753,1,-1.0245713508126737,Negative
"When this year's college graduates first arrived on campus, there was no such thing as ChatGPT. They had to use their own brains for math homework, econ problem sets, coding projects, Spanish exercises, biology research, term papers on the Civil War and the Shakespeare essay that made them want to gouge their eyes out. Now they can just use artificial intelligence. Students outsourcing their assignments to AI and cheating their way through college has become so rampant, so quickly, that it has created a market for a product that helps professors ChatGPT-proof school. As it turns out, that product already exists. In fact, you've probably used it. You might even dread it. It's called a blue book. The mere thought of that exam booklet with a blue cover and blank pages is enough to make generations of college kids clam up -- and make their hands cramp up. But inexpensive pamphlets of stapled paper have become a surprisingly valuable tool for teachers at a time when they need all the help they can get. All of which explains how a paper company in Pennsylvania has unexpectedly found itself on the front lines of the classroom AI wars. Most blue books for sale in campus bookstores and on Amazon for 23 cents apiece are made by Roaring Spring Paper Products. The family-owned business was founded more than a century ago in Roaring Spring, a small borough outside Altoona that has become the blue-book capital of America. The company now sells a few million of these classic exam books every year and all of them are manufactured in the U.S., said Kristen Allen, its vice president of sales and marketing. And yes, I asked her if everybody makes jokes about Dunder Mifflin when they find out she works for a paper company in Pennsylvania. ""Nobody,"" she said. ""It's weird -- and it's sad. I love 'The Office.'"" Roaring Spring sells all the paper products you could possibly imagine: composition books with black-and-white marble covers, yellow legal pads and notebooks in every color, style and ruling. It also makes custom notebooks and folders with college logos, a crucial part of its business. But the most fascinating part of the company's latest catalog is page 63, where you can find more than a dozen products like Stock Item No. 77516: exam books with blue covers. There are blue books in different sizes, page counts and order quantities -- and there are green blue books with recycled paper. Allen has a keen understanding of why colleges still want Roaring Spring's blue books. After all, she has children in high school. ""I thought people just used AI for big things,"" she said. ""But no, they use it for everything. Which is pretty terrifying."" It also happens to be pretty good for business. This new golden age of blue books is not something that anyone would have predicted a few years ago, when remote school put them on the verge of extinction. But after sales tanked in 2020 and 2021 due to the pandemic, they have picked up in recent years because of AI cheating. The company declined to provide specific numbers on its blue books, so I asked public universities across the country to pull data from their campus bookstores. Sales of blue books this school year were up more than 30% at Texas A&M University and nearly 50% at the University of Florida. The improbable growth was even more impressive at the University of California, Berkeley. Over the past two academic years, blue-book sales at the Cal Student Store were up 80%. Demand for blue books is suddenly booming again because they help solve a problem that didn't exist on campuses until now. It might feel like ChatGPT came out yesterday, but students who were freshmen when it was released in 2022 will be seniors next year. That means they've had access to the most powerful cheating machines ever made for basically their entire time in college. And they have come to rely on ChatGPT. One of the most remarkable things about the product's explosive growth is that ChatGPT traffic declined in each of the past two summers -- when students were not in school. Only now are schools catching up. In math classes, students are being asked to leave their phones behind when they go to the bathroom during exams because OpenAI's ChatGPT, Anthropic's Claude and Google's Gemini make calculators look like abaci. In humanities classes, professors who understand the mind-boggling capabilities of AI models don't bother assigning traditional papers anymore. ""It's a pointless exercise,"" said Stan Oklobdzija, a Tulane University assistant professor of political science. ""It's like going to the gym and having robots lift the weights for you."" Not long ago, teachers could simply grade the words on the page. Now they can't be sure where those words came from. Kevin Elliott, a lecturer in the Ethics, Politics and Economics program at Yale University, learned this for himself in a seminar he taught this past semester, when he assigned a take-home essay and received a few papers with made-up quotes from famous philosophers. ""Smoking-gun evidence of AI,"" he says. For their next assignment, he asked students to write essays and come to his office to discuss them, and those oral exams revealed which students really grasped the material. Then he busted out the blue books for their finals. The first part asked students to identify passages and explain their significance. If they see the phrase ""forced to be free,"" for example, they should recognize that it's an idea from Rousseau and compare it with theories of Locke and Hobbes. The next part was an in-class essay. The students were given the prompt in advance so they could prepare, but they weren't allowed to bring their notes, which meant they actually had to think about how they would fill the empty pages. The only way to ace the test was to do the work themselves. It worked so well that Elliott is sticking with blue books next year. But even professors who have gone analog to defeat the latest technology are deeply conflicted about it. Many of them believe students should be using AI to get smarter. It would be stupid not to. These tools will be a part of their lives and knowing how to use them effectively will be an important advantage in their future workplaces. ""They will use ChatGPT all the time for all sorts of things, and that will make them more efficient, more productive and better able to do their jobs,"" said Arthur Spirling, a Princeton University professor of politics who gives proctored blue-book exams. ""It is strange to say you won't be permitted to do this thing that will be very natural to you for the rest of your career."" That's only one of the problems with blue books. Another is that absolutely nobody likes them. These bound paper booklets have been torturing both students and professors for as long as they have existed. In the 1800s, when Harvard University began requiring written final exams, a professor named Evangelinus Apostolides Sophocles was so vehemently opposed to everything they represented that he staged a protest: He burned them unread. Prof. Sophocles may have been the first but he definitely was not the last person who felt the urgent need to light a blue book on fire. These days, there are students in college who were born after the iPhone. They aren't used to writing on paper -- and it shows in their penmanship. To call it chicken scratch would be an insult to poultry. Last year, Oklobdzija allowed his class to use laptops for exams so they could type responses and he wouldn't have to decipher handwriting that looked more like hieroglyphics. He asked them to obey the school's honor code and made them promise they wouldn't use ChatGPT. Then one of his teaching assistants took a picture of a student using ChatGPT. ""So,"" he said, ""blue books it is."" When he told his class this year, he was surprised by their reaction. ""The students didn't revolt as much as I thought they would,"" he said. And before they cracked open Roaring Spring's blue books, they saw a phrase trademarked by the paper company on the cover: Use your imagination. Enlarge this image. By Ben Cohen ERIC",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-05-24,25,1389,2025-05-01,2025.2,Long,92,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.7199424046076314,1,0,0,0,-0.3592090492476277,Neutral
"Bosses already live in fear that a verbal misstep will be recorded and go viral. Now they can look forward to a new nightmare in which artificial intelligence analyzes their rhetorical stumbles and suggests they're no longer sharp enough to lead. If that sounds like the premise of a new dystopian workplace drama, consider this: The same week a halting debate performance shattered public confidence in President Biden's acuity, the journal Alzheimer's & Dementia published a study showing AI can use a person's speech patterns to forecast Alzheimer's disease with roughly 80% accuracy, six years before the condition becomes diagnosable. Lead researcher Yannis Paschalidis, a distinguished professor of engineering at Boston University, told me the current version of his machine-learning tool relies on a person's responses to a set of questions in an oral exam administered by a medical professional. It can't determine whether someone is on the road to dementia from, say, a recording of a CEO struggling through an all-hands meeting. Not yet, that is. Paschalidis envisions a near future of people using AI-powered mobile apps to raise red flags on themselves or others. The risk of a false positive is outweighed by the potential harm of cognitive decline going undetected, he says. ""If you can develop tools to democratize screening, then more people would have access to better care,"" he says. Neurologists -- not AI -- should make final diagnoses, he adds. The advances still have the potential to unfairly cast doubt on people's competence, even as they help some patients get a jump on treatment, says Dr. Bradford Dickerson, a neurologist at Massachusetts General Hospital. ""It's going to be tricky to figure out what is useful to empower people to take better control of their own health versus what needs professional interpretation,"" he says. Suppose you lose your train of thought while battling jet lag or blank on a colleague's name after pulling an all-nighter. AI could deem such gaffes signs of cognitive decline -- accurately or not -- and an employee who posts the boss's AI assessment on social media could set off a wave of scrutiny. Candidates often go through a battery of tests to show they're fit for duty, and AI is bound to make the process feel even more invasive. Research teams at West Virginia University and the University of California San Francisco are each testing AI tools that use cholesterol levels, bone density and other biomarkers found in blood to warn of Alzheimer's disease years in advance. For people who might receive a troubling AI prognosis, ""I don't think the message is 'Quit your job right now,'"" says Alice Tang, lead author of the UC San Francisco study, published in February in the journal Nature Aging. ""It's more about setting expectations and planning for what your future might look like."" Steven Blue, chief executive of railway equipment maker Miller Ingenuity, submitted to a psychological assessment before he was hired. His board of directors receives reports from his annual physicals, including blood work and EKG results. Blue, 73, says he trusts that most corporate boards wouldn't terminate a solid CEO based on an AI prediction of future dementia. Still, executives negotiating their next contracts would be wise to get limits on companies' use of AI health models in writing, he adds. ""If the board said, 'You could be a vegetable in five years, so we're going to fire you or compel you to have this kind of treatment, I would just say, 'I quit.'"" One thing could stop AI from sounding the alarm about your cognitive ability: more AI. Most business leaders don't have the White House image managers Biden had to shield him from scrutiny in the months before he dropped his re-election bid. High-tech assistants, though, are the next best thing. OpenAI's ChatGPT and Google's Gemini can instantly draft responses to questions submitted by employees during town-hall meetings. For example, I prompted ChatGPT to ""write a CEO's answer to this question from an employee: Why is the company laying people off after record profits last quarter?"" ""I understand that recent news about the layoffs, despite our record profits last quarter, is both surprising and concerning,"" began the suggested answer, generated in a split-second. ""I want to address this head-on and provide clarity on our decision-making process."" The rest of the response was a deft mix of empathetic lines about ""supporting those affected"" and business rationales like ""market dynamics"" and ""strategic realignment."" For managers with diminished abilities to think on their feet, AI text generators are a useful crutch. Then there are voice-cloning tools made by Descript and ElevenLabs that can be trained to mimic your delivery and fix your flubs. If you ""um"" and ""uh"" your way through a recorded PowerPoint presentation, you can feed the software a corrected script. The resulting audio sounds perfectly smooth and virtually indistinguishable from a human voice, according to the companies' samples. Voice and video editors only work on recordings, but AI tools could make you appear sharper on the spot, too. Live speech generators developed for amyotrophic lateral sclerosis patients could be adapted for others, AI researchers say. Imagine an upgraded version of the Stephen Hawking voice helping a boss who's lost a step sound as smart as the late, witty physicist. Credit: By Callum Borchers",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-08-26,21,883,2024-08-01,2024.3,Long,93,1,1.1325028312570782,1,0,0,0,0,0,0,1,1.1325028312570782,1,3,3.3975084937712343,1,0,0,0,0,0,0,1,1.1325028312570782,1,2,2.2650056625141564,1,2,2.2650056625141564,1,-0.3299764405821967,Neutral
"Jennifer Maravegias has been applying for dozens of jobs, so she is ready for questions about work experience and salary expectations. A recent question on an online application stumped her, though. ""Check this box if you want to make sure this isn't scanned by a machine,"" the laid-off project manager says she was prompted before submitting her application. More job seekers in New York City can now request to opt out of letting artificial intelligence vet their resumes and job applications, thanks to a new law governing AI and hiring in the city. Some companies are extending the choice to non-New York applicants, too. But is skipping AI scrutiny a good idea? Most major employers use some sort of automation to vet job applications, since companies often receive too many resumes coming in to manually review every one. Though efficient, algorithms can exclude qualified candidates or embed unintentional bias in hiring decisions. New York's law -- the first of its kind in the nation -- aims to bring transparency to the role of software in the job-application process. For any job based in New York, employers must disclose when AI is used to ""substantially assist"" in hiring and offer job applicants the chance to pass on such vettings in those cases. But letting prospective workers forgo AI resume reviews doesn't ensure a human will review those applications instead, employment lawyers and researchers say. Like Maravegias, many job seekers are unclear what the trade-offs of opting out are, and some are surprised that machines are reading their resumes at all. Months into her job search, Maravegias hadn't gotten many bites and wondered whether opaque algorithms were hurting her chances. So she opted out, only to get zero response once again. ""I was still unemployed,"" she says. Job seekers remain skeptical of AI's role in the recruitment process. Two-thirds of U.S. adults said they wouldn't want to apply for a job with an employer that used AI to help make hiring decisions, according to a 2023 Pew survey on AI in the workplace. The view was even more pronounced among women. Jeff Sepeta, an IT manager in Chicago, works as a contractor who's often moved from job to job in quick succession. Companies call him in to troubleshoot problems and move on, he says. But he fears that machines reviewing his resume will judge him negatively if they misinterpret his short tenures, particularly when applying for noncontractor roles. ""At least when I'm dealing with a human I can explain,"" he says. Among Americans surveyed by Pew a year ago, more than 70% opposed allowing AI to make a final hiring decision, while another 41% opposed using AI to review job applications. New York's new rule, Local Law 144, requires employers using software to assist with hiring and promotion decisions -- from chatbots that conduct interviews to resume scanners that look for certain keywords -- to regularly audit the tools for potential race and gender bias. Some employers argue that the New York law doesn't apply to them because AI isn't replacing the final human decision makers, said Emily Lamm, an attorney at Gibson Dunn. A Cornell University study of nearly 400 employers earlier this year was only able to identify 18 employers that had posted their audit results online, and even fewer that had posted notices informing job seekers about which automated hiring tools were being used and how to opt out. For years, hiring software has helped employers winnow down what can be hundreds or thousands of applications to a smaller number of candidates who seem, at least on paper, best-suited to the role. Millions of qualified workers get screened out every year by automated tools that reject people for reasons like resume gaps or failing to use the right combination of keywords, according to a 2021 Harvard study. Yet opting out of AI vetting can hurt your chances of getting hired, because companies aren't obligated to review all the applications they get, employment lawyers and researchers say. ""I'd say you're more or less guaranteed not to be looked at,"" said Joseph Fuller, a professor at Harvard Business School who was the lead author on the study. AI-assisted screening could ultimately help many job seekers, Fuller said, noting that human-led hiring is also subject to concerns about discrimination. Unless job seekers have a disability that qualifies them for an accommodation under federal or state disability laws, an employer doesn't have to provide an alternate vetting process, said Niloy Ray, a lawyer who specializes in AI in the workplace at law firm Littler Mendelson. ""This is but a harbinger of things to come,"" Ray said. ""You may as well start figuring out how to address this."" On the applicant side, many have already taken steps to navigate AI-driven hiring, paying for services and coaches that aim to help optimize resumes and make them an algorithmic match. Athena Karp, chief executive of HiredScore, which supplies AI-powered hiring software to employers, said that more than 80% of job seekers agree to the use of AI during the application process when its function is clearly explained. AI can offer benefits to job seekers, Karp says, such as scanning for other job postings at a company that might match an applicant's skills even when the person is rejected from the role initially sought. Robert Kerans, an IT manager based in Lake Bluff, Ill., said a recent experience left a sour taste in his mouth. He agreed to AI vetting while applying for a technology-support manager role at Accenture. He was rejected within 45 minutes. The speed of the snub made him question whether the system really worked, Kerans said, because he believed he was well-qualified for the role. Accenture said that it uses AI to help inform its decision-making but that humans always have the final say on whether a candidate advances in the recruiting process. Kerans said he's happy to have a choice, at least, and has since chosen to forgo AI vetting. ""It can fail,"" he says. ""The reality is that having the human connection is more important."" Credit: By Te-Ping Chen",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-02-26,31,1018,2024-02-01,2024.1,Long,94,0,0,0,0,0,0,4,3.929273084479371,1,0,0,0,4,3.929273084479371,1,0,0,0,2,1.9646365422396854,1,9,8.840864440078585,1,1,0.9823182711198427,1,0,0,0,-0.7299338859771042,Negative
"Investors rushing to capitalize on artificial intelligence have focused on the technology -- the capabilities of new models, the potential of generative tools, and the scale of processing power to sustain it all. What too many ignore is the evolving legal structure surrounding the technology, which will ultimately shape the economics of AI. The core question is: Who controls the value that AI produces? The answer depends on whether AI companies must compensate rights holders for using their data to train AI models and whether AI creations can themselves enjoy copyright or patent protections. The current landscape of AI law is rife with uncertainty. The New York Times, Getty Images and individual artists have challenged AI companies over their use of copyrighted material in training data sets. How these cases are decided will determine whether AI developers can harvest publicly available data or must license the content used to train their models. Should courts decide in favor of rights holders, AI companies will face increased costs that could reduce profit margins and put many current valuations into question. Investors shouldn't underestimate the risks. Equally significant are the questions surrounding intellectual-property rights for AI-generated creations. Can a novel invented by AI be copyrighted? Can a discovery guided by an AI model be patented? Recent rulings have denied such protections, emphasizing that only human creators can claim IP rights under current laws. This creates ambiguity for companies using AI in creative or inventive processes. Without clarity on whether AI-generated works can be defended as proprietary, what those companies produce may lack the legal protection necessary to secure a competitive advantage, undermining a key element of investor confidence. Critics argue the legal framework will catch up with technology, and that lawmakers will adapt to accommodate AI's evolving role in society. They also claim the value of AI lies primarily in its functional capability -- its ability to analyze, generate and innovate -- and that legal questions around copyrights and patents are a secondary concern. Those arguments underestimate the complexity and the slow-moving nature of legal systems, especially in areas involving fundamental shifts in technology and human rights. Media giants and global artists looking to protect their IP rights have deep pockets and a vested interest in ensuring they aren't left behind by AI's march. The results of these disputes will shape AI's economic potential, from cost models to go-to-market strategies. Investors can't afford to dismiss these risks as merely bureaucratic obstacles that will be sorted out eventually. A historical analogy offers a stark warning. In the 1990s, the music encoded on compact discs wasn't encrypted, leading to a piracy crisis in the music industry. The movie industry learned from those mistakes and pursued a strategy to protect its IP that was primarily legal rather than purely technological. The Motion Picture Association of America pushed for a provision in the Digital Millennium Copyright Act of 1998 that made it unlawful for hardware or software developers to circumvent copyright-protection systems. This legal threat created far more protection from piracy for owners of movie rights than technological innovation alone could have provided. AI investors take note: Technological strength alone doesn't suffice when the regulatory and legal environment is left unattended. For investors paying attention to the legal battles around AI, opportunities will abound. If the New York Times and Getty Images win their copyright-infringement lawsuits, AI companies will be required to pay for access to training data. Companies with large libraries of hard-to-obtain data and content -- such as high-definition video, medical information, financial and legal datasets, and geospatial information -- could see significant increases in the value of their assets. Some may become gatekeepers in the future AI ecosystem. Entrepreneurs and investors are already buying up licensing rights to training data and content, positioning themselves to profit from a potential surge in demand. The excitement surrounding AI is understandable, but investors must pay close attention to how courts rule on cases involving copyright infringement in training data, and to legislative developments around AI-created intellectual property. These decisions will ultimately determine who profits and who loses in the AI age. --- Mr. Harlan is founder and managing partner of Harlan Capital Partners. By Josh Harlan",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-12-27,23,699,2024-12-01,2024.4,Medium,95,3,4.291845493562231,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,8.583690987124463,1,1,1.4306151645207439,1,3,4.291845493562231,1,-0.25027443808780175,Neutral
"Every December for the past decade, we've put on our futuristic glasses to predict the year ahead in tech. Looking back, we've gotten a lot right -- and, OK, a few things wrong. Come on, who didn't think Harry Potter's augmented-reality adventure would be a smash hit? But trust us: This year our predictions are more on-point than ever. In 2025, big loose ends will be tied up, including TikTok's legal troubles and electric vehicles' federal subsidies. Long-awaited promises will be fulfilled, like self-driving Ubers, cleaner-energy data centers and crypto for everyday investors. And, of course, artificial intelligence: AI agents, AI weather forecasters, AI. . .everything. As for us humans, if our longevity-tech predictions pan out, we'll be making these annual forecasts for another 80 years. Go go AI gadgets! For years, our annual exercise has included lots of predictions about exciting new developments in smartphones -- and fail -- is now in AI-supercharged gadgets. Apple is set to release a 6-inch smart-home display, according to reports from Bloomberg. Think an iPad for your kitchen or living room wall with a big emphasis on Siri and Apple Intelligence -- areas where we're looking for big improvements. An Apple spokeswoman declined to comment. Then Amazon's Alexa is finally getting its long-promised generative-AI upgrade. Along with it, we expect to see smarter Echo speakers and deeper, more seamless interaction with the long-running voice assistant. AI will also venture out from the home. ""Next year is a big year for Meta glasses,"" Meta Platforms Chief Executive Mark Zuckerberg posted in response to our recent report that the Facebook parent will release Ray-Ban smart glasses with a small display. And then there's the wild card: Former Apple design legend Jony Ive and OpenAI CEO Sam Altman are reportedly collaborating on an AI device. It's unclear if that will be out before the 2026 ball drops. Weather apps get better It doesn't take a meteorology degree to know weather apps are often wrong. Extreme, unprecedented weather events are difficult to determine ahead of time -- and can be deadly. Enter GenCast, a new model from Google's DeepMind artificial-intelligence lab that can provide accurate forecasts up to 15 days in advance, a leap beyond current predictions. The public will be able to see real-time forecasts from the model within the first few weeks of 2025, says a company spokesman. A typical 10-day forecast is accurate only about half of the time, according to the National Oceanic and Atmospheric Administration. Google says its machine-learning-based weather model produced forecasts that were 97.2% better than a widely used European model. It's also faster: Supercomputers that crunch the numbers for traditional models can take hours to create a forecast. GenCast can generate a prediction in eight minutes. Google isn't the only one making advancements. Due in part to new models and better data capturing, NOAA made its most accurate hurricane-season predictions ever in 2024. Tick-tock goes the TikTok ban TikTok faces a U.S. ban by Jan. 19 if it doesn't shed its Chinese ownership. Will the popular social-video platform continue to exist in this country? Possibly -- but how is the question. There are a few paths: The first is the Supreme Court. The justices will decide the constitutionality of the law that banned the platform. Does it violate the First Amendment? The court scheduled fast-track oral arguments for Jan. 10. Another option: the Trump administration. If the court doesn't delay or derail the ban, which takes effect on the eve of the inauguration, the second-term president could help. While he can't unilaterally wipe away an act of Congress, the law does allow the president to lift the ban if his administration determines the site is no longer under Chinese control. TikTok has hinted in legal filings that it's hoping for executive action to block the ban or soften its impact. Finally, ByteDance, TikTok's Chinese parent company, could decide to sell the app outright. It has said it won't. But if things change, billionaire Frank McCourt is ready to buy; he says he's expecting over $20 billion in capital for what he's calling ""The People's Bid."" The clock is. . .well, you know. Agent provocateur Every big (and small) tech company will hype up the promise of AI ""agents"" this coming year. So far, generative AI has mostly been about creating text, images and videos. But in the next evolution, AI systems don't just create -- they do. Agents will understand context, learn your preferences and interact with you and other software to get stuff done: booking travel, ordering food, shopping for those new sneakers. ""Our devices are making us work way too hard to get things done,"" said David Singleton, co-founder and CEO of /dev/agents, a company building an agent operating system. Singleton said his platform, launching in 2025, aims to remove the friction from tasks we repeat 10 to 20 times a day. Google already declared its new Gemini 2.0 the ""model for the agentic era"" -- yes, they said ""agentic"" -- and has shown how its AI agents can buy plane tickets and other stuff. Anthropic recently began testing a ""computer use"" feature where you can direct its Claude model to search the web, open applications and input text using a mouse and keyboard. And, no surprise, OpenAI is reportedly launching its own AI agent platform in early 2025. Crypto boom 2.0 Bitcoin just keeps setting new records, having shot through the $100,000 barrier in early December. There are many reasons to believe it could go a lot higher than that -- and a lot lower. For those with an appetite for risk, particularly America's young men, that's apparently a feature, not a bug. With the debut of bitcoin ETFs, it's now easier to buy a ticket to ride the roller-coaster. Wall Street and big banks are cashing in, with former crypto skeptics now running giant funds themselves, such as BlackRock CEO Larry Fink. The market will be fueled by a crypto-friendly incoming administration, possible new ETFs for smaller and riskier crypto tokens, and changes at the Securities and Exchange Commission, crypto's archnemesis. These shifts may mean more millionaires will be minted, but it doesn't guarantee safety for unwary investors. ""Bitcoin in particular is becoming more of a 'normal' part of a risk-on portfolio for investors,"" says Grant Engelbart, an investment adviser at financial-advisory firm Carson Group. However, he cautions, the next time there's a winter in crypto, enthusiasm could wane. EV sales could slump The incoming administration is planning to deliver a powerful one-two punch to electric-vehicle sales in the U.S. First, there's the long-promised rollback of federal subsidies for EVs. It's something Trump adviser and Tesla boss Elon Musk supports, though it could negatively affect sales of his EVs. The knockout blow could come in the form of steep tariffs on goods from China and elsewhere -- specifically all imported battery materials. Some startups working on building domestic supply chains for batteries have said that tariffs could protect and expand U.S. battery production. ""We're trying to make sure policymakers -- no matter what administration or party they're part of -- understand the real opportunity in domestic battery manufacturing,"" says Cameron Dales, co-founder of Peak Energy, a Denver battery startup. ""If we get some of these policies wrong, we could smother a burgeoning opportunity before it gets a chance to get going."" Here's where the crystal ball gets cloudy: Will the Trump administration allow carve-outs for domestic carmakers importing battery materials? Will EV makers respond to the loss of a tax credit by lowering prices -- and would that benefit established players while hurting startups? And finally, would Republican lawmakers benefiting from subsidized investments -- such as Georgia's Hyundai EV factory -- vote to abolish such subsidies? Taxi, drive thyself OK, maybe we said 2017 was the year you'd ride in a self-driving car. This time, we mean it. Pinkie promise. Starting in early 2025, you'll be able to hail a fully autonomous Waymo via the Uber app in Atlanta and Austin, Texas. The service, from Google parent Alphabet, is even going international, with plans to launch in Tokyo. This is on top of the 150,000 trips a week Waymo is already providing in San Francisco, Phoenix, Los Angeles and Austin. In those cities, the autonomous vehicles might even start hitting freeways. Meanwhile, General Motors' Cruise is officially out of the robotaxi race, but Amazon's Zoox is still very much in. Next year the company will begin offering rides to the public in Las Vegas, followed by San Francisco. The company also recently began testing in Austin and Miami. And then there's Tesla. Musk has promised a fully autonomous version of its ""Full Self-Driving"" software for the Model 3 and Model Y deployed in Texas and California this coming year. That could mean no more human drivers on standby like the system has now. The car would be in full control, for better or for worse. The futuristic Cybercab -- no steering wheel, no pedals -- isn't expected until 2026 or 2027. However, Musk himself said at the unveiling that he tends ""to be optimistic with time frames."" Health tracking becomes longevity tracking People want to increase their lifespan, but they also want to increase their ""healthspan"" -- how long they remain healthy -- and keep their ""biological age"" young. You may be 55, but thanks to a healthy diet and regular workout regimen, you could physiologically resemble a typical 45-year-old. Now, there are apps that reveal how you are aging. In the new year, the Death Clock app's age progression tool will produce two images: what you'll look at 70 living with good habits -- and with bad habits. FaceAge, developed by Mass General Brigham researchers, can analyze a person's selfie to determine biological age. One goal is to identify whether a cancer patient is healthy enough to endure treatment and will live long enough to benefit from it. The researchers are developing the algorithm to analyze diabetic or orthopedic patients, too. Another longevity trend is monitoring glucose to identify blood-sugar spikes. When repeated and prolonged, these have been linked to higher risk for heart disease and diabetes. January, an app that will be showcased at the upcoming CES tech show, uses AI to analyze photos of what you eat to predict your blood sugar -- no hardware required. If you aren't afraid of needles, continuous glucose monitors are more accurate. DexCom's Stelo is using AI to connect blood-sugar spikes with sleep, diet and other lifestyle habits Cleaner power for data centers The AI and cloud-computing booms mean more data centers all over the U.S. They demand power and can drive up local electricity rates, stress out residents with noise, and pollute the air by tapping fossil fuels. Some tech giants are trying to figure out how to be better neighbors. If they come to your town in the next year or two, they might even bring cleaner sources of energy. Amazon, Google and Microsoft are investing billions in nuclear power. Microsoft recently signed a deal with Constellation Energy for power from the undamaged reactor at Three Mile Island, site of the nation's worst nuclear accident. In early 2024, Amazon negotiated a power deal with a plant in Pennsylvania. The companies are also looking into so-called small modular reactors, mini plants akin to the ones on nuclear-powered submarines. Amazon's head of cloud computing said that's a more distant goal, coming to maturity over the next decade -- if they work at all. Meanwhile, other nonfossil approaches will likely fill the gap. Adding really big batteries to solar and wind plants can be a cheaper way to collect and store power, so those same big cloud providers are also investing here. And we'll also see more exotic approaches: Google is already drawing power from a geothermal plant. And U.K. company Drax is hoping to build a power plant in America's pine belt that will be fueled by burning wood chips, with emissions pumped underground. By Joanna Stern, Christopher Mims and Nicole Nguyen",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2024-12-28,29,2006,2024-12-01,2024.4,Very Long,96,5,2.4925224327018944,1,3,1.4955134596211366,1,0,0,0,0,0,0,1,0.4985044865403788,1,0,0,0,1,0.4985044865403788,1,5,2.4925224327018944,1,4,1.9940179461615153,1,1,0.4985044865403788,1,-0.029335167888880537,Neutral
"Superagency By Reid Hoffman and Greg Beato Authors Equity, 288 pages, $32 How does it feel to live through the birth of a transformative technology? Historically, the experience has proved unnerving. While technologies like the printing press, the power loom, the telephone and the automobile would ultimately change the world for the better, doomsayers at the time of their first appearance focused on what could go wrong. The telephone, for instance, was viewed as a threat to in-person relationships, while cars were seen as a lure to profligate men, who would blow their money on a fancy new car and neglect to save up for family life. Today a powerful new technology -- artificial intelligence -- is again stirring up fears and finding resonance in dystopian narratives from ""1984"" to ""Terminator."" In ""Superagency,"" Reid Hoffman, with the journalist Greg Beato, seeks to reset our reflexively pessimistic framing. ""What if every child on the planet suddenly has access to an AI tutor that is as smart as Leonardo da Vinci and as empathetic as Big Bird?"" he asks. Reminding us of transformative tech's historical arc, he urges us to step away from our gloomy preconceptions and imagine ""what could possibly go right with our AI future."" Mr. Hoffman, a noted tech entrepreneur (he co-founded LinkedIn) and venture capitalist, strives for moderation. He rejects the pessimism of the ""Doomers,"" who regard the development of artificial intelligence as an existential threat to humanity, and the ""Gloomers"" (he name-checks Shoshana Zuboff, the author of ""The Age of Surveillance Capitalism""), who focus on AI's potential near-term harms, including job losses, disinformation and the exacerbation of systematic biases. Yet he also doesn't endorse the gung-ho techno-utopianism of the ""Zoomers"" (the venture capitalist Marc Andreessen comes to mind, though he is not named), who want ""a clear runway and complete autonomy to innovate."" Mr. Hoffman describes himself as a ""Bloomer"" -- optimistic about AI but cautious enough to recognize that it ""should not be developed and deployed in a unilateral fashion."" To achieve the right balance of speed and prudence, Mr. Hoffman advocates ""iterative deployment"" -- the idea that by developing and sharing technology broadly, in bite-size increments, everyone becomes a partner in the process. This approach would allow us to evaluate problems as they arise rather than smothering the field with regulation from the get-go, prompted by our worst fears. While acknowledging that Silicon Valley is faulted for ""solutionism"" -- the inclination to offer simplistic tech fixes to complex social challenges -- Mr. Hoffman is more worried by the ""problemism"" outside Silicon Valley: the tendency to exaggerate fears by dwelling on theoretical concerns. Where skeptics fret about dehumanizing ""techno-surveillance,"" Mr. Hoffman argues that technology has enabled us to be seen in all our glorious individuality. Fraud and disinformation? The ""larger story,"" he insists, is how the internet has functioned as ""an unprecedented trust machine,"" allowing us to routinely use Airbnb to book a stay in a stranger's house; Uber to hop into an unknown car; or (of course) LinkedIn to expand our professional networks. To critics who contend that big tech is exploitative, making billions off our data and participation, Mr. Hoffman counters that users get extraordinary benefits in the ""private commons"" of tech platforms; they create, he says, a ""mutualistic ecosystem."" Meta's calculated revenue per user works out to about $4 a month, while an academic study found that the median Meta user would give up the service for no less than $48 a month, suggesting, as they say, a win-win arrangement. At times, Mr. Hoffman's dogged pursuit of his thesis evokes the excesses of high-school debate. He questions whether developing a relationship with a nonhuman intelligence like AI (one of the skeptics' worries) is particularly exceptional, noting that people already feel an intimate connection to dogs, dolls and gods. Other sentences are unwittingly cringy: Comprehensive databases are ""not chains of plastic tape so much as climbing ropes you use to scale greater heights and achieve new levels of meaning and fulfillment."" A more substantial concern is the thin line between hopefulness and naivete. He blithely suggests that if the U.S. leads the way, AI technology, as it spreads across the globe, will be infused with democratic values. He allows that regulation of or limited access to most advanced models may be required -- but does he really think that such tepid measures will meaningfully curtail the risk of this rapidly evolving technology winding up in the hands of our adversaries and other bad actors? Mr. Hoffman is especially focused on the primacy of human agency, which he defines as the ability ""to make your own choices, act independently, and thus exert influence over your life."" While Gloomers, in particular, worry that technology undermines individual agency, he argues that embracing AI ""will increase our agency across nearly everything we do as humans."" He believes technologies like the large language models underlying ChatGPT and Gemini will prove particularly enabling for the least skilled, exerting a democratizing effect. Yet he also notes these models favor users with deep expertise. The result is that while AI may elevate everyone, it seems likely that some will be more elevated than others. A recent publication examining the impact of AI on material scientists bears these concerns out: While AI increased total productivity, the best researchers were boosted the most. The study also found that AI reduced the scientists' sense of satisfaction, automating ""precisely the tasks that scientists find most interesting -- creating ideas for new materials."" Mr. Hoffman, presumably, would argue that future scientists, accustomed to partnering with AI, will experience an enhanced sense of agency based on all they will be able to accomplish. Whether this forecast ultimately accords with our lived experience is the unanswered question upon which Mr. Hoffman's cheery thesis depends. --- Dr. Shaywitz, a physician-scientist, is a lecturer at Harvard Medical School and an adjunct fellow at the American Enterprise Institute. By David A. Shaywitz",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-01-28,17,990,2025-01-01,2025.1,Long,97,3,3.0303030303030303,1,1,1.0101010101010102,1,1,1.0101010101010102,1,0,0,0,0,0,0,0,0,0,2,2.0202020202020203,1,2,2.0202020202020203,1,4,4.040404040404041,1,2,2.0202020202020203,1,-0.26517663520818574,Neutral
"All over Silicon Valley, the brightest minds in AI are buzzing about ""The List,"" a compilation of the most talented engineers and researchers in artificial intelligence that Mark Zuckerberg has spent months putting together. Lucas Beyer works in multimodal vision-language research and describes himself as ""a scientist dedicated to the creation of awesomeness."" Yu Zhang specializes in automatic speech recognition and barely has an online presence besides his influential papers. Misha Bilenko is an expert in large-scale machine learning who also enjoys hiking and skiing -- or, as he puts it on his website, ""applying hill-climbing search and gradient descent algorithms to real-world domains."" The recruits on ""The List"" typically have Ph.D.s from elite schools like Berkeley and Carnegie Mellon. They have experience at places like OpenAI in San Francisco and Google DeepMind in London. They are usually in their 20s and 30s -- and they all know each other. They spend their days staring at screens to solve the kinds of inscrutable problems that require spectacular amounts of computing power. And their previously obscure talents have never been so highly valued. The chief executives of tech goliaths and heavyweight venture capitalists are cozying up with a few dozen nerdy researchers because their specialized knowledge is the key to cashing in on the artificial-intelligence revolution. Nobody in this rapidly escalating arms race is chasing the prized recruits quite like Zuckerberg, who has tried to raid Silicon Valley's top research labs, dangling $100 million pay packages to a select few superstars with the hopes of poaching them. The CEO of Meta wants them to join his company's new lab focused on superintelligence, or AI that is smarter than humans, after the release of its latest model in April fell flat. One recruit who has spoken with Zuckerberg, who is personally courting his dream team of potential hires, describes Meta's goal as nothing less than a ""transfusion from the country's top AI labs."" The universe of engineers with deep experience pursuing this type of AI research is tiny. Their loyalty to each other transcends companies. As they decide whether to leave for Meta, they are comparing notes, trading intel and plotting their futures together. They're trying to figure out who else is joining the lab -- and who else is on ""The List."" Some are teaming up as package deals. Others are negotiating lavish counteroffers to stay at their companies. So who are these people -- and why have some of the world's richest companies decided they are worth so much? In the secretive world of building ever more powerful AI models, labs have begun to take drastic steps to protect their scientific breakthroughs. At Anthropic and OpenAI, researchers work on separate, access-restricted floors, where the blinds are often drawn to guard against prying eyes. At Safe Superintelligence, the few candidates who secure in-person interviews are asked to leave their phone in a Faraday cage, a container that blocks cellular and Wi-Fi signals. Anthropic's leaders became so worried that their researchers would be targets of foreign espionage that they once invited an agent from the Federal Bureau of Investigation to visit headquarters and describe the risks to employees. The leader of Zuckerberg's handpicked Superintelligence team is Alexandr Wang, 28, who grew up in New Mexico as the son of Chinese immigrant physicists at Los Alamos National Laboratory. He began making plans to create a company in the ninth grade, when he and a friend created a Google Doc of startup ideas. This month, Meta shelled out $14 billion for a stake in his company, Scale AI -- and Zuckerberg made Wang one of the priciest hires of all time. Meta has made offers to dozens of researchers at OpenAI. The startup has responded to Zuckerberg's blitz with impressive packages of its own. Not everyone on Meta's list gets $100 million, though they're still fetching sums that were previously unimaginable. One of Zuckerberg's latest hires is a DeepMind alum and former OpenAI employee named Lucas Beyer. As a boy in Belgium, Beyer dreamed of making videogames and developed a fascination with AI. At college in Germany, he focused on mechanical engineering and dabbled in machine learning. But when he applied for a job at Google as a software engineer more than a decade ago, he was rejected and decided to pursue a Ph.D. At first, he studied quantum physics and learned something useful. ""Quantum physics is just not my thing,"" Beyer said on the AI Epiphany show. He made computer vision and robotic perception his thing instead. When he hit the job market in 2018, he'd done two summer internships at Google and, this time, the company offered him a job. In fact, his talents were in such high demand that he received offers from all the industry's top AI destinations -- except one. ""Meta,"" he said on that show. ""I never heard back."" These days, Beyer is hearing directly from Meta's CEO. After six years as a staff research scientist at Google Brain and DeepMind, Beyer left in 2024 to start OpenAI's office in Zurich with two colleagues, Alexander Kolesnikov and Xiaohua Zhai. Now they're on the move again -- because Zuckerberg just hired all three of them. ""Yes, we will be joining Meta,"" Beyer wrote on X this week. ""No, we did not get 100M."" There are some clues that help executives and recruiters assess talent. They can poke around Google Scholar and browse through the papers that researchers have published, checking to see how often their work has been cited. Zuckerberg himself has been spending his days weeding through wonky papers, searching for hotshot engineers and scientists to recruit for his Superintelligence team of roughly 50 people. He's in a group chat with two Meta executives called ""Recruiting Party"" in which they discuss hundreds of potential candidates and tactics for approaching them -- like whether they prefer to be contacted by email, text or WhatsApp. The people who get notes from Zuckerberg have a few things in common. They need to know calculus, linear algebra and probability theory, like one of Meta's recent hires, who says he's fascinated by algorithm design. An undergraduate degree in computer science isn't nearly enough training. These researchers get Ph.D.s from elite programs -- Berkeley, Stanford, Carnegie Mellon, MIT -- whose most selective fields admit less than 1% of applicants and have become a feeder system to AI labs. Today's researchers also had perfect timing. When many embarked on their Ph.D. work a decade ago, they were tackling esoteric topics in robotics and generative artificial intelligence. There was nothing remotely sexy about their fields. But they were exploring the frontier of AI -- and their areas of interest have yielded revolutionary advances. Zhang, a researcher at OpenAI who works on speech, was finishing up his first internship more than a decade ago when his mentor offered him some practical advice. ""You should not work on speech,"" Zhang was told, he recounted to Carnegie Mellon students last year. ""This is a dead field."" That mentor soon left for Yahoo to work on ads. A few months later, the seemingly dead field was resuscitated by advances in deep learning, which created a new way for computers to analyze and understand speech patterns. Now, Zhang is the sort of engineer that every lab covets -- and Zuckerberg has reached out to him. One reason all these companies are willing to shower recruits with cash is that even a superteam of AI engineers costs only a fraction of the price of AI infrastructure like data centers. This year alone, Meta plans to invest around $70 billion on AI, while Amazon, Microsoft and Alphabet are spending even more. Next to hardware, humans look like a bargain. People in the tight-knit AI community say the researchers whose skills have never been so lucrative are not primarily motivated by huge piles of money. Until recently, many were perfectly happy to become professors. These days, they're becoming researchers at AI labs -- but not just because of the pay. It's because only tech companies with seemingly limitless resources offer the computing power, data, infrastructure and freedom they need to run their experiments and scale their models. ""For my students and postdocs, the objective was never to do hot things and become a millionaire,"" said Alexei Efros, a University of California, Berkeley, professor whose recent Ph.D. students include research scientists at OpenAI, DeepMind and Anthropic. ""The objective is to try to solve cool, interesting, important, unsolved problems."" Those problems require money and chips. Meta, OpenAI and Google have both in abundance. But to win recruiting battles, top executives also have to get involved themselves. OpenAI CEO Sam Altman has invited potential hires to play poker or have dinner with him at his Russian Hill mansion in San Francisco, while President Greg Brockman used to host ""Game of Thrones"" watch parties. Elon Musk once hosted a recruiting party at OpenAI's former headquarters to entice researchers into joining xAI. At Google, CEO Sundar Pichai and co-founder Sergey Brin carve out time to bring over important hires. The eight- and sometimes nine-figure pay packages are tempting, and the outreach from Zuckerberg is flattering, but Meta's laggard history in generative AI has made some recruits hesitant. When the company released its latest model to little fanfare, some researchers at Meta distanced themselves from the model, removing the project, called Llama 4, from their LinkedIn bios. Zuckerberg hasn't had much success in his efforts to hire the field's biggest stars, including OpenAI's co-founder Ilya Sutskever and its chief research officer, Mark Chen. Many candidates are happy to take a meeting at Zuckerberg's homes in Palo Alto and Lake Tahoe. In private, they are comparing gossip and calculating Meta's chances of winning the AI race. The handful of researchers who are smartest about AI have built up what one described as ""tribal knowledge"" that is almost impossible to replicate. Rival researchers have lived in the same group houses in San Francisco, where they discuss papers that might provide clues for achieving the next great breakthrough. When a Ph.D. student at Berkeley named Bill Peebles submitted his dissertation in 2023, he thanked another Ph.D. candidate named Tim Brooks. ""We are very aligned on research directions and interests,"" he wrote. ""I hope we get to work on more stuff together in the future."" As it turned out, they would. Peebles and Brooks both went to work for OpenAI and led the team that released its groundbreaking Sora text-to-video generator in 2024. Brooks has since left for DeepMind, while Peebles is still at OpenAI -- even after Meta recently tried to hire him. And then Zuckerberg moved on to the next person on ""The List."" Enlarge this image. By Ben Cohen, Berber Jin and Meghan Bobrowsky",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-06-28,46,1793,2025-06-01,2025.2,Very Long,98,1,0.5577244841048522,1,0,0,0,0,0,0,0,0,0,2,1.1154489682097044,1,0,0,0,0,0,0,0,0,0,4,2.230897936419409,1,0,0,0,2.5505013443541373,Positive
"Akido Labs is using artificial intelligence to bring medical care to the streets of New York. The Los Angeles-based healthcare provider has created an AI doctor that suggests diagnoses and treatments based on patients' symptoms and medical histories. A human doctor approves, modifies or rejects the recommendations. Now Akido is bringing this technology, ScopeAI, to ride-share drivers in New York through a partnership with two groups that can help it connect with these workers: the Independent Drivers Guild, an advocacy group, and Workers Benefit Fund, which works with labor unions and policy leaders to provide health and other benefits to gig workers. Akido plans to station ScopeAI-equipped physicians in offices and mobile locations around the city, starting in Queens, to speed access to primary and specialty care for drivers who might otherwise skip the doctor to avoid taking unpaid time off. Most healthcare-AI startups streamline administrative tasks, but Akido and some others are pushing the technology into everyday medicine. Others include K Health, which has raised $384 million in venture capital and says its AI copilot helps primary-care doctors diagnose and treat patients. As AI enters patient care, it is raising concerns that it might increase physicians' burnout rather than lessen it. Researchers at the University of Texas at Austin and Johns Hopkins University, for example, recently argued in JAMA Health Forum that medical organizations are adopting AI faster than laws and regulations governing its use are evolving. This puts a burden on physicians: They feel pressure to accept AI recommendations to minimize medical errors, but bear responsibility for knowing when to overrule or defer to the technology. Hospitals and medical practices should share responsibility with doctors for decisions reached through AI and these organizations should continually review their use of the technology, said Christopher Myers, associate professor of management and organization at Johns Hopkins Carey Business School and an author of the article. ""We assume that because physicians have access to AI, they should definitely get it right,"" Myers said. ""We should be thinking more on a system level."" Akido says ScopeAI gives its reasoning for its diagnosis and treatment recommendations and that doctors have the final say and are responsible for their decisions. ScopeAI hasn't caused any misdiagnoses or other problems, said Akido co-founder and Chief Executive Prashant Samant. ""The goal is to provide more access to care, faster,"" Samant added. Akido launched in 2015 after spinning out of the University of Southern California's D-Health Lab, a digital-health lab Samant and co-founder Jared Goodner founded in 2013. Akido initially sold software to healthcare providers, nonprofit and government groups used to predict which people were likely to fall ill, or suffer a crisis such as homelessness, so customers could intervene accordingly. From the start, however, the team had its sights on using technology to improve and extend the reach of healthcare. The company, which has raised $25 million in venture capital from Y Combinator, Slow Ventures and others, in 2022 shifted its business model. Instead of selling software, it became a healthcare provider itself, acquiring a multispecialty medical practice outside Los Angeles. Akido now says it serves more than 500,000 patients across 26 specialties and has locations for in-person doctor visits in California and Rhode Island in addition to its latest expansion into New York. Akido developed ScopeAI using its historical data on patient visits to train its models to accurately predict diagnoses and treatments. It introduced it in August at a cardiology practice near Los Angeles and now intends to extend it broadly across its practice. Akido uses a human, certified medical assistant it has trained to sit with patients and ask questions prompted by ScopeAI, which asks further questions based on the person's answers. With ScopeAI, these appointments can be longer to gather more information, said Goodner, Akido's chief technology officer. The typical visit is 30 to 45 minutes, according to Akido, which says ScopeAI doesn't need Food and Drug Administration approval because physicians make the final call. Michael Cardamone, a managing partner of Forum Ventures who isn't an Akido backer, said he doesn't believe AI will replace doctors but that it is inevitable AI will become more commonly used as a clinical decision-support tool. In addition to cardiology, Akido has used ScopeAI to support care it provides to homeless people and patients in rural areas. Dr. Steven Hochman, medical director of Akido's street medicine in Los Angeles, said ScopeAI doesn't feel like talking to a computer because the patient is conversing with a human medical assistant. ""You're talking to a trained person who is using the brain power of a computer to work with you towards the best diagnosis,"" Hochman said, adding that he rarely sees ScopeAI make more than minor errors and that most of the corrections he makes when reviewing and approving AI charts are clerical in nature. Brendan Sexton, president of the Independent Drivers Guild, said ride-share workers often drive 10 or more hours a day, which takes a toll. He hopes ScopeAI is part of the solution to encouraging drivers to take better care of themselves. The drivers' medical care will be covered by their health insurance with no out-of-pocket costs, Samant said. ""Every hour missed is an hour they're not earning money,"" Sexton said. By Brian Gormley",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-03-28,26,876,2025-03-01,2025.1,Long,99,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3.4246575342465753,1,0,0,0,0,0,0,-0.1863151846043557,Neutral
"Sam Altman once said OpenAI and Microsoft had the ""best partnership in tech."" Now, their Silicon Valley marriage is on the rocks. Microsoft turbocharged the artificial-intelligence startup's growth over the past six years with billions of dollars in funding, helping OpenAI's ChatGPT accumulate more than 500 million weekly users. OpenAI powered cutting-edge generative AI tools for the technology giant, helping its share price triple. That relationship has become strained. The chief executive officers are increasingly at odds over the computing power Microsoft provides to OpenAI, the access the startup gives the technology giant to its models and whether the Altman-led company's AI systems will soon achieve humanlike intelligence, according to people familiar with their relationship. Microsoft CEO Satya Nadella has also made it a priority to beef up sales and usage of ChatGPT rival Copilot, and last year hired a rival of Altman's who launched a secret effort to build models for Microsoft that would reduce its dependence on OpenAI. While they are preparing their companies for independent futures, both still wield tremendous power over one another. Microsoft can effectively block OpenAI's effort to restructure into an independent for-profit company, according to people familiar with the matter. OpenAI could lose tens of billions of dollars if the conversion doesn't happen by the end of this year. Microsoft hasn't threatened to do so, a person familiar with the matter said. OpenAI's board, meanwhile, can trigger a clause in its contract that prevents Microsoft from accessing its most cutting-edge technology, people familiar with the matter said. OpenAI officials have raised the possibility of doing so over the past year, some of the people said. Nadella and Altman's partnership grew after a chance meeting in 2018 at investment bank Allen & Co.'s annual conference in Sun Valley, Idaho. Altman had co-founded OpenAI as a nonprofit research lab three years earlier with the aim of creating artificial general intelligence, or AGI -- AI that outperforms humans. In a five-minute conversation, Altman told Nadella that OpenAI, then still a nonprofit, was ""going to raise a bunch of money,"" Altman told the Journal in 2023. A year after that encounter, Microsoft invested $1 billion in OpenAI. The investment granted Microsoft exclusive access to OpenAI's technology, while Microsoft became OpenAI's exclusive cloud provider. But it was OpenAI that built the first smash-hit product of the generative AI boom with ChatGPT in November 2022. ChatGPT, with its powerful ability to generate new sentences and answer questions, forced Silicon Valley titans such as Alphabet and Meta to overhaul their product plans. The OpenAI partnership transformed Microsoft from an aging tech company into one of the leaders of the AI boom. In the closest moments of their relationship, it wasn't unusual for Nadella to text Altman five or six times in a row and for Altman to reply back in kind. Microsoft invested an additional $10 billion in early 2023 -- a deal that gave OpenAI the firepower to rent data centers from Microsoft for training its new models. Later that year, when Altman was briefly pushed out of OpenAI by board members, Nadella offered to hire him. Within OpenAI, the episode became known as ""the blip."" For Nadella, it was a sign that the company needed insurance against Altman and OpenAI. Unbeknown to Altman, Nadella set his sights on hiring Mustafa Suleyman, one of the three co-founders of Google's DeepMind. The Microsoft CEO wooed him over a series of meetings. Microsoft paid $650 million to hire Suleyman and his colleagues at startup Inflection. Suleyman began work building a large language model that aimed to rival what was then OpenAI's most advanced publicly released technology, GPT-4, people familiar with the work said. The project got off to a rough start. An early training run showed that building a model comparable to OpenAI's would be more difficult than expected, leading Microsoft to extend its reliance on OpenAI, people familiar with the matter said. In one meeting about sharing intellectual property, Suleyman yelled at OpenAI's lawyers in front of senior OpenAI executives, people familiar with the incident said. Suleyman didn't respond to requests for comment. One key area of tension: OpenAI's development of models with humanlike intelligence. The companies' agreement gives OpenAI's board the power to change its relationship with Microsoft once the startup successfully builds models with humanlike intelligence. Altman has said he is confident his team can build that functionality soon. Microsoft negotiators have told OpenAI that the present technology is nowhere near that threshold, the people said. OpenAI, meanwhile, wants more computing power from Microsoft and access to top-of-the-line chips. Microsoft has told OpenAI that it is giving everything it can and has loosened its exclusivity restrictions. Microsoft was frustrated last summer, when OpenAI was unusually slow to hand over the code for its powerful new reasoning model, people familiar with the negotiations said. OpenAI needed extra time to understand the model's capabilities, said people familiar with the company's thinking. These days, Altman and Nadella text less and primarily speak to each other on scheduled weekly calls. In January, Altman stood at a White House lectern with President Trump, SoftBank CEO Masayoshi Son, and Oracle co-founder Larry Ellison announcing that the Japanese conglomerate and Oracle would anchor an up to $500 billion investment in Stargate. The project closely resembled what Altman had hoped to accomplish with Nadella, who was thousands of miles away at the World Economic Forum in Davos at the time. Enlarge this image. By Deepa Seetharaman, Berber Jin and Keach Hagey",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-04-30,34,915,2025-04-01,2025.2,Long,100,1,1.092896174863388,1,0,0,0,0,0,0,0,0,0,1,1.092896174863388,1,1,1.092896174863388,1,0,0,0,1,1.092896174863388,1,4,4.371584699453552,1,3,3.278688524590164,1,-0.022028107611905535,Neutral
"OpenAI is investigating whether Chinese artificial-intelligence startup DeepSeek trained its new chatbot by repeatedly querying the U.S. company's AI models. The Silicon Valley-based company said Wednesday it has seen various attempts by China-based entities to exfiltrate large volumes of data from its AI tools, likely to train their own models in a technical process called distillation. OpenAI said it has banned the accounts it suspected of distilling its models and has worked with Microsoft to identify the actors behind the attempts. DeepSeek is among those that OpenAI is looking into, according to a person familiar with the matter. ""It is critically important that we are working closely with the U.S. government to best protect the most capable models from efforts by adversaries and competitors to take U.S. technology,"" an OpenAI spokeswoman said. Microsoft declined to comment. DeepSeek didn't respond to requests for comment. OpenAI's suspicions raise the prospect that the performance of DeepSeek's model, which is said to be on par with some of the world's top AI models, could be less impressive than it originally appeared. It also raises the specter that companies spending hundreds of millions of dollars to train state-of-the-art models may have trouble keeping rivals from copying their work. Distillation is used to develop smaller, more efficient AI models by training them on a database of responses from a larger and more expensive model. David Sacks, President Trump's AI czar, on Tuesday explicitly accused DeepSeek of using distillation of OpenAI models to build its own. ""There's substantial evidence that what DeepSeek did here is they distilled the knowledge out of OpenAI's models,"" said Sacks, a veteran Silicon Valley venture capitalist. ""And I don't think OpenAI is very happy about this."" News Corp, owner of The Wall Street Journal, has a content-licensing partnership with OpenAI. By Sam Schechner",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-01-30,13,301,2025-01-01,2025.1,Medium,101,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.39274505173846963,Neutral
"OpenAI is in early talks to raise up to $40 billion in a funding round that would value the ChatGPT maker at $300 billion, according to people familiar with the matter. SoftBank would lead the round and is in discussions to invest between $15 billion and $25 billion. The remaining amount would come from other investors. The two companies were recently in talks to value OpenAI as high as $340 billion. One of the people familiar with the matter said newer negotiations moved the proposed valuation down to $300 billion. The Japanese company is helping assemble investors for the rest of the round, one of the people said. The discussions are still in flux and could fall apart, the person said. The $300 billion valuation would include the cash OpenAI raises in the round. OpenAI was last valued at $157 billion in October, when it raised $6.6 billion. Roughly doubling its value in just a few months would be extraordinary even by the standards of Silicon Valley's AI boom. The funding will be used in part to help OpenAI fulfill its roughly $18 billion commitment to Stargate, a joint venture with SoftBank and others to finance the construction of data centers in the U.S. powering OpenAI's technology. OpenAI also expects to use the cash to fund its money-losing business operations. At $300 billion, OpenAI would be the second-most valuable startup in the world, behind only Elon Musk's SpaceX, according to the data provider CB Insights. A funding round of this size would be one of the largest in Silicon Valley history and pass OpenAI's previous fundraising record achieved in 2023, when it raised $10 billion from Microsoft. OpenAI is attempting to raise the cash after AI models released by the Chinese firm DeepSeek led to a selloff in big tech stocks, including Nvidia, earlier this week. DeepSeek's success with cheaply made and free-to-use AI technology has led many investors and executives to question the big-spending strategies of OpenAI and other U.S. developers. OpenAI expected to lose around $5 billion last year on revenue of $3.7 billion, The Wall Street Journal reported in October. At the time, it projected its revenue would grow to $11.6 billion this year. The funding talks mark a quickly deepening relationship between OpenAI Chief Executive Sam Altman and SoftBank CEO Masayoshi Son. SoftBank has separately committed to contribute some $18 billion to Stargate, which Son announced at the White House earlier this month, alongside Altman and Oracle Executive Chairman Larry Ellison. The project's partners have committed to invest $100 billion in U.S. data center projects for OpenAI and plan to invest up to $500 billion over four years. By Berber Jin and Deepa Seetharaman",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-01-31,20,448,2025-01-01,2025.1,Medium,102,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,4.464285714285714,1,-0.5408886930614819,Negative
"Tech giants have spent billions of dollars on the premise that bigger is better in artificial intelligence. DeepSeek's breakthrough shows smaller can be just as good. The Chinese company's leap into the top ranks of AI makers has sparked heated discussions in Silicon Valley around a process DeepSeek used known as distillation, in which a new system learns from an existing one by asking it hundreds of thousands of questions and analyzing the answers. ""It's sort of like if you got a couple of hours to interview Einstein and you walk out being almost as knowledgeable as him in physics,"" said Ali Ghodsi, chief executive officer of data management company Databricks. The leading AIs from companies like OpenAI and Anthropic essentially teach themselves from the ground up with huge amounts of raw data -- a process that typically takes many months and tens of millions of dollars or more. By drawing on the results of such work, distillation can create a model that is almost as good in a matter of weeks or even days, for substantially less money. OpenAI said on Wednesday that it has seen indications DeepSeek distilled from the AI models that power ChatGPT to build its systems. OpenAI's terms of service forbid using its AI to develop rival products. DeepSeek didn't respond to emails seeking comment. Distillation isn't a new idea, but DeepSeek's success with it is raising new doubts about the business models of tech giants and startups spending billions to develop the most advanced AI, including Google, OpenAI, Anthropic and Elon Musk's xAI. Just last week, OpenAI announced a partnership with SoftBank and others to invest $500 billion in AI infrastructure over the next five years. If those investments don't provide companies with an unbeatable advantage but instead serve as springboards for cheaper rivals, they might become difficult to justify. In the wake of DeepSeek, executives and investors in Silicon Valley are re-examining their business models and questioning whether it still pays to be an industry leader. ""Is it economically fruitful to be on the cutting edge if it costs eight times as much as the fast follower?"" said Mike Volpi, a veteran tech executive and venture capitalist who is general partner at Hanabi Capital. OpenAI CEO Sam Altman on X called DeepSeek's latest release ""an impressive model, particularly around what they're able to deliver for the price,"" and added, ""we are excited to continue to execute on our research roadmap."" Anthropic CEO Dario Amodei wrote on his blog that DeepSeek's flagship model ""is not a unique breakthrough or something that fundamentally changes the economics"" of advanced AI systems, but rather ""an expected point on an ongoing cost-reduction curve."" Tech executives expect to see more high-quality AI applications made with distillation soon. Researchers at AI company Hugging Face began trying to build a model similar to DeepSeek's last week. ""The easiest thing to replicate is the distillation process,"" said senior research scientist Lewis Tunstall. AI models from OpenAI and Google remain ahead of DeepSeek on the most widely used rankings in Silicon Valley. Tech giants are likely to maintain an edge in the most-advanced systems because they do the most original research. But many consumers and businesses are happy to use technology that is a little worse but costs a lot less. President Trump's AI czar, David Sacks, said on Fox News on Tuesday that he expects American companies to make it harder to use their models for distillation. DeepSeek has said it used distillation on open-source AIs released by Meta Platforms and Alibaba in the past, as well as from one of its models to build another. Open-source AI developers typically allow distillation if they are given credit. DeepSeek's own models are open-source. NovaSky, a research lab at University of California, Berkeley, this month released technology it said was on par with a recent model released by OpenAI. The NovaSky scientists built it for $450 by distilling an open-source model from Chinese company Alibaba. The Berkeley researchers released the model as open-source software, and it is already being used to help build more cheap AI technology. One startup, Bespoke Labs, used it to distill DeepSeek's technology into a new model it said performed well on coding and math problems. ""Distillation as a technique is very effective to add new capabilities to an existing model,"" said Ion Stoica, a professor of computer science at UC Berkeley. Competition in the AI industry is already fierce, and most companies are incurring losses as they battle for market share. The entry of DeepSeek and others that use distillation could drive prices down further, creating a feedback loop in which it is harder and harder to justify spending huge sums on advanced research. Prices for software developers accessing AI models from OpenAI and others have fallen dramatically in the past year. Open-source AI such as DeepSeek's only promises to lower costs further, tech executives say. ""It will be harder to justify very large margins for this level of intelligence,"" said Vipul Ved Prakash, CEO of Together AI, which sells computational services to developers of AI applications. By Miles Kruppa and Deepa Seetharaman",USA,"Wall Street Journal, Eastern edition; New York, N.Y.",2025-01-31,28,854,2025-01-01,2025.1,Long,103,0,0,0,0,0,0,0,0,0,0,0,0,1,1.17096018735363,1,0,0,0,0,0,0,0,0,0,5,5.85480093676815,1,0,0,0,0.3237169180556702,Neutral
